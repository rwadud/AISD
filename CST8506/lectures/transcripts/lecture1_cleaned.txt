So the methodology, what does methodology mean? CRISP-DM. And I will not go through every step again. It's not going to be quiet, I feel. But these are the steps that you should follow for any machine learning project. One exception is like if you're working with images, you do not need the understanding phase. There is no understanding phase or much of a data preparation phase. Only scaling is required. What is the input for the images? So if you have 10,000 images, what is the nature of inputs, or what are those values? Makes sense, right. Those values are pixels. We do not need to work on encoding. You do not need to do things like creating new columns, constructing new columns. These things are not required for that. But otherwise, for a machine learning project, we need to understand the business, understand the data, prepare it, create models, find the answers from the models to make sure that it's working. It's working fine. And then after the evaluation phase, you can deploy it. This process is the same for any machine learning project in general, okay?

And initially we talked about these two parts already, supervised learning and unsupervised learning. Last term, we learned about classification and the different classification techniques. Then regression, we talked about different approaches. For unsupervised learning, clustering algorithms, our K-Means, Agglomerative, and DBSCAN. Yeah? Then for the preprocessing, in general, what are the different preprocessing steps? Mainly the data. That means if there is missing data, we should handle it. If there are duplicates, we have to handle it. We have to understand whether they are actual duplicates or not.

So I think last time or so, even after all the discussions, sometimes in the project they mentioned that they have 30% duplicates. Why are you getting 30% duplicates in a normal dataset? Did you guys think about it? If you are getting a high percentage of duplicates in your data, let's say crash data, maximum data. Do you think every officer is simply entering data? Why are you getting a percentage of duplicates, or what is the possibility of getting duplicates? What mistake could there be? Yeah. Maybe removing variables or unique information. It just bundles everything together. So by the time you remove a lot of columns and keep only like, you had like 80 columns and you have to come down to, you're narrowing down to 20 columns or 15 columns. And then if you look, you'll see many of them are duplicates. They are not actually duplicates. It's very unlikely that you can have duplicates in government data. You can have maybe one or two, maybe two officers entered one case twice. Two officers entered the same case, then we have it twice, plus a day. But normally there won't be many duplicates. Okay? Or if some sensor is transmitting and it just sends some information with the same timestamp up to millisecond or microseconds, it is possible you can get some duplicates, something like that. Otherwise there is less chance to have duplicates in your data. So these are all data cleaning steps and we can integrate data from multiple sources, which we talked about last time.

Then we can transform data, right. Yeah. So for transformation, we are talking about these terminologies. Remember. What is normalization? What is normalization? Your values fall between zero and one. You are rescaling your values so that every value is between zero and one, okay? That is normalization. And you can do range normalization, which means you are rescaling it. All of this is rescaling. You have a scale, but you are rescaling it. So that means in range normalization, assume that I want to put everything between one and ten. So my range is 1 to 10, I'm just rescaling those values, so that every value is between one and ten. In that case, my min is 1 and max is 10. That is range normalization. What is standardization? What do you mean? Standardization is when the data will be centred around the origin, okay? That is standardization. And then we talked about binning, right? Yeah. So if you have a variable, you can bin it in different ways. You have equal width binning, you can have equal frequency binning. Equal width binning means you can specify the width of the bins. So everyone from age 0 to 10 in bin 1, 10 to 20 in bin 2. You're specifying the width of the bins. Or you can specify frequency, how many items will be in each bin. You can tell, okay, first 20 instances in bin one, next 20 in bin two. In that case, it must be sorted. Okay. So binning, we talked about last term.

And then we talked about sampling. We talked about different sampling families, random sampling and stratified sampling families. Okay, what is stratified sampling? Stratified, or give an example of stratified sampling. Any example? You take from the data? Not a stratified example. Like a group of strata and then you select a group from each stratum. So one from each and then you can do like, you split by age and then you select. So if you split by age, then if you have a structure based on age, for example, child, adult, senior, elder, okay? And you are taking a certain percentage of instances from every group, that is stratified sampling. For example, every month is a stratum, like January is one stratum. So you are getting some percentage from every month. Those instances happened in January. So 10,000 are from January, 10,000 from February. You can set the number of instances, the count of instances from every month. That is stratified sampling. Okay? Yeah?

So these are all things we learned last term. So I expect you know these things, okay? If you do not know them, go and refresh your knowledge from the last term and make sure that you have a good understanding about these preprocessing techniques.

So today we are going to talk about dimensionality reduction, okay? So dimensionality reduction, why is this an important topic? Sometimes we may have more features. Or 1 million features. Okay. So you can manage hundred features, but if we have 1 million features or 100,000 features, it is not easy to manage. Okay. So we want to reduce the dimensions for different reasons. If you have, just take the example of Euclidean distance. You have two instances. This is a simple example. So if you have two instances and you have 10,000 features. So there are two instances, F1 to F10,000. Okay? And you have two instances, instance one and instance two. What will the Euclidean distance look like? So what about, like, how many terms will be inside the distance formula? Square root of the sum of squared differences. So, how many terms will be there inside? 10,000? You will have 10,000 terms inside, right? Yeah, because you have 10,000 features. Agree? So, if we have only a thousand features, you'll have a thousand terms. So the computation will increase if you have a large number of features. Yeah. So you want to reduce the number of features always, but without losing a lot of information. So if from 10,000, if I come down to 10, I may lose a lot of information. Okay? So I can specify how much information I want to keep. So maybe I can specify 90% of information I want to keep. Or 80% of information I want to keep, and keeping that percentage in mind, if I can reduce this number of features, then I can reduce the complexity of my models. Agree? Yeah, that is the basic understanding, the plain English meaning of dimensionality reduction: reduce the number of features while retaining essential information, and we are solving the problem in lower dimensions.

The drawbacks include high memory consumption and complex models. You can understand, we can at maximum visualize three dimensions, right? If we have four dimensions, there is no way that we can visualize. Those four dimensions, at most three dimensions we can visualize. So these are the drawbacks. And the main problem, the curse of dimensionality, is if we have 10,000 features, this is a 10,000 dimensional space. Every point is in that 10,000 dimensional space. So when you have two dimensions, we can draw it like this, right? So if you have only two dimensions, I can draw it like this, okay? So this is my X axis. This is my Y axis, my points would be something like this, okay? If I have three dimensions, I have one more dimension, and then still I can plot it. If I have 10,000 dimensions, in theory, there is a 10,000 dimensional space. Every instance is a point in the 10,000 dimensional space. Agree? Yeah. When we have many dimensions, every point will be sparse. Okay? When you have a large number of dimensions, every point will be scattered and very hard to find patterns in it. Okay? That is basically a source of losing information. We are not getting any meaningful decisions from those distance values. And still, if we want to make some sense, you should have a lot of data, because in 10,000 dimensions, you need a lot of data to make some patterns in that large number of dimensions. Okay? So it's good to come down. If you have a smaller number of dimensions, that is better, but without losing a lot of information.

So we have two options. One is feature selection, and one is feature extraction. Feature selection, we did it last term. So we are selecting features, we are ignoring redundant ones. We are ignoring irrelevant ones. We are selecting the ones that we want to keep or that have some meaning for the question we are trying to answer. That is feature selection. In this class, we are talking about feature extraction. Okay. Feature extraction means we are basically creating synthetic features. I'll show how it looks. We are creating meaning out of it and creating synthetic features using, for example, instead of having these two axes, I'm going to create one axis that can represent these points meaningfully. That is basically feature extraction. So how to do it? We have two techniques, one is Principal Component Analysis, and one is Linear Discriminant Analysis. So we take Principal Component Analysis first.

So I'll go with an example first. So these are my points, okay? Yeah? So the objective is we have two axes, maybe this is like length and this is width. Maybe assume that you have a dataset and this is the length and this is the width of a tune. Okay? So instead of keeping this length and width, how can I reduce it to one factor? Now we have two factors, right? Two features, length and width. How can I reduce these two features into one feature? There is no name for that feature, but can I find an axis? Can I find a new axis that I can represent these points meaningfully in one dimension? Okay? That is the question. Can I find a new axis? What's the purpose of that axis? Instead of using this length and width, can I have some synthetic axis that represents these points meaningfully? That is the purpose.

So, if I have these points, I can pass different lines through the origin like this, right? These are all different axes, right? Yeah. So if I take one axis, how are we able to do this? So I take one of the lines and show how we can do this. So assume that this is my choice, okay? So I will be projecting these points onto this line. Okay? So all these points are projected on this new line, okay? And I can have many lines, right? To any of these lines, I can project these points. Yeah, agree? To any of these lines, I can project these points. But which line should I select? Which line gives me the maximum variance?

Maximum variance means: so this point is projected here. So there is a distance from here to the origin. This point is projected here. There is a distance from here to the origin. This point is projected here. There's a distance from here to the origin. This is here, so the distance is here. This is here. So all these distances, the sum of all these distances should be maximum. Understood?

So take the first point. The first point is projected here, right? So the first point is projected here. There is a distance from this projected position to the origin. Agree? Yeah. So this is d1. Then I have the next point projected here. This distance is d2. Then I have this point projected here, which is distance d3. Okay? Yeah. So likewise, I have six points. One, two, three, four, five, six. So I have six distances, d1 to d6. Okay. So if the sum of distances is the biggest, it means the projected points are spread on that new line. Okay? Yeah?

If the distance is small, so I have two lines, line one and line two. Line one, sum of distances, and line two, sum of distances. If this is 2.8 and this is 5.6, what is the meaning of it? We have L1 and L2. What is the meaning of these two lines? Which line has points spread more? Second. L2, right? L2 line has points spread more as compared to L1. Okay? So we can create infinite number of lines. Okay? But the objective is to find the line that maximizes the distance. Okay, that's the idea. So if it is spread on the line, that is the maximum variance. We are trying to see how much variance that line is giving us. Yeah?

That is the logic of PCA. So once you find a line like this, once we have this line created with the maximum variance, then these are our new points on this new axis. Understood?

So, when you're finding the projection, how do you find the projections? The point on the line? Do you try to get the X component and the Y component? The math is different. But the logic, logic wise, you got the logic.

Let me explain one more time. So I have a few points. So this is the axis length. This is the axis width. So assume that they are on the same scale. Otherwise, if one axis is age and the other is salary, it's not meaningful, right? So we have to standardize the data for sure. But I'm assuming that part. I assume that they're already on the same scheme. So I'm going to put the points. So we have points. Okay, five points. So I can draw infinite number of lines passing through the origin in this space. Right? I can draw an infinite number of lines. But my objective is to find the line that satisfies some condition. Okay? The condition is, I want to find a line such that once these points are projected onto that line, they should be spread the maximum. Okay? Those points should be spread on that line, and whichever line gives the maximum spread, that is the best line. That is the axis we are going to consider. Understood so far, yeah?

So we want to find one axis and the condition for that axis is once I project points onto this new axis, the points on that axis should be spread to the maximum. So once it is done, I can show you: there is one line like this, okay? On this one, the points are like this. And another line, maybe something like this. This line is giving me points spread like this. Okay, this is one line and this is the other line. Which line gives the maximum spread? This is line one, and this is line two. Line two? Line two gives the maximum spread. Right? So I can create infinite number of lines, but whichever line gives the maximum spread for the projected points, that is the new axis. Understood. The spread is like the sum of all the distances.

So I can have infinite number of lines. So this can be one line. This can be another line. This can be another line. So I draw three lines, L1, L2, L3. I can project these points to any of these lines, right? Yeah. So on the first line, maybe this is giving me a point here, here, here, here and here. But for this one, this may be giving something like this. Okay. So whichever line gives the maximum variance, that is the axis I want to find. Yeah.

So when I have this optimal line, maybe something like this. So this point is projected here, like this is projected here, like this. This is projected here. This is projected here. This is projected here. Okay. Makes sense. All good?

So then there is an animation here. See? All these lines that they are showing, various lines. Okay? Yeah. For every line, the points are projected. But the objective is to find the line that maximizes the distance. This is on a scale from minus 3 to plus 3. Understood? Yeah. So in this case, the line going through this purple segment, parallel to this purple segment, is the best line that maximizes the variance. Okay. Understood.

This is the basic logic of Principal Component Analysis, but you will not see this type of explanation in a textbook. In the textbook, you will see eigenvalues and eigenvectors, and transform the values. These are hard to understand what's happening behind the scene. This page gives you a very good explanation. This is given in your slides. I put it there. They worked on this to create a simulation. So they should get the credit. So I put that there. The first way is the one that I showed now. So they are giving a very good explanation on what's happening also in the subpage, okay? So when you get some time, read this page to get some more clarity on this.

Okay? So now we talk about the logic. But how to do it mathematically. That is what you normally see in the textbook, just some calculations from eigenvalues and eigenvectors, but no idea what is happening behind the scene.

So basically, I identify the directions in which the data varies the most, that means the most variance. Okay. Whichever direction gives the most variance for the data. That is the purpose. So for that problem, we centre the data and align with the directions of maximum variance. I will show mathematically how to do this.

First is standardize. We all know why we have to standardize. Why? Otherwise, if the variables are in two different scales, it doesn't make any sense. So we have to make sure that the values are standardized. And we want to find the relationships between attributes, that's why we have to calculate the covariance matrix, because we want to get these projections, right? At the end of the day, we're going to get these projections and the distances, and whichever gives the maximum value, that is what we want to find. But how to find it using linear algebra concepts? This is mostly linear algebra, the math of it, by finding eigenvalues and eigenvectors.

So we know correlation, like if it is one, what is the meaning of it? If it is positive, what is the meaning of it? Positive correlation. It's increasing or decreasing together. Two attributes, both are increasing. Height and weight together. Okay. There are some cases where one increases and the other decreases, that is negative. Okay? So both increase or both decrease together, that is positive correlation, and when one increases and the other decreases, that is negative correlation. So to find a covariance matrix, we use linear algebra concepts, which are eigenvectors and eigenvalues. Okay?

So we are not going to the proof of it, but the objective is we want to find a direction such that when these points are projected onto it, the variance will be the maximum, okay? That is the purpose.

One question about step two. If you find that they are correlated, shouldn't we remove them? No, no, no. We are not doing feature selection at this point. So we have some assumptions. Like they are not exactly identical to each other, but in reality, most of them have some correlations. Okay. But we'll keep those correlations, and the correlations will be reflected in the final calculation. Once you get this new axis, they will be uncorrelated.

So we have to do the math of this. I have an Excel sheet so that we can work on this. I took a set of values and we do standardization. I want to know the average, the min and the standard deviation, apply the standardization, then do the eigenvalue calculation, eigenvector calculation, and finally find the new points on the new axes. So we have to do the math of this, okay? But before going into the math of it, we will talk about LDA also, and then once we finish the four discussions for today, we can start the math of this. Otherwise, if I go with the math now, you won't have the energy. I want you to understand what's happening with LDA. Sounds good? Yeah?

So when you do the math of this, you may get more clarity about what's happening. But what I drew in this picture, this is two axes into one axis I showed, but in real life, we may have 10,000 axes. 10,000 dimensions means we have 10,000 axes. We are coming down to maybe 10, so we need to know how many axes we want to keep. Okay, that is the next thing. So the new axes we just created are linear combinations of the initial features.

So here, we have height on the X axis and weight on the Y axis. So on the X axis we see height, and on the Y axis we see weight. Okay. So we are reducing this height and weight into one axis. We don't have height information and weight information separately once we reduce it to one axis. Okay? Then one dimension, the new axis, may be something like "size." We can figure out what it represents. We can't always specifically tell what is the name. But basically, if it is height and weight, we can tell the new axis may represent the size of the person. Okay? Just as an example. But in theory, we just name this new axis as something.

So this is length and this is width. They call this Principal Component 1, okay? So if you have, in the case of Iris, how many attributes do we have? Four. Four attributes, right? So in the case of Iris, you know, four attributes. So if you have four attributes, in theory, you can find four components. Four principal components. Okay, but if you're keeping all four, then what's the use, right? You do not need to keep all four components. There is no point in doing all these math calculations, okay? So we have to find the optimal number.

So, Principal Component 1, Principal Component 2, Principal Component 3, and Principal Component 4. Okay. But we will check, just like in our earlier plots, we have something called a scree plot. Yeah, this is a scree plot. So in this plot, where the elbow is, we'll consider that as the best number of components to keep. So in this example, we can see the elbow is coming at a certain point, but two is also not bad, right? At two we are getting a good amount. But going from four to three, we are not reducing much.

So there is a tradeoff between accuracy and complexity when you reduce the dimensions. In most cases, you lose some information. But in the case of Iris, Iris is already well classified, right? Only Setosa is always very far apart from the other two classes. So we can see that accuracy is a bit increasing when you take the principal components, because the principal components are more synthetic values. Okay. So more robust values. So in Iris, the accuracy changes are very subtle. But in other datasets, like the one we are going to use in your lab, accuracy will change more noticeably. So initially it was 75% or so, and it might go down to 69% or 70% or 71%. We have to decide how much we can accommodate. From 75%, can we only go until 73%? Or is it okay if we go to 69%? For medical datasets, we don't want to lose a lot of information. So we may tend to keep 90% of information. For other datasets, even 70% is not bad.

So there are two settings. One, we can set how much information we want to keep: 70%, 80%, 90%, 0.83 for 83%, or 0.92 for 92%. We can decide how much percent of information we want to keep. Or we can specify how many components we want to keep. There is a parameter for n_components, I'll show that. We can decide how many number of components we want to keep. So either the number of components we want to keep we can set, or how much information we want to keep, that we can set. Makes sense? Yeah?

For four attributes, PC1 has four coefficients. That means one coefficient is like this, one is like this, one is like this, and one is like this, okay? So PC1 is creating a linear combination of all four original attributes. It can create four new axes, okay? But the math will ensure that the first axis will have the maximum information.

So in the case of Iris with four attributes, you can create four new axes. But the math will make sure that the first axis will have the maximum information. Second axis will have the next highest. So assume the first component is keeping 65% of information. Okay. The remaining is 35%, right? So the first component is keeping 65%. Then, if the second component is keeping 20%, that means the first two components have 85% of information. Okay. In that case, if you are looking for only 80% of information, you can just use only the first two components. You can skip components three and four. Yeah?

So, when you do the eigenvalue calculation, whichever component gives you the maximum variance, that is the best one. So all four axes have variances. So when you find the first one, the second will be orthogonal to the first one. And the third will be orthogonal to the first and second. And the fourth will be orthogonal to the first three. So first two are like this, third will be perpendicular to those, and fourth will be pointing in the remaining direction. Yeah?

When you say percent of information, is that a number you get through the eigenvalue calculation or how do you determine it? That is associated with the eigenvalues. We are going to work with a small example. We have only four or five data points and a few attributes, and we will work with the full calculation. Okay?

So even though four attributes go to four principal components, we have sepal length, sepal width, petal length, and petal width. So when you get PC1, we can't specifically name it. PC1 is the combined information of all these four. PC2 is information of all these four, but less information. PC3 has the remaining information, not overlapping. Are they overlapping? Like this one taking 65% and the other 20%, how do you know that it's not taking information from those 65% as well? So that's why they are orthogonal. So when we do the math of it, you may understand it.

So now we are going to talk about LDA. After discussing LDA, I'll give a break and then start the math part, okay?

So, when we talked about PCA, we haven't talked about the class at all. Okay? So these points, maybe from two different classes. Maybe these points are class one, and these are class two. Okay. So PCA is not considering the class information at all. It takes all the points and tries to find the axis that spreads the points the most. Class information is not considered in PCA, so that's why PCA is called an unsupervised technique. Okay? The class label is not considered, that means the technique is unsupervised.

But LDA, similar approach, but keeping the class information also in consideration. Okay? So in the case of PCA, we want to make a new axis such that the spread is maximum. In LDA, we want to make sure that the separability between classes should also be maximum. So we are considering the class information also in the calculation. So we want to make sure that the separation between classes should be maximum. And the points within each class, the distances for those should be smaller, they should be closer. Okay. So points from class one should be closer to each other. Points from class two should be closer to each other. And the classes should have maximum distance between them. Understood?

So basically what you are doing is you take the mean of class one and the mean of class two, and the distance between those two means should be the maximum. Okay, that means those classes are far apart. So I have my first class something like this. My second class is something like this. So the mean of this class would be somewhere over here, right? Yeah. And the mean of this class is somewhere over here. So I'm considering this distance, okay? Yeah. So whichever axis gives this distance the maximum, but with a condition: the same axis should also minimize the variation among the points of the same class. The same logic as PCA, but the difference is that LDA considers the class information also in the calculation. That is why LDA is a supervised technique because the class is also considered. Yeah.

So the main difference between LDA and PCA is: PCA does not consider the class label at all. We are trying to find the axis that maximizes the variances. In the case of LDA, because the class is also considered, we want to find the axis that maximizes the difference between the means of the classes, and at the same time minimizes the distance between the points in the same class.

So I have, if I create two new axes. This is axis one and axis two. So my points, in one axis, they are giving me a certain separation. And the other one is giving me something different. Which one is best? Why? Because in the better axis, these points are closer to each other, these points are closer to each other, and the distance between the classes is bigger. So in both cases we can see separation, but still the distances differ. For this class, this distance is D1, and for that class, it is D2. Okay. So D1 is bigger than D2, and two things are considered simultaneously: the within-class distance and the between-class distance. Understood? Yeah, that is the basic logic of LDA. Sounds good.

You guys need to read a little bit more, because it's not an easy concept. So more reading is required. I tried my level best to draw pictures and explain, but still you may need a little bit more clarity.

Can you say that PCA preserves the distances between the points, the spread, and LDA preserves the classes, the separability and the class cohesion? Can you use LDA for regression too, or does it have to have class labels for classification? Do we have to have class labels? Because if we have numbers, we don't have classes to separate the means, right? Can we use LDA here?

What I explained now is to reduce the dimensions. Correct. How to reduce the dimensions. If you have 10,000 features, you can find, maybe from 10,000, you can preserve 90% or 80% of information by keeping only 50 features. 50 new features. And those 50 features will be representing around 85% of the information. Instead of 10,000 features giving me 100% information, my 50 new features, new axes, are giving me 85% information. So the complexity difference between 50 versus 10,000, that's what I was explaining now.

So can we use LDA for classification? If you have a class, yes. But if you have a number, if it's regression, can you still use LDA? I have to think about your question. I haven't used it for that, so I have to think. I mean, if it's just about reducing dimensions, you would ignore the class in PCA. In LDA, we are keeping the class labels. In PCA, we are just finding the new axes.

So once you get the new axes, you have to find the points on the new axes. So you are getting principal components, meaning new axes. Now we have to find the points. The already existing standardized data is there. The standardized data has to be transformed to this new axis. So finding principal components means we are finding new axes. You are not finding new points. And then we have to transform.

So in practice, we have to initialize the object for PCA. Then we are fitting. There is a fit operation. Even for standardization, we have a fit operation, right? Yeah. What is the fit operation of standardization doing? Fitting? So, what is done by fitting in standardization? What do you want to calculate if you want to standardize some data? The mean and standard deviation. So fitting means you're finding the mean and standard deviation. That is the only thing that is happening when you fit a standardizer object onto a train set. Okay? And then you are transforming. The transform operation actually applies this mean and standard deviation, which you found during the fit operation, on the dataset to find the new standardized values. Okay?

So fit is just to find the mean and standard deviation in the case of standardization. So there's a fit, there is a transform, and there's a fit_transform. Three methods are there in Python. Three methods are there. Fit operation, we find the mean and standard deviation. If you're using fit_transform, it's basically, in the case of the train set, you can use fit_transform. That means you are finding the mean and standard deviation and transforming those values into the new standardized values. In the case of the test set, you do not need to find the mean and standard deviation. For the test set, you will be applying the same mean and standard deviation of the train set onto the test set. Right? So that means you are only transforming for the test set. Agree. At least, that is how it should be done. Even if you are doing it differently, that is wrong.

So the fit operation will calculate the mean and standard deviation, which should be applied on the train set. The mean and standard deviation should be calculated from the train set, and then we have to transform using the mean and standard deviation to find the new standardized values. Once you standardize, you will see that the mean will be zero and the standard deviation will be one for the new transformed values.

Okay. In the case of PCA also, when you fit, when you initialize the object for PCA and then you have to fit it, during the fitting time you're finding the new axes. You are getting principal components. That means you're getting new axes. Okay. And then to find the values, you have to transform the original values to the values of this new axis. That is done with the transform operation.

So when you do the lab, you will see everything: initialization, fit_transform, or initialization, fit, transform, transform. You need to understand what's happening during that time. Makes sense. Yeah. So you need to standardize first before applying PCA. When you do normalization, you get the new standardized values. So the way that I explained is passing through the origin. That means it has to be standardized. There are different variations of explanation for PCA. So there is mean centered and standardized explanation. There is some literature that talks about mean centered data, which means it is not fully standardized, just taking out the mean. Okay. But the way that I explained is working on the standardized data. That means it will be centred about the origin. So that means my line will be passing through the origin.
