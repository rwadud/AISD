We have two classes. So one, two, three, five points in class one. Next is class two. When we take the averages, we take the average based on class. So this mean is coming from the mean of these five points. We are not doing this in class, but the logic is like this. The objective is two. First, the distance between the classes should be the maximum. At the same time, points within the same class should be closer together. So we are considering the class, and we consider all of the points in each set. In each set is a class, like class one, class two. So LDA is also able to take the class into account. In PCA, you are just taking the components, not considering the class. In LDA, everything is class based.

So one thing I noticed is that I was randomly taking the subject. For the submitted lab one, you are not explaining. So what is this lab about? Like, what is the new thing that we learn in this lab? The new thing that we learned in this lab, what would you say? PCA, right? And it specifically mentioned the parameters. So a parameter set that specifies the number of components. So what are the possible values for that? What is the maximum for that? Can you set any number of components? What is the maximum for PCA? Yeah, the number of what? Number of features, technically number of features or number of samples, whichever is less. The rule that we normally work with is the lesser of the two. So the minimum of number of samples or number of features. That is the condition for PCA.

What about LDA? The maximum should be based on number of classes. So, if we want to have a separator between two classes, how many separators do we need? If we have two classes, and we want to separate them, how many lines do I have to draw? One, right? Yeah. So if we have just two classes, we have to draw one line. If we have three classes, two lines. So assume that we have one more class here. We may have to draw something like two lines. So we need to have a line to separate between these two and then two lines. Three classes, right? So basically the number of classes minus one. So if you have three classes, two components. Five classes, four components.

For this lab, every parameter should be explained. Here, you have to explain the number of components. There are specifics for that. If you do not explain, you will lose points. So you have to explain what that parameter is, what are the possible values. For LDA, the value has to be the minimum of number of classes minus one, or number of features, whichever is less. If we have two classes and eight features, then two minus one is one, and eight features. So what is the minimum, one or eight? One. That is the number of components you can create.

So do not forget to write these things in your lab, because finishing the labs is pretty easy. My objective is for you to at least try to document these things, because you will learn. This will be on the exam. In the exam, I will be giving some data with this number of features, this number of classes, this number of samples. How many PCA components can you make? So almost like a short question. The numbers will be different, but it will really be a question like this. I will be telling you, okay, we have five classes, 20 features, and some number of samples. How many PCA components can you make, and how many LDA components can you make? Without knowing these concepts, you cannot get the marks. So make sure that when you do this lab, go to the documentation, all these scikit-learn documentation, you have to make sure that you know those parameters.

So last week we did PCA. I went through the math of PCA. Try to do it by yourself. That will be a question. In lab two, I will ask you to do this with a specific set of numbers so that is my way of forcing you to do it. But if you decide you do not want that, fine, but you should know what is happening, right?

So today we are going to learn about classification. Support vector machines. So how many classification methods have we learned so far? Decision trees. And then logistic regression. Logistic regression is used to predict a value, like house price or some cost. But we can use it to perform classification. The value you get is sent to a function, a sigmoid function, and that function determines if the value is greater than 0.5, it is one class, less than 0.5, the other class. So we have that concept.

So this normally comes as an advanced topic. I will try my best to teach it in a very simple way. Let us see how it goes. It is not an easy thing.

So we have two classes. We have two classes, blue points and red points. So if I want to separate them, right, we have to draw a line. I can draw a line. And that line is the decision boundary. Based on lines, I can use a line to separate, right? I can draw a line. All these lines which I drew here are all valid. So in the context of support vector machines, in two dimensions, we just see a picture, right? But always think, we can have three dimensions, we can have a hundred dimensions. So it is not easy to draw and find the line.

So here, the concept is, assume that I just have these two classes. So I can draw a line like this, right? Is this the best line in the scenario? Why not? The blue points are safe on one side, but red points are not safe, because I could get a red point maybe somewhere over here. My next red point is here. So basically this is like an overfitted line for the red class. We could also say this is overfitting for the red class, right? So this line is not that great. The same type of line I can create for blue points also. In this case, my red points are safe, but my blue points are not safe. So you can get overfitting. I can have some line like this, but I need to find the right line. I have to find the line that is not overfitted, that is not biased.

For that, the concept that support vector machine uses is this: it is trying to create a margin. For example, I want to create a margin between these two classes. This is a possible margin, right? But if I put it like this, this is another possible margin. If I create my margin like this, you can see I have points like this. So my boundary can have a margin which is a little bit wider. I have another margin. So this is one I created. But now I am going to create another one. This one is a little bit wider than the first margin, right? More space to separate these points. Both the first margin and second margin can separate the blue points and red points.

But the objective of the support vector machine is, we have to find, we have to maximize this margin. We have to maximize this margin. So in the first one, my reference points are the closest points. So these are the points I use to create the first margin. And the next two reference points I use to create the second margin. So these are the points that are used as reference points to create this margin. So once I create this model, I only need these reference points. All of the other points are irrelevant in my scenario.

So basically, our objective is to create the maximum possible margin. And the points that help us to create that maximum possible margin are called support vectors. Support vectors are those points that help us to create a margin with the maximum width.

So now if you look here, this is the margin. So this is the biggest margin I can get. And these points are the support vectors. So the concept of support vector machines is simple if we explain it this way. In two dimensions, we want to create separation between classes. We can have multiple ways of separation. As I showed before, we can have different types of lines, all these lines are separators, right? The line that gives me the maximum separation between the classes, that is what we are trying to achieve.

So now, how can we find this margin? So simplification: in one dimension, it is simple. Because you have points in one dimension, the separator is a dot, right? If you have something like one dimension, and we have points, so this is my one dimension. And my points for class one are here, and I have class two points here. So my separator will be a dot, right? It is a dot in one dimension. When I have two dimensions, I have two axes, x-axis and y-axis. My red points will be like this. And my blue points will be something like this. A dot is not enough to separate these two classes, right? So I need a line to separate these two classes. When it comes to three dimensions, we should have a plane. More than three dimensions, we should have a hyperplane.

That is the nature of support vector machines. So the hyperplane is the separator between the classes. That is for the classification part.

So now, what are support vectors? Support vectors are the points that are closest to the hyperplane. Once we have the hyperplane, they are the ones that can influence the position. So comparing between these two scenarios, these are the support vectors. Support vectors can influence the position and orientation of the hyperplane. So if my support vectors are just these four, then my decision boundary, right, is based on them. Support vectors really determine the position and orientation of the hyperplane. So using the support vectors, we maximize the classification margin. In one dimension it is a dot, in two dimensions it is a line, and in higher dimensions it will be a hyperplane.

Is there a way to check whether the points are separable by a line or a plane or a hyperplane before starting the process? In real life, we will not get points like this. They will be scattered. So we talk about that a little bit later. So if we have more dimensions, we can use the techniques we learned last week, which is to reduce the dimensions or reduce to two and we can visualize. That is why we have to reduce dimensions. One reason of reducing is to reduce the complexity, and at the same time, visualization is very important. Unless we visualize, we have no idea how it looks like, right? Like how can we see whether the line is separating or not? It is not possible. We can find it by creating a line and finding accuracies. If we are getting good classifications, obviously that tells us. Before creating a model, we cannot find whether there is a separable plane.

What is support vector machine? It does classification. It is a supervised learning method. It has classes. That means it is most commonly used for classification tasks.

Now we are going to get into the math of it. My expectation is, what I teach you today, you will go and read up on it, and then come back on Monday and if you have questions I can go over it again. Maybe you will not get it in one stretch.

So we have these points. Red points, blue points. We are getting a new point. We are getting a black point. When we see the picture like this, we know it is the blue class. But that is not the case, we do not have the picture. Assume that we do not have the picture. How can we tell whether this point is on the blue side or on the red side? That is the task. To achieve that, we have a model already created. This green line is the model.

So we have this green line. In normal logic, we can think, can we find the closeness to the green line? The line itself has multiple points on it, right? If you want to find a distance between this point and the green line, you find the projection, right?

Just to show what the problem is. If you want to find a distance, we can have a line like this. We have a line, and we want to find the distance from the point to the line. We can also ask, how can we determine which side of the line a point is on? One way is to find a distance from this point to the green line. But where on the green line? We do not know which point on the green line to measure to.

So for that, the next step is, because we should have some criteria that should be applicable for any point anywhere in this dimensional space, not just for one point. Next time this black point could be anywhere. So it should be working for points anywhere in this dimensional space, or for vectors anywhere in the dimensional space.

So for that, the first thing is we will create a vector that is perpendicular to this green line. That is the mathematical standard. So I am going to create a line that is perpendicular to the green line. That is a standard statement I can make.

So this vector has a direction, because what is a vector, right? It represents a direction in space. So this is going to be called W. And this is the point X. So I assume that this is the perpendicular vector. And we have this vector created. And this is the point in question. We want to predict for this point. So I am going to create another vector for this point X also as a vector. So that means I should have both W and X as vectors.

So now, if I want to find the distance, I can find the measurement by projecting X onto W, right? Because I have a perpendicular reference, I can find the perpendicular distance. So I want to project X onto W. What is the projection formula? Dot product. So when I project X onto W, that is X dot W.

And I forgot to mention one thing. Once I create this vector, I have an assumption. My assumption is the perpendicular distance between the green line and the origin is some value C.

So this is not as simple as KNN or naive Bayes. You have to pay attention. Try a little bit to understand this. If not, we can go over it one more time, but do not get discouraged. There is math in machine learning. We cannot avoid math in machine learning.

So I am calling this vector W, and this distance C. The perpendicular distance to the origin is C. So I am going to project X onto W, which is the dot product, right? X dot W. If the dot product of X and W is equal to C, what is the meaning of it? If the dot product of X and W is equal to the value C, what is the meaning of it? X is on the green line. And if it is greater than C, that means it is on the red side. If it is less than C, it is on the blue side.

Let me see what we have so far. So if you consider red as the positive side and blue as the negative side. For the positive points, the equation would look like X dot W greater than C. For the positive points. So if X dot W is greater than C, we can take the C to the left side. That gives X dot W minus C is greater than zero. So far we just took the C to the left side. There is no sign change. So X dot W minus C is greater than zero.

Now we are going to represent this minus C as B. So that means it will be X dot W plus B, greater than zero. We just took this minus C and are representing it with another variable name. The reason is, in any line equation, we have like y equals mx plus b, right? So here this is a plane, so X dot W. We need to get in that format. That is the only reason we represent this minus C as a new variable named B. So our equation becomes X dot W plus B is greater than zero.

So now we can tell, if X dot W plus B is greater than zero, that is the positive class. So now we have a function that says Y equals plus one if X dot W plus B is greater than or equal to zero, or minus one if X dot W plus B is less than zero. So our task is to classify any new point.

Is it always a line or can it be a curve? It is not always a line. So I cannot draw it in higher dimensional space, so for the explanation, I am showing it in two dimensions. It is a plane. But mathematically, it is a plane. It would be straight, but it is not a line in higher dimensional space.

Just as in clustering, we assign the points to the cluster based on the distance to the centroids. The distance decides the class assignment, one or the other. So we use these equations. We are assigning either to the red or to the blue. We do not have a third class here. So one or the other, we have to assign either to the red or to the blue.

Do we recalculate the line after each new point? The green line creation and the prediction, all these things are happening together. The green line is the model that is created. So training is separate. And then for the predictions, the new points are predicted based on the green line. So the model is created during training, and once the model is finalized, then there is prediction.

How do we get the right model? There is a testing phase and validation. Creating the correct model is based on a trade-off between accuracy and complexity, right? So in one of the classes, we will talk about grid search and random search.

So we already understand this much. Now we have to find the right line. The core concept is about the margin. Our separator, our line, should create the maximum possible margin between the classes. Our objective is to create a line such that if two parallel lines are drawn through that line, we have the maximum margin.

So the margin, D, is the difference between these two lines, line one and line two. That is the margin. The width of this is D. But we want to find that specific line that maximizes D, maximizes the margin.

So now, I am getting two parallel lines. W dot X plus B equals one. W dot X plus B equals minus one. One on the right side, one on the left side, because my objective is to maximize the margin. So if I want to see whether this is maximized or not, I should have two parallel lines. One question you can ask is why is the value one? Why not five? You can. It would be a multiplier. It does not make a practical change, just like the one I showed last week for the PCA calculations. We were checking the eigenvalues. So for the easiness, for the convenience, in literature you will see the word "convenience," we are going to use the value one. So line one is W dot X plus B equals minus one, line two is W dot X plus B equals one.

So for the red points, the condition will be it should be greater than this line, right? And for the blue points, it should be less than this line. When we draw these two lines, for the blue points it should be lower than line one, for the red points, it should be higher than line two. So that condition can be written like this: Y_i times (X_i dot W plus B) is greater than or equal to one. Because the correct points should clearly satisfy the condition, and we are writing the same condition by multiplying by Y_i. Y_i can be one or minus one, based on the class. So instead of writing these two equations separately, we write it as one equation, but we put the conditions like this. Y_i equals one for a red point and Y_i equals minus one for a blue point.

So now we have two planes. L1 and L2 are two planes, and what we are trying to maximize is the distance between these two planes, right? We want to find L1 and L2 such that the distance between them should be the maximum. We can have many different L1 and L2 pairs. We can have slightly tilted ones and another pair.

So we want to maximize the margin band. The equation for the distance between two parallel planes: if the planes are W dot X plus B equals C1 and W dot X plus B equals C2, the formula is the absolute value of C2 minus C1 divided by the norm of W. So here, our lines are equals one and equals minus one. So the distance will be one minus negative one, which is absolute value of two. So two over the norm of W, which in the case of W becomes the square root of W1 squared plus W2 squared, and so on. So the distance formula is two divided by the norm of W.

So this is a formula. You do not need to prove it. In math classes, if you are doing advanced math, you will prove this formula. But for us, we should know that this is the formula. If we have two planes in this format with the same W on both sides and same B on both sides, C is minus one and plus one, then the formula would be like this. It is basically the distance between two parallel planes.

So our objective is to maximize this distance, right? The objective is to maximize the distance, subject to these conditions, the conditions we already covered. For red points, they should be above the upper line. For blue points, they should be below the lower line. That is the condition. Given that condition, we want to maximize the distance.

So the goal of training is to maximize this distance, two over the norm of W, subject to the constraints. One class should be above, one class should be below the corresponding line. That is the objective for the linear support vector machine, if the class is linearly separable.

So this is called a hard margin. We are not accepting any misclassifications. This is called a maximum margin classifier. We are maximizing the margin. We are producing a classifier by maximizing the margin. That is called a maximum margin classifier. And here, hard margin means we are not accepting any misclassifications.

So when we have X and Y axes and a few dots, the color is the class. And the X and Y represent the features, like length and width.

What about if you have two features but three classes? So it would be class one versus class two, class one versus class three, class two versus class three. This is called one versus one. We get a separate line for each pair.

In real life, classes are not linearly separable. It is very, very rare to have that scenario in real life. So when you look at the data, you will have overlap. That means the linear separator will not work perfectly. We have to find a way to work with non-linearly separable classes.

So if the classes are not linearly separable, how can we create a separator? One option is you can go with a linear separator but accept some misclassifications. You use the same concept of the margin, but we will set how much misclassification we are willing to accept. So at that time, we have one more condition. Not just the maximum margin, at the same time, we have a value for how many misclassifications we are willing to accept. Maybe two percent, or one percent, or five percent. So we can set that value.

So in this case, on one side, I might have three blue points misclassified. And on the other side, two red points might be misclassified. But by some reason, if this is the best margin I can get, I go with it. I should have the condition set for how many misclassifications I can accept.

The previous condition is still there. We still want to maximize the margin. We have to find a plane with the maximum margin, provided we can accept a certain number of misclassifications too. That is called a support vector classifier. The previous one was a maximum margin classifier. In the maximum margin classifier, we are not willing to accept any misclassifications. The classes should be perfectly separable. What we are talking about now is the support vector classifier, which means we are going to use a linear classifier, but we are willing to accept some misclassifications. How much misclassification we are willing to accept, we set that. And based on that parameter, the margin will be created.

Now, there is another case. In some cases, there is no way to create a linear classifier. No linear separator will work. So what can we do? In this case, the approach is: can we project these data points into some higher dimensional space so that once it is projected, there will be a separation between the classes? The objective is to project the points to higher dimensions. Currently, two dimensions are not enough. My objective is to project this into higher dimensions. I do not know how many dimensions I need. Maybe 20, maybe 200, that is not important. How many dimensions I need to project into, I do not know. But the idea is that I will do this projection process until I find a dimensionality where I get a separation between the classes. So basically, I am trying to make a non-separable problem into a separable problem by projecting to a higher dimensional space.

But projecting to a higher dimensional space is computationally very expensive. In real life, we may have 100 features. Projecting 100 features to an infinite number of dimensional spaces is highly complex and has a high computational cost. It is not just squaring or cubing a value. Higher dimensions means potentially infinite number of dimensions. So it is not computationally efficient.

So for that, there is something called the kernel trick. The kernel trick, in plain language, is basically taking the concept of the dot product. The assumption is that the kernel function will find these values without specifically projecting all these points to higher dimensions. Without projecting to higher dimensions, the kernel function will do the same activity or give the same value, the same answers. So it is doing the math behind the scenes.

So for example, if I have 10 features, maybe the kernel function is working so that I will get the same results as if I projected these points into a 10,000 dimensional space. But I am not actually projecting these points to 10,000 dimensional space. The kernel function will do the work.

So our understanding about the kernel is: the kernel function will make sure that it does the dot product calculations and gives the results, the same results that you would get if you projected into higher dimensions, without actually projecting. That is the kernel trick.

So there are multiple kernels available. If we go to scikit-learn, this is the parameter. If we set the kernel to linear, it will not do the kernel transformation. It will just create a linear classifier. But we have other options. We have the polynomial kernel, which can create polynomial separators. And we have the radial basis function, RBF. The polynomial kernel creates a curve. If it is quadratic, you know how a quadratic function looks like, right? For higher dimensions, a curve of that type will be created. You can have a polynomial separator.

But the most widely used is the radial basis function, RBF. This will make sure that until the classes are separable, the function works as if it is projecting to higher dimensions until the classes become separable. That is the foundation.

In the case of polynomial, you are specifying the degree. You have the option to specify the degree. But in the case of the radial basis function, it will do the work until we get a situation where the classes are separable. So RBF will make sure of that. But it is very expensive. It is widely used, but make sure you evaluate whether you need to use it.

If we check the documentation, the default value for the kernel is RBF. Here is one example using the wine dataset. If I use a linear kernel, I am creating a linear support vector classifier. So if I am using a linear kernel, for this dataset after LDA, the linear separator is enough, because this is after LDA with three classes. So with three classes, we are separating them. These two points, these two points, and these four points are the support vectors. The rest of the points are not support vectors.

But when we use RBF, the radial basis function, it is finding points around the class. We can see these points around the boundaries. We need to know these boundary points. There is no need to know points far from the boundary, because they are not relevant to the decision boundary. But if the data is not linearly separable, for example if we have something like a circular arrangement, we need the radial function to separate classes like this.

If I compare linear, polynomial, and RBF, and look at the support vectors: the linear one uses just a few support vectors, maybe five or eight points. Eight points can make a very good classification, because they are proper boundary points. Polynomial creates more support vectors. And RBF gets points around the classes. In a linearly separable case, the linear kernel with fewer support vectors is the best.

Regarding the kernel projection: the RBF kernel will project to potentially an infinite number of dimensions until the classes are separable. It does not matter about the original number of features. This is about the projection space, not the features. If we can separate the classes in five dimensions of the projected space, that is about the projection, not the features. The number of features and the projection space dimensionality are two different things.

So the objective is, if we are doing the projections, we will do the projections to three, to four, to five, to six, to seven dimensions until whenever we get the classes separable. We stop when the classes become separable. It can go to infinity. That is why RBF can be very expensive.

We talked about the degree for the polynomial kernel. We will talk about gamma and C later. There are multiple considerations. We are willing to accept some misclassifications. It is not a hard margin. With the kernel function, we are willing to accept some misclassifications, which we are going to set. We do not want to overfit. We do not want to underfit. Both are not good.

Overfitting means the model is performing 100 percent well for the training data but when new points come, it cannot perform well. That means the borders are very close to the existing points. It is just accommodating that specific set of points. That is overfitting. We do not want that to happen. At the same time, we do not want the margin to be too wide, performing poorly. Even the training data is not performing well. Then what is the purpose of that model?

So there are multiple conditions: margin should be maximized, at the same time we are willing to accept some misclassifications, at the same time we do not want to underfit and we do not want to overfit. All these factors are there when we create this model. It is not a single consideration. And in some cases, we do not really need support vector machines because support vector machines work really well for complex data. For simple data, it is not a wise thing to use support vector machines. You may get even less accuracy than other simpler methods like naive Bayes or logistic regression.

So basically this is what we learned so far. The maximum margin classifier has a hard margin. We are not ready to accept any misclassifications, so that means the classes must be linearly separable. The support vector classifier means we are willing to accept some misclassifications, but the classifier will be linear. It will be a hyperplane that we are going to create. The support vector machine is the support vector classifier plus a nonlinear kernel. So that means we can have RBF, we can have polynomial, we can have sigmoid, different kernel functions are available. But we have to decide which one to use. There is no shortcut to decide which one is the best. If you can visualize, you can get some idea. Otherwise, we do not know how it looks like in a 50 dimensional space.

So now let us talk about the parameters. We do not want to overfit. We do not want to underfit. How can we control that? There are two parameters: C and gamma. In some literature you will see C referred to as the regularization parameter. C and the regularization parameter are inversely proportional to each other, one is one over the other. C is the value that we set to represent how many misclassifications we are willing to accept. If you are willing to accept more misclassifications, that means a smaller C value, higher regularization, which means you are making a wider margin for the hyperplane. So there can be some misclassifications, but if you put a large C value, it will try to make sure that we are predicting mostly all training data correctly, which leads to overfitting. Basically, C controls how many errors we are willing to accept.

And gamma. In plain language, if you have data points, gamma controls how close the decision boundary should be to the class points. That is what gamma controls. It is about how well the model fits the training data. A lower gamma value leads to underfitting, because it is not fitting well. If it is a higher gamma, it is fitting too well, which means it may end up overfitting, because it will work very well for the given training data but when new points come, it may not perform well.
