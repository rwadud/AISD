So this is another kind of sequence data processing task. When we use a translator to translate text from one language to another language, it's not a word by word translation, right? It's not. So how does it work? How can it handle this kind of problem? Or Siri, or Google Assistant, converting spoken words into text, or speech to text, or understanding text, or vice versa. Or AI composing or generating background music. Or how about predicting future prices based on historical trends? This is related to time series. Predicting the stock price, the future price based on the past history.

We need a model to solve this kind of problem. We need a model to handle sequential data. As I said, the MLP or feed forward neural networks are not able to handle sequential data. Also, the model must be able to consider the current input and also the previously received inputs. When we are talking, to predict the next word, first of all, I should know what I said right now and what I've said before. So we need a model that has the ability to take into account this kind of information. Also, it should be able to memorize history in its internal memory. So we need to have memory.

So what is the solution? The solution is a recurrent neural network. A recurrent neural network is something that you can see here. Just neglecting this loop or this cycle, you see just a simple feed forward network. But in this one, this cycle means that it can have a loop, or it can keep the previous information as well. And if we unfold this architecture, you will have something like this. So let's say we have a text with tokens or keywords. The first word, we will give the first word to the first model, the first feed forward or MLP, whatever. So it's going to generate a hidden state. Then the second word, to process the second word, we will give it to the second network. But the second network also gets something from the previous system to generate its own hidden states. And similarly for the next step as well.

When do we use MLP or feed forward? What is an example of using a feed forward network? For any kind of classification problems that you don't need to have information from the past data. For example, predicting the house price, or predicting the gender of people. These are some static data, the features are static. There is no relation between different features. You can use the MLP for those kinds of problems.

For each input, totally, as I said, the MLP or any kind of model can be placed here with any architecture. It takes the input from the input and the output of the previous steps. What about the first system? For the first system, the input comes only from the input. So what about the previous state? Since here there is no previous state, we don't have it. So we're assuming that the previous information is zero. So it means that we have it, but it's zero.

Feed forward networks are not able to process sequential data. Now, let's have a look at some uses of sequential data. First of all, speech recognition: audio to text, or vice versa, text to speech, whatever. Sentiment analysis: a sequence of text to a number of stars, or just simply, instead of rating, we can have something like classifying the text into positive, negative, or neutral. Or DNA sequence analysis, to find patterns. Or machine translation. Any task that is involved in processing a sequence, either in the input or in the output, can be considered a kind of usage of sequential data. Or video. And finally, time series forecasting, predicting the next value based on the previous values. For example, during the past week, Monday we had snow, on Tuesday we had snow, on Wednesday we didn't have snow. So what would be the prediction for Thursday?

Before diving into the RNN and its architecture, let's have a look at time series. What time series is and what you can do with time series, and also the components, the important components of time series. When we talk about time series, the definition: time series is a sequence of data points collected or recorded at specific time intervals. Let's say every week, every day, every hour, every minute, every year. Unlike static or fictional data, time series focuses on one or more variables over a duration. For example, during the past week, we are only looking at whether we are having snow or not. Or we can have more variables, like having snow plus temperature, plus the speed of wind. To plot the time series, the X axis almost always represents time, like days, weeks, years, whatever the intervals are, and the Y axis represents the variable you are measuring, like price, temperature, population, depending on your task. The goal in time series is to understand the past, what we had before, and predict the future, predicting the values in the next step or next interval.

Something like this. Let's say this is an example for time series of airline passengers. During this interval, every two years, we have the number of passengers. They are collected from 1949 to 1996. It is just about the number of passengers.

For time series, we have four components. Every time series has four components, namely: trend, seasonality, cycle, and noise (or sometimes called variation or residuals as well).

What is the trend? The trend is the long term direction. For example, here you see it's increasing, we have the increment. So it's increasing over time. Is it a trend? Yes. In general, it is generally going up or down or staying flat. The general view on the time series.

What is seasonality? It's a pattern that repeats over fixed periods or fixed intervals. For example, you see here, in each interval, we have some repeating patterns. We have something similar here, and here. This is seasonal or seasonality.

Then we have cycles. The cycle is similar to seasonality, but with one difference. For seasonality, we are looking at short intervals usually, but for cyclicity, the interval is longer. For example, for seasonality, we are looking at every two years, but for cyclicity, we look at every ten years to find the pattern.

And finally, noise or variation or residuals. These are some random fluctuations. Random values not under control, data that can't be explained by the other three. We cannot explain this noise, it's just noise.

When we talk about time series analysis or forecasting, usually it means that we are trying to predict the future. To predict the future, we need to first find the pattern. The usage of the pattern is for predicting next values. In that case, in time series analysis, usually important things for us are finding the seasonality and trends. Not noise. Sometimes we can also have a look at finding cycles, but most of the time, we only care about those two. And for noise, we don't care about noise. It doesn't have any meaning to predict the noise. Why should we predict the noise?

For example, if we assume that this is our original plot, our original time series, we see the trend is the green line. This would be our trend. This would be the seasonal component. And these are the noises or the residual variations.

Let's have a look at recurrent neural networks. An RNN is a kind of learning model that takes the previous output or hidden states as input. It means that the input at time T has some historical information about the happenings at all times before time T. RNNs are useful because their intermediate states, their hidden states, they can store information about the past for a period of time (it is not fixed).

In an RNN, each input is a word vector. You know what a word vector is? It's the numerical representation of a word. In your NLP course, when working with NLP, you are working with text, right? But the computer only knows numbers. You cannot give it the text or token directly. You need to convert them to a vector, to a numerical representation. The numerical representation of a word is known as a word vector. So typically, they are fed into the network one at a time, not all at once. We give it one at a time: first this one, then this one, then this one.

The difference between feed forward networks and RNNs: visually, we can look at it here and compare these two. This is a very simple feed forward neural network. We have the input, the hidden layer, and the output layer. We give the input, and we are supposed to learn the weight values for each hidden link. If you have another layer, we should have several weight matrices. But this is a simple feed forward network.

For the RNNs, we have the same things, but with repetition of the same architecture, each for one part of the input, like the token or word vector. And each feed forward network in the RNN will get an extra input from the output of the previous one. Since we have another input to the network, it means that we need to learn additional weights for those new inputs, W_h. And similarly for the other types as well.

If you have multiple layers, multiple layers, maybe the next step is going to take the output of the previous feed forward. It means that it's going to take the last hidden states from the previous feed forward. Because of that, I said each block here can have any architecture, but the final state is important for us.

This is the detailed representation of the RNN cell or RNN block. We have the input, we have the hidden layer or hidden state, so we need to learn W_x, and also for the output layer, we need to learn the weight W_y. If we unfold this block, we will have something like this. For each block, you have the input with the corresponding weights. The output of each block will pass into the next block as input. In the next block, we will have W_h, W_x, and W_y. The output of the hidden layer, which is included, passes to the next one, and so on.

The only thing you should know is that all these weights, the W_x values, are shared. It means that the W_x weight matrix here is exactly the same as the weight matrix here, exactly the same as this one. And W_y, again, here is exactly the same as this one.

To find HT, the hidden state at time T, we have this formula. What is F here? F is the activation function. So W_x times X_T, the input times the weights, plus W_h times HT minus one. HT minus one is the hidden state from the previous step multiplied by its weights. Plus theta, the bias. The hidden state at time T equals the activation function of W_x times X_T plus W_h times HT minus one plus theta, where theta determines the importance of the present information and the weight matrices control how past and present inputs contribute.

Let's have a look at, for example, image captioning. You know what image captioning is? Assume that we have a model, we give it an image, and we want the model to describe it or to generate the caption for that image. The input should be an image, and the output must be multiple words, a sentence, paragraph, or description. Is it the same as data annotation? It's kind of data annotation, but let's say it's not really annotation. Because usually when we say data annotation, the annotation must be done by a human. But here, we want the model to generate the description.

So, for example, the dog is hiding. First of all, our input is an image. And we output a text, a sequence of text. We have a neural network, and we know that neural networks only get float numbers as input. But we have an image here. How can we convert the image to numbers? You could say it's the intensity of the RGB values, but if you go that route, the input size would be very huge. So instead, we give it to a CNN, then we take the features from the CNN.

We can first give the image to a CNN. You've used CNN for image classification, right? At the last layer, we have something like a dense layer. If you had three classes, the last layer had three neurons. This layer can be any size, any dimension. We can say the output of this layer is the input for the RNN. It's the encoding of the input image. You gave your image to the CNN, you got the encoded version, just a vector. Now you can give this vector to the RNN. The first RNN will generate the first word. Then you give the second hidden state, and it will generate the next token. Similarly for the next one, up to the end, to receive something like "the dog is hiding" or whatever.

In terms of the input, the input for the first RNN is this vector from the CNN. Then for the second is the hidden state that was generated. We only gave the RNN the hidden states from the previous steps. But what about the input? We can also look at it another way. The input for each RNN step would be the word vector of the previously generated word, used as a character to process. We generate the first word. The first word would be the input for the second step, plus the previous hidden states from the previous system. Now I have the second input and previous hidden state, and I generate the second word. Now I give the second word as input to the third step, and the hidden state from the second step as another input. Similarly, you can repeat it until you meet the stopping conditions.

For using deep learning models, there are several scenarios. We have single input, single output. In this case, we are using the feed forward neural networks. For example, predicting the price of houses. You have one input, a set of features, and only one output, the price. This is single to single.

We have single input, multiple output scenario, like image captioning. We only have one input, one image, but a sequence of words. This is single to multiple.

We have multiple input, single output scenario, like text classification or sentiment analysis. You have a sequence of words, and at the end, you want to have only one prediction, positive or negative.

Also, you have the multiple input, multiple output scenario, like machine translation. Translating English to French, we have a sequence of English words, and in the output, a sequence of French words. That's a multiple to multiple scenario. Or video captioning, it can also be considered a multiple to multiple scenario. Because you have a sequence of still images (video is a sequence of images over time), so multiple input, and the caption is multiple output.

When we have a picture, we can classify it as a dog or a cat from a CNN. But for the RNN, is there any process that matches different pictures with different captions and then learns through that, or how do they do this? Yes, for training image captioning, definitely you must have training data, the pairs of images plus corresponding captions. When you want to use them, you have to convert the images and also the descriptions or captions into numerical representations.

Another thing that I believe you are already familiar with is the loss function. What is the loss function? The most important thing in training any model is the loss function. When we say we are training something, or even for ourselves, we are training ourselves to learn something, how can we evaluate how well we trained the model? We must have a metric. We must have a metric to find this gap between what we expected to have and what we have in reality. The difference between the expected output and the real output or the predicted output must be measured somehow. The role of the loss function is only measuring this gap between the real output and the expected output.

What's the difference between the real output and the expected output? The expected output is what you labeled, versus what you get from your model. For example, the expected output is here, but the real output, the predicted output, is here. How can I measure this gap?

We use the loss function for training, not testing. Why are we not using the loss function for testing? Because we try to train a model. Our goal is to train a model with any algorithm, and then use the trained model to apply on the test data. As a developer or machine learning engineer, I try to develop a model, train a model, and then after I train it, I will give it to someone else and say, "Now you can use this model." The customer should not be worried about the loss function, hidden states, batch iteration, learning rate, or training details. That's why we separate training from testing.

The loss function is a metric used to evaluate how well an algorithm models the given data. It's an approach to finding the gap or distance between the real output and the expected output. This gap between the real output and the expected output is also known as error. When we talk about error, it is always this gap between what I wanted it to be and what it is right now. Also known as cost function or error function.

There are several well known loss functions. In machine learning and deep learning, in general, they are in three categories: regression losses, probabilistic losses, and hinge losses.

For regression loss functions, you must be familiar with them. For regression tasks, we use regression loss functions like the mean squared error or mean absolute error. Mean squared error, also known as L2 loss: the average of the sum of the squared differences between the actual values and predicted values. Mean absolute error is similar, but we don't consider the squares, we use the absolute differences instead. This is also known as L1 loss or Lasso.

Then we have the probabilistic loss functions. The MSE, MAE are for regression tasks, where we are predicting a real value. For those real values, we can use those loss functions. But if the output of our model or network is a probability distribution, to find the difference between the expected probability distribution and the real probability distribution, we use probabilistic loss functions. Most of the time, we use cross entropy loss functions. If our task is binary classification, we use the binary cross entropy loss function. If it's a multiple class classification task, we usually use categorical cross entropy loss function, if the classes are encoded as one hot encoding. Otherwise, if the labels are labeled as numbers, we use sparse categorical cross entropy.

Then there is the hinge loss. We use it most of the time for traditional machine learning models, like SVMs. The labels here must be in the form of minus one and plus one for binary classification. For multi class classification, there is another type of hinge loss known as categorical hinge. It can also be used with neural networks, but most of the time, we use the cross entropy loss functions for training our models.

To predict Y hat (the predicted output), having Y (the expected output), we can find the loss, the difference between them. The loss would be a function that takes the expected output and the predicted output.

To apply the gradient descent approach, to update the weights, we use gradient descent. Gradient descent is the approach we use. The new value of W equals the current W minus alpha (which is the learning rate) times the derivative of L (the loss function) with respect to W.

To find this derivative with respect to W, we use the chain rule. The derivative of L with respect to W equals the derivative of L with respect to Y hat, times the derivative of Y hat with respect to W. This is known as the chain rule.

For the chain rule: if we assume Y equals F of G of X, we can show it this way. X goes into G function, then into F function. F takes the output of G, so F of G of X. Now if I want to find the derivative of Y with respect to X, I can say it equals the derivative of Y with respect to F, times the derivative of F with respect to G, times the derivative of G with respect to X.

If you have multiple layers, at the end of the first layer we have Y hat one (prediction for the first layer), at the end of the second layer we have Y hat two (prediction for the second layer). Y hat one equals F1 of X and W1, Y hat two equals F2 applied to F1 (these F1 and F2 can be the same or different activation functions). The loss function would be the difference between the expected output and the predicted output. If you want to update W1 and W2, we need to take the gradient of the loss function with respect to W2 and W1 and update them according to the formula. For example, the new W1 equals the current W1 minus alpha times the derivative of the loss with respect to W1, and similarly for W2. Again, we can apply the chain rule to find these derivatives.

This is the approach that we use for normal feed forward neural networks. But for recurrent neural networks, if you remember, we had something like this. We have the architecture of the RNN, and this is the unfolded version. The RNN can be considered as several feed forward networks that are related to each other. It means that the backpropagation will be more involved. It would not be something fixed for just one feed forward network. We need to apply backpropagation on all of these networks. This is known as backpropagation through time.

For example, if our final output is here, then we need to apply backpropagation from here up to the beginning. Depending on the task you are solving, the way of computing this backpropagation through time would be a bit different.

For example, if you are doing text classification, and we assume that each of these steps is one token, one word, the final output would be just a label. You will need to compute the backpropagation from the output all the way back to the first step.

But if your task is something like part of speech tagging or named entity recognition, where for each word, you assign a tag (for example, whether each word is a noun, verb, adjective, or adverb), then in that task, for every feed forward network here, at the end you will have an output. In that case, you should first save the loss of the first one, the second, the third, and so on, and backpropagate from each one. Then backpropagate from here to here, backpropagate from here to here. Then take the average of all the updates on the weights.

Depending on the task, you should do multiple backpropagations. For a classification task, you only need to do one backpropagation from the output to the beginning. If it's part of speech tagging or named entity recognition where you have an output for each step, then you should do the backpropagation several times through time and update the weights, and at the end take the average or sum of those updates to update the weights.

So, backpropagation through time: in a normal neural network, we use backpropagation to update weights by calculating gradients layer by layer. In an RNN, the same weights are used at every step. The network is unrolled across time, so backpropagation through time means we compute gradients across all these time steps and update the shared weights. The weight updates are computed for each copy of the unfolded network, summed or averaged, and then applied to the RNN weights. The weights for the input and the weights for the hidden states are shared. After backpropagating several times depending on the task, when we find the updates for each weight, we take their sum or average to find the new values for the weights.

For forward propagation in a simple RNN with three steps and three inputs: at the first step, it takes X0, with the initial hidden state set to zero, and it generates the first output. Assuming that here we have a named entity recognition task or part of speech tagging task. For the second step, it takes the input from the second time step plus the hidden state from the previous step. Similarly for the third step, it takes the third input plus the hidden states from the previous step.

If we have H10 here, and one of the inputs for that is H9, which kind of information does H9 include? The information from only the last word, or all the previous words? All of them. Because this carries the information from the first step to here, and this carries the information from the second step. The information here also includes the information from the previous steps. So this one carries the information from both, and similarly for all others.

For the backpropagation through time: the derivative of L at time T with respect to the first layer H1 would be the derivative of L at time T with respect to the predicted output at time T, times the derivative of Y hat T with respect to HT, which can again be unfolded using the chain rule.

Now, the problems with the vanilla RNN. If you remember, with backpropagation, when we use the chain rule to unfold the derivatives, we end up with multiplication of several terms. For example, the derivative of L (the loss function) with respect to the output at time T, times the derivative of Y hat T with respect to HT, then the derivative of HT with respect to HT minus one, and similarly, up to the derivative of H2 with respect to H1. We are using the multiplication of several values. The length of this expression depends on the number of steps in our RNN. If we have an RNN with three steps, this chain would have three terms. If we have 100 steps, we will have at least 100 elements in this expression.

Usually, these values are small, because they are the outputs of derivatives of sigmoid functions. When you take the derivative of the sigmoid function, the maximum value would be 0.25. When you multiply several small values together, the result approaches zero. If this value is zero, there will be no change. The gradient vanishes. If this value equals zero, the new W would be exactly the same as the old one. There would be no training, no learning.

In other words, if we have very long sequences or a very large number of steps, applying backpropagation on these RNNs would prevent the previous layers from learning anything. This is known as the vanishing gradient problem. Through time, the gradient disappears for long sentences.

As an example, if we use the simple vanilla RNN to process long sentences, they are not able to memorize or recall any information from the previous steps. For short sentences, like "I live in France, and I speak ___," the model can simply predict "French." The length of the sentence is short, it can recover from previous steps. It is living in France, so the language is French.

But if we have very long sentences, like "I live in France," then a long story with many paragraphs, and then "Now, I speak ___," the model will be confused. This is because of the vanishing gradient problem.

On the other side, we have another kind of problem known as the exploding gradient problem. The vanishing gradient was about the gradient disappearing through time. For the exploding gradient problem, we have the reverse situation. We have very long sentences, the values of the gradients are large, and multiplying large values several times together results in a very large value. Again, the model will not learn anything good. This is the exploding gradient problem. In summary, the product of several numbers can shrink to zero or explode to infinity, which causes the backpropagation to break down.

In the vanishing gradient: the gradient becomes extremely small as it propagates backward. The first layers receive almost no updates, and the network fails to learn long term dependencies. And similarly for the exploding gradient problem.

What are the solutions for these gradient problems, vanishing or exploding? First, instead of using the vanilla RNN, we can use gated RNNs, more sophisticated architectures like the LSTM or GRU.

Also, we can apply gradient clipping. When we see the amount of the gradient exceeds some threshold, we can clip it, saying it should stay within a certain range.

Instead of using sigmoid functions as the activation function, we can use the ReLU function to prevent the vanishing gradient problem.

We can also apply layer normalization or batch normalization. In batch normalization, when we train the model batch by batch over the training set, in each batch, we normalize the output of each layer for each batch and then give it to the next layer to process. This is batch normalization.

Another solution is using shorter sequences. This is not a general solution because sometimes we need to work with long sequences.

The most popular solution to avoid this problem is to use transformers. You will see transformers in your NLP course. The transformer is the best way to deal with this problem. As evidence, you all have experience working with GPT models, ChatGPT, or LLMs. You know that you can give them very long documents and they can understand and process them very easily without forgetting about the information at the beginning of your text. That's because of their ability to handle very long sentences.

Now, variations of the RNN. The first important variation is the LSTM, which stands for Long Short Term Memory. For LSTMs, we consider two types of memory: one for long term information, to keep information from the very previous steps in the sequence (long term memory), and one for short term information, from the very recent steps.

This is the overall architecture of the LSTM cell. It works based on gating. In LSTMs, when the LSTM receives some information, it first tries to find which part of this information must be forgotten (it's not necessary to keep). This is named the forget gate. Then there is another gate to decide which part of the information must be emphasized.

In each step, we have these gates. The yellow boxes can be any network. Sigma is sigmoid and tanh is the hyperbolic tangent function. The circle with the X is the element wise multiplication (multiplying each element by another element, like vector element wise multiplication). The plus sign is addition. The line represents concatenation, and there is also copying.

We have two types of information to be kept in the memory of the LSTM. One from the very long past, and one from the recent past. CT minus one is the context from the past, this is long term memory. It takes CT minus one and HT minus one: long term and short term. Information from the distant past, and information from the recent past, plus XT, the current input.

The LSTM has the ability to remove or add information to the cell state. How? By using gates. To remove some information, it learns weights. We use those weights to say which parts of information must be forgotten, must be removed. If the weight is zero, it means that part of information must be removed.

For example, if we have a vector with values 1, 2, 3, 4, 5, and a gate vector with values 0, 0.1, 0.8, 0, 0.8, and we do the element wise multiplication, the result would be 0, 0.2, 2.4, 0, 4.0. What does this mean? It means we don't need this part of information. But this part of information must be emphasized. And this part of information is not that important, but we don't want to remove it completely, just decrease its importance. Using gates, we do this kind of operation.

Gates are composed of a sigmoid neural layer and element wise multiplication. We also need to learn the weights for the gates.

Step one is the forget gate. It decides what information to throw away from the cell state. Given the input and information from the previous step, we concatenate them, and it will generate the forget gate output. The output of the forget gate: FT equals sigma of (WF times the concatenation of HT minus one and XT, plus bias). If the gate value is one, it means keep that part completely. If it's zero, it means neglect that part of the information.

Step two: we decide what new information we are going to store in the cell state. HT minus one has the information from the recent steps, the recent history. But CT is the long term memory. At the second step, we should decide what information we should add to our long term history. The input gate decides which values will be updated and by how much, and it creates a vector of new candidates.

For the long term memory, for example if we have "I grew up in France" followed by a long passage, in its long term memory, the LSTM keeps that information about France. Then we use that history, that memory, to predict later.

For updating the cell state, we use this formula: CT equals FT times CT minus one (which determines which information must be removed from the previous state) plus the input gate output times the new candidate values (which determines which new information must be added). And that's it.
