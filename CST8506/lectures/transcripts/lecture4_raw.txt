Oh, what you are typing, what you typed before, right? Or how to release how translators, converting sentences from English to French. So this is another kind of sequence. Data processing task, okay. So when we say, okay, so when the translator, when you use the translator, to translate one language, text from one language to another language, okay, it's not the word by word translation, right? Or it is. It's not. Okay? So, but how it works? How it can handle this kind of problem? or hostility or Google Assistant, converting spoken word into the text or speech to text. Okay, or understand text or vice versa. Our AI compose Middle East or generates background music. Okay, so how it's supposed to watch a predictive feature price based on historical trends. So this is what is related to what? The really one included Thai series. Okay, so correcting the stock price, the future price based on the, you know, the past history. So... We need a model to solve this kind of problem, we need a model to handle sequential data. As I said, the MLP or feed forward neural networks, we are not able to handle sequential. data. Okay? Also, the model must be able to consider the current input also the previously received improves. Okay, so when we are talking, 1st to process the, to save the next word, 1st of all, I should know what I said right now and what I've said before. Right? So we need to have a model to have ability to take into account these kind of information, right? Also, should be able to memorize history in its internal memory. So we need to have memory. So what is the solution? Solution is a recurrent neural network. So recurring neuron network is something that is that you can see here, just neglecting this loop, or this cycle, you see, just a simple, fit forward network, right? But in this one, in this cycle, means that it can have a loop, or it can keep the, you know, the previous information as well. And if you, if we unfold this architecture, you will have something like this. So, let's say we have a text, we key tokens or keywords. Okay. So the 1st word, we will give the 1st word to the 1st model, the 1st letter, or the thing, the first. Fit forward, you know, or MLP, whatever. So it's going to generate a hidden writer. Okay, here. Then the 2nd word, to process the 2nd word, we will give it to the 2nd network. Okay, but the 2nd network, you also get something from the trim system. Okay, to generate the, you know, its own hidden layers of, okay, and similarly to the next step, as well. Any question? No question. So when do we use MLP feed forward? MMP? Yeah, what is an example of using feed forward network? So for, let's say for any kind of classification problems, okay, that you don't need to have information from the past data. I see. I got you. Just, for example, let's say, predicting the house price. Okay, or predicting the, let's say, the gender of people. Okay. So these are some aesthetic data, so the features are static data. So there is no relation between different features, right? So you can use that. For you can use the MLP for those kind of product. No question. Can I ask a question? Okay, so, for this one, for each token or for each input networking, for each input, totally on a net, as I said, the MLP or any kind of model can be placed here with any architecture. Okay? It takes the input from the input and the output of the previous steps, right? Okay. So, What about the 1st system? But for the 1st system, the input comes only from the input. So what about the premises? No? So, scenes here, there is no privacy state. So, we don't have it. So, we're assuming that at the previous information argue. So it means that again, here, we have you, but it's zero. Okay. So, I need feed forward, you know, networks are not able to processes sequential data. Okay. So... Now, let's have a look at some uses of sequence, sequence, they cover sequence. First of all, speech recognition, audio crypto, text, or vice versa, text, speech, speech, text, whatever, sentiment, analysis, sequence of text to a number of stars, okay? Or just simply, instead of, you know, like, the kind of rating. So instead of rating, we can have something, like, just classifying the takes into, you know, positive, negative neutron. whatever. Or DNA sequence, aralysis, so... To find the pattern... Or the machine translation. So any task, it is involved in processing the sequence, either in input or in the output, okay? We can use that can be considered and kind of usage of the sequential data examples, right? Or video. detecting the... Finally, time series forecasting. Okay, so based on predicting the next value based on the previous space. Okay. So, for example, during the past week, we had, I don't know, for example, Monday, we had snow on Tuesday, we had, we had snow on Wednesday. We don't, we don't have this snow. So, but, so what would be the prediction for, let's say, 30? Any question? Okay. So... Just be from diving into the art and then its architecture. Let's have a look at the time series. What types are you being? And what you can do with time series and also the component, the important components of the time series. Okay. So, when you talk about the time series, and it's start our definition, time series, the sequence of data points collected or recorded at the specific time interworld. Let's say every weeks, every day, every hour, every minute. Okay, every year. It has some, we should have something through. Unlike starter, fictional data, okay. Time series focusses on one or more thing or duration. Okay, for example, during the past week, we only we are only looking at all we having bit snow or not. Or we can have more to parties. Like, no, having snow, plus temperatures. Okay, plus the speed of wind. So, to show the time series to plot the time series, the X axis almost always represent time, like 6 days, I don't know, weeks, years, whatever. Get the intervals, and why access? The variable, you are engineering, like the, as I said, price, temperature, population, depending on your task. And the goal, in time series, is the goal is to understand the past, what we had before. And I really care into the future or protesting or predicting the next, the values in the next step or next interval. Right. Something like this. So let's say this is an example for the times there is a passenger. So during... This interrupt, every 2 years. So We have the number of passengers. Okay, so we'll see if we have in Kirman, or the time, or the passenger. And they are collected from one. to, let's say, 1949 to 96. It is just about the number of passengers. So, Any question? If you have any questions, please stop me. Okay. So for the time series, we have 4 confidence. So every and each time Siri has 4 companies, so namely trends, seasonal, psycho, or nose, or sometimes car variation, cars, variation, or residuals, as well. So what is the trend? So, trend? It's a long term direction. So, for example, here, see, let's be increasing, we have the increment. So it's increasing or time. right? So, is it a trend? So, in general, it is generally going up or down or staying flat. Okay. So, the general view on time, sir. Because it is now... Is it a seasonal or the other seasonality? It's a pattern that's repeat over peaks, period. Or fixed intervals. Okay. So, for example, you see here, let's say, in each interwater, we have some defeating, you know, patterns. A here. We have something gained similar to this one we have here, here. This is seasonal or seasonal. And then we have cycle. So the cycle is similar to the seasonal. Okay, but with one difference. So for seasonality, we are looking at 3 short intervals, usually, okay, but for cyclicity, okay? The interval is get wrong. So, for example, here, let's say, okay, for seasonality, we are looking at every 2 years, okay, but for seasonal, for cyclicity, we look at every 10 years. Okay, to find the pattern. Right. And find it in noise or variation or residuals. So, these are some, let's say random hiccoughs. So the random values is not under control. Okay. In a data that can't be explained by the other trees, so we cannot explain these noise, so these are noise, just noise. See. And when we talk about the time series on a receipt or forecasting, so usually it means that we are trying to find what or to predict what? To find the trade. Okay, to predict the feature, to future, to predict the future, okay, we need to, first of all, find the pattern. Okay? So the usage of the pattern is to predicting next values. Okay. So, in that case, it means that in times there is an odyssey, usually important thing for us, are seasonal, finding the seasonality and trends. Okay? Not this one. You know, sometimes before, so we can also have a look at finding the cycle, but most of the time, we only care about these two. Okay? And for noise, we don't care about noise, why? So it's not, it doesn't have any meaning to predict the noise. Why we should predict the noise? Right? For example, here, if we assume that this is our original club. Okay, our original time series. Okay. So we see the trend is green. So this would be our job. So this would be seasonal. And these are the noises. or the residual radiations, okay? Okay, so let's say Tamati type series. Yeah, I just covered, I just wanted to talk about the time series because it was yours, CSI, the course, session, information. So that's about, that's sort of about the time series. So then let's have a look at the current mirror networks. Any question? No question. Is it difficult? So, on an end, are kind of a learning motel that takes the previous output or hidden states as input? It means that the composite input at time has some historical information about the happenings at old times before time people. Right. Orleans are useful at their intermediate states. And their communities, they can store information about testing food. For a time, it is not fixed. Okay. Then in Ireland, each infrastructure, so just say you work, Victor. You know what's the word picture? Yes. The Numerical representation of the world. So in your NFP course, in working with the NP, so you are working with the text, right? So, but the computer, they only knows the numbers. So you cannot give the text worth token. So you need to convert them to a vector to a numerical representation. So the numerical representation of a war, known as border. So typically fitting to the network, one at a time, not all at forms. If you remember, yeah. We give it one time, so we assume that each of these eggs here are one word picture. First, they give this one, for them, this one, this one, this one. Uh, not cutey. Okay. So, let's say, the difference between the feet forward networks and otherness, just visually, we can look at here, you can compare these to each other. So this is very simple feed forward neural network. We have the input. Okay, we have the hidden layer and the output layer. right? So, and we give the input, and we also are supposed to learn these great values for each hidden, like, right? Probably weeks. If you have another layer here, here. So, definitely, we should have, you know, several big matrices. Right. But this is simple fit forward, you know. So. But now, for the other lands, we have the same teams. Okay, but repetition of the same, let's say, the architecture, each for one part of the input, like the token, or word venture. So, and each one, each feed forward networks in other men, will get the extra input. Oh, wait. The extra inputs from the previous, the output of the previous, or previous, right? So, again, here... So, since we have another input to the network, right? So, it means that we need to learn additional ways for those new input. W image. Right? And similarly for the other types of cases, right? Is it? Okay, yes. I think we have a multiple engineers in the 1st one. That may skateboard, then. Yeah, so if you have, let's say if you have multiple layers here, multiple layers. Maybe we say, okay, the next step, we're going to take, you know, the output of the previous feed forward. It means that it's going to take the last, you know, last hidden states from the previous feed forward. Right? Because of that, I said here, Each block, each angle here, can have any architecture. Okay, but the final state is important for us. Any question? Good. Okay. So, this is... forwarded representation of the Ottoman cell or Ottoman block, let's say. So we have the input, we have the hidden layer or hidden state, so we need to learn WX, okay. And also. Here for the output layer, we need to learn the new weight, WY. And if we unfold this plug, okay. So we will have something like this. Right? So for each product, you have the input. Okay. So with the corresponding base, Okay, then. The output of the each plug will pass into the next plug, and input, right? In the next plus, we will have an operating code, WX. Again, WY, in... Here the output of the, this hidden rare, which has included the next one and so on, so forth. Okay? The only thing that you should know that in all these ways WX's, okay, are shaped. You know what I mean? So, mean that it means that these WX, the weight metrics here is exactly signal, the weight metrics here. Exactly, signal this one. Or WY again, here is exactly same as this one. Okay. So... To find the HOT for the hidden steak, at times, the key, to have this formula. What is F here? F is excavation function. Right? Oh, yeah. So, F is activation function. So, WX, extinct down the weeks. Okay. So plus WYHT minus one. HT minus one, the hidden state from the previous step, it, uh, multiply by, it, right? Okay. So, hidden... hidden steak at times 50, 60, you put a times 50, W, w, w, or this is theatre, that determined the importance of tea, presenting process. So, it's the role of the victim matrices. Great. Okay. Um... So, do you know, is it? The eggs are changed or...? Any question? No question. So, just, let's have a look at, for example, image captioning. So, you know what's the image captioning? Just assume that we have a model, we give it an image. And we want to, we want the model to describe it or to generate the caption for that image. Okay? So, the input should be image, and the output must be multiple words, I mean, the same consequence, of course, the sentence, paragraph, or description, of other. Is the same like the data annotation? It's kind of data annotation, but I would say it's not, I don't think, it's kind of an annotation. Okay? Typical, you know, we No, let's say it's not the adaptation. Because usually when we say the data adaptation, the adaptation must be done by the human. Okay, but here, and also, here we want the model to generate these things. description, okay? So, for example, the dog is hiding. Okay, now your question 1st of all, our input is image. Okay? How we, and we output a text sequence of text, right? Okay. We have a network, neuron network, and we know that the neuron networks only gets the flood numbers as input. But we have image here. We have an image here. How we can convert the image to the numbers. Image to a number? Image to number. So it's the intensity of the RG. Yeah, no, if you want to go in that phase, it doesn't make sense, it does not make sense, because, you know, if the size of image is gonna be huge. The input size would be very huge. Once to the CNN, then we take the NBA, see the image, you have the RUV, you convoluted, get your features, and then from there, to the, to uh, under all the work, and then you get a, a number of, I know. So, yes, we can, 1st example, we can give the image to the CNN. Okay, so you are you use the CNN for image classification, I believe, right? Okay. So it means that at the last layer, at the last layer, we have something like this. If you had three classes, the last time you had three neurons, right? For the last day, you had one dance layer. Like, so... can... can be in any size, in any damage. So, we can say the output of this layer is the input for the RNM. It's the encoding of the input image. Right? So you gave the your image to the CNN, you've got it here encoded version. Right, just a big turn. Okay. Now, you can give this one, this vector to the other end as the... Okay, so give it to the CNN to get this information, then you give this vector to the oven. So, the 1st one, very generate the 1st sport. So, the 1st other name here, then you will give it. Give, give, give 2nd hidden estate. Okay, 2nd hidden say, who deserves that, and it will generate TikTok. Okay, similarly, for this one, up to the end, and to receive the dog is hiding or whatever. Right? Okay. Any question? Is it clear? 100% clear? Are you sure? But to me, it's not that clear. It's not clear, because, um... So, how the... What? So, we've been seeing the mechanism of the art and then we've seen it later. okay? So, but in terms of the only the improve. All of the input. So the input here, the input for the 1st is that, as I said, is this picture. Okay. So then for the 2nd is the 73rd is the next is that. okay? We only gave the RNN, the hidden states from the previous steps. Okay, but what about the input? Those XT, XT, minus one XT plus one. Okay. So here, the input to each other is the same. In this task. In this task, it is... this way. We can also look at it in another way. Okay, so we can say, the input, for example, input, input vector for this other end, would be the, the word vector of the, If we want to look at it as a character, to decrease. Okay. So, definitely, we make turn off the, So we generated the first four. The first one would be the input for the second step, right? Plus the previous hidden states from the previous system. Right now, I have the 2nd input and previous hidden state, and I generate the 2nd world. Now, I give the 2nd word an input to the 3rd step, right? And the hidden state from the 2nd step, as another input. Okay? Similarly, you can repeat it for up to you, to you meet the stepping conditions. Okay. So, there are, for, to use the deep learning models, there are several scenarios. So, for example, we have single input that are similar. Okay. So we have single input, singer, output. So in this case, we are using the speed forward neural networks. For example, when you want to predict the price for the houses, You have water input. Okay, set of features. Okay, you put it wrong. And only one output the price. So this is single to single, right? We have signal to, single, and put multiple output scenario, like the one you saw before the, like, the image captioning. So we only have one input image, one image, but sequence of words. So, this is single, two, multiple. We have multiple, input, single output scenario, like what, text classification, or segment analysis. So you have seplants of words. And at the end, you want to have only one prediction, positive negative. Also, you have the multiple input multiple output scenarios, like what? Like the machine translation. So we have, for example, translating the English to French, we have sequence of English words, right? If we want to have the in the output, you sequence of, let's say the French port. That's the multivirtual, multiple, scenario. Or. For example, the video captioning, it's also can be considered as the multiple scenarios. So why consider the video captioning as the multiple to multiple multiple output scenarios? Because you have, you're sitting still on the field, so you're making... So yeah, multiple images going in. Yeah, okay, yeah, yeah. So the video is a bunch, you know, the sequence of images over the time, right? So let's see multiple input. And the caption is multiple. I have a question about the penis is like, so when we have a picture, we can classify it as a dog or a cat or whatever from like a CNN. But for the RNN, is there any process that matching different pictures with different captions and then learning through that or how they make this? Yeah, yeah, for the training, for training, the image captioning? Yes, definitely you must have a training data, the pairs of the images plus corresponding caption. So... I think it's actually a change kind of action. Okay, what better? There is a vector. Those eyes, like, as absolutely. No, we are talking about the training data. Okay. So, but when you want to use them, you have to convert most of them, the images and also the descriptions or captions into the normal characters. Okay? Any other question? No question. Okay. So... another thing. That's you. I believe that you already are familiar with. So, it's a loss function. So, have you reviewed the lust functions in the previous lectures? Last function. What's the last function? Lastecture? You know, the series. No, I don't think we did a lot. You didn't have it? Like, we had talked about the concepts of... Okay, we've talked about maybe a couple of them, at least where we absolutely. Okay, okay, okay, okay. So what's the last function? So the most important thing in New York training, in training, any model? Training any model is lust function. So what is the loss function? So when we say, okay, we are training something. Or even for ourselves. Okay, we are training ourselves to learn something. Okay. How we can evaluate that, how well we train the model, or how well we, ourselves, okay, we learn something. We must have a metric. You must have magic to, you know, to find this gap between what we expected to have and what we know really, what we have in the reality. So the difference between expected output and the real output or the predicted output? Okay. Must be measured somehow. So the role of lust function here is only measuring this gap between the real output and expected of. You know what's the difference between the real output I've been expected output? Like, there's no... is what you labelled versus what you get for your model. I say, for example, I... The expected output. And to here. But the real output is here, are you? So, I mean, how can I measure these scap? My predicted minus one? Predicted and testing, predicted and... What could be the testing, the testing? The testing said, okay, so. What's the question? Yeah, yeah, yeah. So for the, for, no, we use the last function, we use the last function for training, not testing. Okay, so why we are not using the last function for steam? Because we try to train a model. Okay, our goal is to train a model, with any algorithm, any mentor. Okay, our goal is to use the train model. to apply on the test data. Okay. So it's something like this, I, as a developer or as a machine learning engineer or whatever, I try to develop a model, train a model, and then after that, after I train, I will give it to someone else. Okay? And I say, okay, now you can use this model. He or she, the customer, should not be, no, fear about this loss function for this stuff, the hidden state, badge, iteration, learning, training. That's why we don't care about the testing. Any other question? Okay, so the last function. The last one she meant her to evaluate how about an algorithm, what is the given data, or... an approach to finding gap or distance between the real output and expected. So this gap, this gap, the gap between the real output and the expected output is also known as Arab. When we talk about the error, error always is this, we get between what I want it to be and what are, right now. So, should we qualify the eros? So also known as cost function or error function. If you get the future. So there are several... No, noun, loss functions. In machine learning and be learning stuff, but in general, they are in tri category, irrigation, losses, probabtic, losses, and angels. Okay, so the recognition last function, you must be familiar with them. So usually for the regression tasks, for the regression tasks, we use the unification loss function, like the mean square error or meaner suit error, or the mini square error, also known as Ntulas. I will reach a lot. This one also will be main absolute error or any, or lasso, but sometimes the... So the average of the sum of the square difference between the actual values and predicted values. So if the actual value, let's say, we have the finest value. Really? Inspector. Okay. Okay. Maybe one way. Two... Three... One... Four, two, three, two, three. So, for anyone. Hey, good morning. Three minus 12 plus 4 minus plus... Ooh..................... T minus 2 to the power 4 minus. So Similar to the main absolute earth, but we don't consider, you know, the ages. Okay. Then... Then... And then we have the probablistic glass function. So, this is for the, this one gave MAE, MSER for the irrigation test. Okay, so in education test, we are predicting a real value. Okay, for those real values, if it has these to predict the real values, we can use those, you know, the function, last function. okay? But if our the output of our model network or any model is just probability. probability distribution, okay? So to find a difference in between the expected probability distribution and the real, you know, the real probability distribution, we use the probability stimulus function. So most of the time used across entropy, last functions, Okay, so if our task is binary classification, we use the binary crust entropy loss function. If it's a multiple class classification task. So we usually category cross entropy, cross entropy, loss function. If the classes are ordinated, are encoded as one heart encoding. Okay. Otherwise, if the labels are labelled as a number, okay, we will use the spars, categorical cross entropy. Okay? So, these are the... I mean, that's what it became last hinge last. So we use it for the most of the most of the term for the traditional machine learning model, like the experience. The labels here must be in the form of minus one and +one for the body classification. And also for multi-class classification task, we can use the, there is another type of the English. It's now as category so you can use this one. It can also be used with the neural networks. So deeper these stuffs. But most of the time, most of the time, we use these loss functions for training our motors in, training, our training. Okay. It's good time. Can you press for five minutes? This one? No. So, uh, to predict the wife, then having the wife, the expected output, and this white hats are predicted, output, we can find the loss, the difference between, okay? So the lust would be something like, okay, so the lust, which it takes the expected output and the predicted output. I mean, last function can be a function. So we know that it's a function, right? To the punch. So, then, to apply the gradient, to apply the information, we use the gradient decent model. Right. So, we think we deal to update the rates, we apply, we use the gradient thing, so gradient is sent, network, for the approach. So, the gravity, the new, I think it's the new value of the W, the new value of the W, or new W equals to the current W current. Minus alpha, which is learning rate, learning rate, high, there is a tip of L, or lust function with respect to the, W. Right? Is it clear? Shall we see the simple gradient? Okay. And then to find this value is derivative with respect to the W, the UC chain room. Okay. derivative of N, with respect to the W equals to derivative of L, with respect to the Y hat. Tire is derivative of Y hat, with respect to W. So, I believe you all know about this, this is known as Chenu. Right? Change. Yeah. No? Okay. For chamber. What's the YouTube video? What is change, if we assume... Who doesn't know the chamber? We don't just have to refresh again. Quickly. If you have something like this, Y equals to F of ger of X. Right? So, we can show it is, is former in this way. Okay, let's say, this is X. It's in G. Function. F function. F function. Right? So, F of X, F, F, F, takes the output of G. Okay, so the same PF of G, of G, X, X, X, or what? So. Now, if I want to find the, the, what you, avoid with respect to the pigs? Okay. So, I can say... It seems... Why? I can't say, it's a course to the reality of why, with respect to the F? Yeah. And delivery of F, if we speak to the G. In devative of G, with respect to the... Just... And if you are, probably, just to say, okay, just make sure that, you know, a new Catholic.. Right? Yeah. This is the chain. No. If you have multiple layers, if you have multiple layers. Okay. So at the end of the 1st day, we have the Y hat one prediction for the 1st day, at the end of the 2nd layer, we have the Y hat 2 depiction for the 2nd, right? So then why have one, we, we, F1, X and W1, YF2, it goes to F2, the 2nd activation function, this F1 and F2 can be the same, can be different functions, can be a different activation function. So, then the last function, would be the difference between expected output and predicted output. Then, if you want to update W1 and W2, okay? So we need to take the gradient of the loss function with respect to the W2 and W1 as well and update them according to peaceful. Okay. So for example, W1, the new W1, it will be current W1 minus alpha, derivative of air, which is speak to W1, then similarly for W2, Okay, then, again, we can apply the shape room to find is... Right. So this is, this is, the approach that we use for the normal fit forward neural networks. But for the for the recurrent neural networks, for the current neural networks, if you remember, we had something like this. We have, maybe, see, the architecture of the, Iran networks, the scheme of your, the different neural networks. And this is the unfortunate version of this one, or here. Okay. So, the R and then, we can consider an R and several... feed forward networks that are related to each other somehow, okay? So, it means that the back here, the back propagation will be truly tugged. Okay, so it would not be something, no, fixed, just for, okay, we use this, it's for this one fit forward networks, one of these things. So we need to apply the back propagation on all of these networks. Right? So this is known as back propagation through the time. Right. So, for example, if we want to apply, let's say, if our final output is here, Okay. So, then we need to apply back probation from here up to here. Right? So depending on the task that you are serving, okay, the way of computing this back verification through the time would be a bit different. For example, If let's say, You are doing kind of text classification. Text classification, in text classification, if we assume that each of these steps are one token, one word, okay? The final output, the final output would be here, just a label. Right? So you will need to compute the back propagation from here to here. You have the output here, okay? To find the lust function, you will use this output, the expected output, right? But if, for example, your task is kind of Part of speech tagging. Or named entity recognition. Okay, so what are the part of the speech text? The text for each word. For example, if each word is a noun, it's funny, I don't know where this will be, so it means that in that task, here, for every of these fit forward networks here, okay? At the end, you will have output. Okay, you will have up. So, in that case, in that case, you should 1st save the love of this one is the 2nd the 3rd step. Okay, the last and back from here to here, one time. Then back propagate from here to here, back propagate from here to here. Then take the average of the, all the, all the updates on debates. Right? Understood? Depending on the task. Okay, you should do multiple several back propagation. For classification task, you only need to do one from that propagation front here to the 1st to the beginning. Right? If it's kind of part of speech tagging or named entity recognition that you have the output for each step. Right? So then you should, you should do the back propagations centre of time and update debate, and at the end, you should take the average or some of those rates to update the rate here. Okay. Pursue. Any question? So, this is now as bad propagation through... A normal, neural, used bathroom sheet to update big bike, calculate ingredients, nearby, layer, as... you so here. In Ireland, the savings are used at every step. I mean, network is on road across time, so it was... Right propagation through the time, meanly compute gradients across all these time steps, and update the shapes. And the rate updates are computed for each copy in our format, networked in some or average, and then apply to ottoman weeks. So these weights, you know, the weights, as I said, the weights for the input weights for the hidden states are shaped. So after finding the after back propagating several times, depending on the task, when we find the updates, for each race, we take their sum, or they average to find the new values for the wigs. Okay. So, just, it's just forward propagation. So, in a simple arment. We have 3 steps. So 3 inuts. At the 1st system, it takes the X0, it usually, it says 0 K then, it generates, it predicts the, let's say, the 1st output. Assuming that here, we have the kind of named and Siri recognition task or part of a speech, tending task, okay? And for the seconds, that it takes the input from the 2nd, that is the canting food. Plus, the hidden state from the previous state, which is which one. Okay, and similarly for the surface. It takes the search input plus the hidden states from the previous system. Right? Like, for example, the mage caption, the output has, like, a very long sentence, and, let's say, the last wars, like, the tomorrow, the last wars. usually like these provocation Is that back probation? It's a forward competition. Okay. So we are Okay. Okay. Any question? Okay, so which kind of information, uh, needs to include here? H1. Okay, so assume that, okay, so we have H 8, or H 10 here. So, and we have, assuming that we have H 10 here. And one of the input for that is H9. Okay, so which kind of information does H9 includes here? The information from, the information only from their last word, or all the last words, all of them, all of them, all of them. Okay, because when we say, okay, this carries the information from the 1st step to here. And this carries the information from the 2nd set. So the information here, this step also includes the information from the privy system, right? So this one carries the information from both services, similarly for me, others that are... So, and to do defect perpetition, we can do something like this, type of central time. So then, derivative of L, I accept tea... tease output, with respect to the, let's say, the first layer, each one would be something like, is derivative of L of tea, with respect to the predicted output, at least the tea, times in derivative of the, Why had he, with respect to which one that he can again, simply unfold it using the change. So, Now, next topic is the... Problems with the vanilla, R&M. Okay, what kind of problems do we have with the one? So if you remember, so with the back propagation, with the back propagation, when we use the length chain to unfold, you know, this... There you achieve, taking derivative steps or the... Okay. We use this land chain to unfold it. Okay, so that could be multiplication of something. Right? So, for example, derivative of L, relative of teeth, out food. Okay, with the, the taste, last function, okay, with respect to the output at times the tea, times the UFO boy had tea, with respect to HT, then time the, the, the, the, the, the, of HT minus one. And similarly, up to the relative of age two, which respect to H one. Okay, so we are using the multiplication of several values. Yes, OK. So, and this expression, or the length of this expression would be very, okay, based on the number of steps that we have in our in it. right? So, for example, if we have Oregon with 3 steps, right? So, this land would be three. If we have 100 steps. Okay, we will have at least 100 lemon in this expression, right? Okay. And usually, usually these values, these values, art, smoke. Okay, but they are the output of the breaks of the sigmoid functions. Okay, and when you take the derivative of the sigmoid function, the maximum value would be 0.25. Okay. So, and when you multiply several small values together, okay, at the end, the results would be approach to zero. Okay? And if you have this value has 0, what would be the results? No change. No. There will be no chance. So this would be gradually succeed, for me. So if this value, it calls to zero. Okay? So, it means that the new W... would be exactly same as... The old one. Okay? So there would be no training, between no learning. So, or in other words, so, in, in, in, in, if we have, you know, very long sequences, or very large number of steps, okay, applying the back for vacation, applying the back for vacation, on these kind of ordinance, would cause to would prevent the previous lawyer, previous lawyer, to learn something. Right. So the previous layers wouldn't learn anything. Right? So this is known as vanishing gradient problem. So through the time, the gradient disappears. For the long sentences. So, or in another way, it says, just as an example, if we use the simple ordinance or when the ordinance, to process the long sentences, okay? They are not able to memorize or recall any information from the previous steps. For example, For the short sentences, if you used it, for example, I live in France, okay, and I speak, I speak French. Okay, so this is short sentence, and if you want the model to predict, I speak something. Okay. So the modern can simply predict it. The length of the sentence is short. Okay, it can recover from previous systems. Okay, so it is living in France, okay? So his language is French. But if we have very long sentences. hitting some stories, I live in France, okay? Then story, story, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah. Now, I speak in. So the model here will be confused. Right? So this is because of the vanishing gradient problems. Right? So in the other side, we have another kind of problem, which is now as exploding gradient problems. So, the one I explain. So what about disappearing the gradient through the time, right? So for the exploding gradient problem, we have something in reverse. So we have the very long sentences. Okay, so the amount of the gradient is high and multiplying the high value several times together, which results in very high value. Again, the model will not learn anything very good. Okay, so this is known as exploding gradient problems. Right. So... As I say, in the same, a product of carrier numbers can change to 0 or explode, to infinity, so it can throw backs up. But she says, it beats. Okay, so in vanishing ingredient, so vanishing ingredient causes, gradient becomes extremely small, as they propagate backward. The 1st layer, received almost no updates. And network space to earn long-term dependencies. Okay. So what are the and similarly for the exploding, exploding, creating problem we have? So what are the solutions for these graduate problems or vanishing or exploding? First of all, the 1st solution is just to not to use the vanilla ordinance, the ordinance that I explained. So there are, when you are the basic types of the audience. So, is it of using the vanilla ornaments, we can use the gated R&Ms or gated, the organisticated architecture, like the LSCM or GRU? And also, we can apply the gradient drinking. So when we see the graduates, the amount of graduates, if the, you know, cusses some threshold, you can clip it. Okay, say, okay, now just be in this range. And instead of using the, for example, the sigmoid functions in activation, the activation function, so we can use a runner function, but I see reduced, zero, zero. Okay, to prevent that kind of problems. The punition problem. And also, you can apply the higher normalization or batch normalization. And other solutions. So in later normalization or batch normalization, you know, what's the normal, you know, what's the normalization, right? So in batch normalization. When we train the model. Okay, so we train the model batch by batch, right? We trade over the training set. Okay, batch by batch, right? So in each batch, when we give interest to models. So we normalize it. Okay, we normalize the output of each layer for each batch and then we give it to the next layer to process. So this is now as Batch normalization and the other one uses shorter sequences. So this is not a general, no, the solution because it's not up for us just to, you know, use only the short sequences. Sometimes we need. We have to work with the long sequences. Okay? And the other one, the most popular one, let's say, solution to avoid, this problem is to make use of the transformers. So you will see the transformers in your NMP course. Okay. So I think the transformer is the best way to deal with this problem. And just as an evidence, when I say the transformer is the best solution for this one, for this problem, that now you all are work, have experience working with the GPT models. Tipstick or, I don't know, or use elements. Okay. So, and you know that you can give them very long documents, very long documents, and they can understand that process them very easily without being from it. without forgetting about the previous, you know, the the information in the 1st, I mean, or the beginning of the beginning of your text. Okay, that's because of their ability, handling, you know, very long sentences. Okay. Um... So, all the variations of the ornaments. So the 1st flavour, the important flavour, or the, the, the, as well, the, the, the, is now as a, the, the, the, the, So, it means that these kind of, for these kind of ONN, we consider two types of memory. One for the long term information to keep the long term information from the very long, very previous steps in this sequence. Now, as long, it's long term, and then short term, very recent, you know, steps before. Okay. So, and to apply that... So this is the overall architecture of the LSTM cells, which LSCM says. So it works based on gating. Okay. So in LSTMs, the LSTMs tries to decide about something. When it receives some information, It 1st it tries to find which part of this information must be forgotten. It's not necessary. Okay. So this is named as forget gate. This is the beauty of forget. So, and then there is another gate. So decide, you know, which part of the information must be emphasized. Okay, so it's now at... So like this, you see? In each set, we have these games here. So... Sure. And it's early by that. So, very well, this is in Europe. This can be any network. So it's yellow and tangos can be painting. Okay, and it's a cleaner. This is going to find operation elements by elements. Element point five means element point element. For example, Victor, multiplication, here means that multiplying the each element by another element. So this is direct for transfer, is made the concatenation hand, this is the copy. Okay, so now the 1st one. We have, as I said, 2 types of information to be kept in the memory of the LSTMs. Okay. One from the very long, the very long has. And one from the recent. So we have CT minus one, the context from the past. Right? This is long term memory. So, it takes it, this is our set. It takes these two, CT minus one, HT minus one. Long term, shorter. Hit information from repass, information from the recent information. Okay, plus XT. The input to the... So, the LSCM has the ability to remove or add information to the same estate. So, by how, by gift? Let me say, remove some information. It just may... We learn, wait, we learn this. So we use those ways to say, okay, which parts of information must be forgot, must be removed. If the weight is 0, it means that this part of information must be removed. Something like this, if it has a picture. If any value of one or 2 or 3 or 4 or 5, And if this is a great, Let's say, this is 0, this is 0.one. It says 4.8, this is 0, this is 0.8, and if you do the fortification here, 0.5 mortification. The results would be something like this. 4.8 alpha one, 0 count one, 0 0. So what does it mean? It means that, okay, I don't mean this part of information. Okay, but this part of information must be emphasized. Okay? And this part of information is not that important, but I don't want to remove it completely. Just decrees, it's important. Right? Using the gate. We do this kind of stuff. games are composed out of signal, neuron, slayers, and .5, mortification. So here, here, we also need to learn these weights as well for the gates. Right? Then step one, just what truly is your work? What decides, what information to throw away from the same state, forget layer. Okay. Have the input and information from the previous extent, okay? We can't captainate them. And it will generate something. So forgetting here, we have, you see, we have also here, the base, based on this to learn, the STM, we decide about, you know, which part must be forgotten. So, FFT... Output of the 40 gate. A course. Yeah, WF. Compatination of HT minus one, and input, XT, plus bonus times. Right? So, 10, we'll have these. So, and if we have one... the output here. If the gate is one, three that keep that part completely, if it's 0 means neglect that part of the information. Right? So this is about the 4 gate. Then we have this, at least there too, we decide what new information we are going to store in the state state. Okay? Okay, so, HT minus one, HT Myson was the information from the recent steps. Okay, recent history. But the CT, C is the long-term memory. At the 2nd step, we should decide, okay, I have this information from the long history. Okay, but now I'm a new step. Okay, I mean use it. I should decide what kind of information I should add to my long history. Right? My love at last. No, no. Give the thing, okay. So then, you have the input set, input gate, decide which values will update, and how much that is, that you have, actually, it creates a greater of new candidates. So basically the formula. So... For the last term, so let's say something, then let's say it is not a term history or last memory. If you have, I grew up in France. Okay, blah, blah, blah, blah, blah, blah. Okay. So... In its long memory, LSG, it's from K, K, throughout the gym, France. Then we use that history, that memory to predict history. Right. Okay. Then for updating this state, use this formula. Okay. C of tea. Maybe I'll take by the F 15 times C, T minus one. Which kind of information must be removed from the previous. Which kind of information must be added? What's the term? And that's it. Okay. So... I'm sorry. Any question? Do you have queens from the country lectures? Like for bonus marks. Two questions, she calculates how many papers? I ask the questions. Are we expecting any ingredient decent map in our Nitan? No, just... Thank you. She said apply everywhere. Just after the technical skills... So, you can put the technical skills at the very top. No, no, these two should be at the very top. Education and technical... Or after technical. Which one you put first?