So, clustering. Each step involves different levels. For example, here, if we assume these are our data objects, then we can partition them into clusters. So we can have, you know, these three clusters. And there is no overlap between classes. But let's say we have our partitioning, and say, for example, you say, okay, P2 and P3, point 2 and point 3, can be combined into one cluster. Okay, then P4 and P1 and P3 can be another cluster, and P1 with these can make another cluster. Or, let's say you say, okay, P1 and P2 can make a cluster. And also P3 and P4 can make another cluster. And at the highest level, those two clusters can create a bigger cluster. In each of these, traditional hierarchical clustering or non-traditional clustering, at the lowest level, each sample is considered as its own cluster.

So, for example, here, say P1 is its own cluster, P2 is its own cluster, P3 is its own cluster, and so on. Then you say, okay, P2 and P3 are more similar or more close to each other, so we can put these two in another cluster. And then, at the higher level, we say, okay, P4 is much more similar to this cluster than to any single point. And inside a cluster, we have different data points. So when we say similarity between points, it is clear. If you have two points, two data points, you can simply find a similarity between those two points, right? You can simply apply, let's say, Euclidean distance. But when we say P4 is similar to this cluster containing P2 and P3, how should we find this similarity? Between P4 and P2, or P4 and P3? Or something else?

Yeah, we use the representative of the cluster. The similarity between the point and the representative of the cluster. So this representative can be defined in any way. For example, you can take the mean of the objects inside the cluster, or you can take the maximum point, the biggest point inside the cluster.

So, that was about different types of clusters. So in clustering, the clustering algorithms can be divided into two groups: partitional or hierarchical. Now, what are the types of clusters themselves? The clusters, the groups, can be in one of these four categories: well-separated clusters, prototype-based clusters, contiguity-based clusters, and density-based clusters.

So first, well-separated clusters. A well-separated cluster is a set of points such that any point in that cluster is closer to every other point in the same cluster than to any point not in the cluster. So, as you can see, all the points in these three clusters, they are closer to points inside those clusters than to points outside. So they are well separated.

The second type of cluster is prototype-based. A prototype-based cluster is a set of objects such that each object in the cluster is closer to the prototype, or centre, of that cluster than to the centre of any other cluster. The prototype or centre is the same as the representative of a cluster. So all the points inside these clusters are closer to the centre of their own cluster than to the centre of any other cluster. For the representative of the cluster, we can take the object in the centre of the cluster, which we are calling the centroid. Simply by taking the mean of the objects, you can get the centroid of the cluster, or the representative of the cluster. Or we can take the median. Also, you can take the maximum value, the biggest object inside the cluster, or the smallest object. But it is typically the mean or median of the objects, and that is what is most commonly used.

For contiguity-based clusters, a contiguity-based cluster is a set of points such that a point in the cluster is closer to one or more other points in the cluster than to any point not in the cluster. So here, the distance between data samples within the same cluster is small. Not the distance between the data samples and the centroid. So here we have eight different clusters: one, two, three, four, five, six, seven, eight.

Now we have density-based clusters, which is the basis for the DBSCAN algorithm that you will see next. The clusters are based on the density of the points, and they are separated by low-density regions. So, for example, here, you see the density of the objects is high. Then we have a low-density space here and here. So this region has high density. And here as well. And all of them are separated from each other by low density regions.

Now, let's have a look at different clustering algorithms. Actually, the clustering algorithms are not limited to these few ones. There are lots of other algorithms, ranging from the very simple ones to very advanced ones. But these are the most famous and most popular algorithms that you can at least use as a kind of baseline. So, we are going to have a look at K-Means and hierarchical clustering. We are not going to cover all the variants, like K-Means++, bisecting K-Means, and others. Then we will have a look at DBSCAN clustering as a density-based approach, and finally, distribution-based clustering, or the Expectation Maximization algorithm.

So the simplest and most basic clustering algorithm is K-Means. I believe you are all familiar with K-Means. It is a partitional clustering approach. So when you apply K-Means, there will be no hierarchy between the different clusters. You have to define K, which is the number of clusters that you want to see at the end. Is K a parameter or a hyperparameter? K is a hyperparameter that you specify.

Each cluster is associated with a centroid, and each point is assigned to the cluster with the closest centroid. The basic algorithm is very simple. First, select K initial centroids. How do we usually select those K points? The simplest approach is just to select them randomly. And also, you can use heuristic algorithms to find optimal data objects as the initial values for these centroids. So then you form K clusters by assigning all points to the closest centroid. Then recompute the centroid of each cluster, and repeat until the centroids do not change.

So, say our data distribution is something like this, and we said K is three. We select these three objects as initial centroids. Then, at the first iteration, all the samples must be assigned to one of these three centroids based on the distance to each centroid. In the second iteration, you should recompute the centroids. How should the centroids be computed? Simply by taking the average of all the objects in each cluster from the first iteration. Then in the third iteration, we do the same thing again. And after several iterations, it converges to stable clusters.

Now, in the first iteration, when we set the initial centroids, this means that the final results depend on the initial centroids. If you do not select good initial centroids, it may cause some issues. It may not converge, or if it does, it may not get to the optimal solution. It depends on the initial centroids. So if you select different centroids, it is not guaranteed that after six or seven iterations you get the same results. Maybe if you say we do not have any limit on the number of iterations, at the end it will converge. But since usually we have some limitations, because of that, we say the final results depend on the initial centroids.

It is similar for optimization algorithms. If we do not have any limit on computation power or time, maybe any kind of optimization at the end will result in the global optimum. But since usually we have limitations on our resources, we say that all optimization algorithms depend on the initial state.

One question. So for this example, we have only one feature, and then we cluster. What if we have two features, X and Y? And those points have the values of those X and Y. How do they form clusters? They do the same thing. The only thing that you do here is finding a distance between the points or data objects. And recalculating the centroids. For recomputing centroids, it is similar. So you have some vectors. Say A1, A2, A3, A4, up to AN. Recalculating a centroid is very simple, just element-wise addition and then divided by the number of the data objects.

Do you calculate the average of each cluster before you reassign? To find the centroid, you calculate for each cluster the average of the points in that cluster. You do this after assigning, not before. So at the first step, you just select some random centroids. Then you make the clusters by assigning the data objects to one of those centroids. Then you recompute the new centroid by averaging all the samples inside the cluster. Then, again, after finding the new centroids, you reassign all the data objects to the new centroids. And you repeat this process.

So this is the K-Means clustering algorithm. A broader question: especially at higher dimensions, you start losing a lot of, like, distances start to kind of average themselves. Lower distances start to get harder to differentiate, and everything becomes almost equidistant apart. At that level, are there any alternative algorithms that are able to deal with this better than K-Means?

Actually, let me rephrase. So essentially, the distance between points becomes almost like all points become almost equidistant to each other because there are so many dimensions. This makes sense because by increasing the number of dimensions, the data becomes sparse. Usually the word we use for this is the curse of dimensionality. At higher dimensions, the volume expands rapidly and it becomes hard to tell differences between different data points. Essentially, it is easy to see separation between individual points, but it is harder to know whether individual points are part of a cluster or are supposed to be just completely distant. That is exactly the issue.

K-Means is not the most efficient algorithm for all scenarios. There are lots of other algorithms, not just clustering, that you can apply for different scenarios, for different data, with different properties. But in any case, for any kind of data, K-Means provides a baseline result. Have you got a problem with K-Means? That is only one problem, everything depends on the initialization. Selecting the number of K is also a problem. So the first problem of K-Means is choosing the value for K. If we do not have any prior information about the data, you cannot find a proper or good value for K.

So, K-Means is a centroid-based algorithm. The initial centroids are often chosen randomly, as we talked about. Clusters produced can vary from one run to another since we are choosing the centroids randomly. Run by run, you will get different results. The centroids are typically the mean of the points. K-Means will converge for common distance measures. If you do not have any limit on time or iterations, at the end everything would converge. But most of the time, the convergence happens in the first few iterations. The complexity of K-Means is O(n * K * I * d), where n is the number of data objects, K is the number of clusters, I is the number of iterations, and d is the number of attributes. So it is linear in all these.

Now, assume that we applied K-Means and we got some clusters. You use K equals three, your friend uses K equals four, someone else selects different initial centroids, and finally each of you gets some results. Now, how can you compare the results? Which one is better? Here, there is a function called Sum of Squared Error, or SSE. We can find the SSE for different clustering results. Since the results are different, we can compare these SSE values for different clustering algorithms to find which one is better.

For each point, the error here is the distance to the nearest cluster centre. It is called error, but it is not truly an error in the supervised sense, because for actual errors we need to have some gold labels. But here we do not have any true labels or uniform data. So to get SSE, or Sum of Squared Error, we square these distances and sum them. So X is a data point, Ci is cluster i, and Mi is the centroid of the cluster. For all the clusters, for all the samples inside each cluster, we find the squared distance between the sample and the corresponding centroid.

Are you squaring each distance, or are you squaring the sum of the distances? You square each individual distance. What are you doing with SSE? You use it to compare. So let's say you applied K-Means and got one result, and I applied K-Means and got a different result. Now, I want to compare these two clustering results. So how can I say which one is better? We need to have a metric. We basically find SSE for this one, then find SSE for that one, then we compare them. Which one should be higher? We are looking for the smaller values of SSE, because a smaller SSE means the points are closer to their centroids.

Now, let's come to hierarchical clustering. As I mentioned before, you have hierarchical clustering algorithms that produce a set of nested clusters organized in a hierarchy. You can visualize a hierarchy of clusters using a dendrogram. A dendrogram is a tree-like diagram that records the sequence of merges or splits. For example, here, first of all, each sample by itself is a cluster at the lowest level. Then, in this example, data object 1 and data object 3 may form cluster 1. Also, data objects 2 and 5 create cluster 2. And after that, cluster 4 and cluster 2 create another cluster. And then these two clusters, cluster 3 and cluster 1, make another cluster. And finally, they all combine into the final cluster.

So, at the first step, all of the data samples are individual clusters. Then we combine data objects 1 and 3. Based on the similarity or distance, the distance between 1 and 3 is less than the distance between any other pair of samples. Also, data objects 2 and 5 are the closest two points to each other. And after that, we discussed merging further.

So, in hierarchical clustering, you do not have to make any assumptions about the number of clusters. We did not define any K or number of clusters. That is one of the advantages of hierarchical clustering. A desired number of clusters can be obtained by cutting the dendrogram at the proper level. For example, if you cut the dendrogram at the lowest level, you will get six clusters. If you cut it at a higher level, you will get four. If you cut it at this level, you will get three clusters. It depends on which level we want to investigate the clusters or samples inside the clusters. This may correspond to real-world taxonomies, so in biology, bioscience, health science, or in DNA analysis, we use this kind of hierarchical structure to find patterns. So hierarchical clustering is much more popular in those domains than partitional clustering.

Now, how can we get these hierarchical clusters? We have two main types of hierarchical clustering: agglomerative and divisive methods.

In the agglomerative method, we start with individual points as individual clusters. We find the closest pair of clusters and merge them. We repeat until only one cluster or K clusters are left. So we are merging the closest clusters until we get one big cluster.

The divisive approach is the reverse. We start with one all-inclusive cluster. Initially, we have only one cluster including all the samples. And at each step, we split a cluster until each cluster contains only an individual point. So the agglomerative approach is a bottom-up approach, and the divisive approach is a top-down approach.

Traditional hierarchical clustering uses similarity or distance metrics for the merging or splitting of clusters. Sometimes for the agglomerative clustering, we also do a kind of post-processing to improve the results. The post-processing in clustering typically involves combining similar clusters with each other or splitting big clusters into multiple clusters. One of the usages of the agglomerative algorithm is for doing the post-processing on clustering.

The basic algorithm is very simple. Compute the distance matrix, sometimes you may see something like proximity matrix in literature. Then, each data point is its own cluster. Then repeat: merge the two closest clusters, update the distance matrix. Continue until only a single cluster remains. So the important thing here, the key operation, is to find the distance between two clusters. And there are different approaches to finding the distance between clusters. Different approaches to finding this distance can result in different clusters as well.

Let's say we have these samples. You can find the distance matrix. For each pair of the samples, you find their distance. After finding the distance between each pair of data samples, you need to combine the closest samples. In an intermediate situation, some samples have been merged to construct different clusters: C1, C2, C3, C4, C5. Then we need to recalculate the distances, but not for individual samples, for clusters. So when I say finding the distance between these two clusters, each cluster has different samples in different positions. To find the distance between two clusters, there are several ways.

The simplest one is to find the centroid of each cluster and then find the distance between those two centroids. Taking the average of samples in each cluster and finding the distance between those averages. That is the simplest one. But there are some other approaches. For example, the distance between the closest samples from the two clusters. Or the distance between the farthest samples. And depending on the type of distance calculation approach that you take, the results would be different.

Now, we want to, for example, merge the two closest clusters. And update the distance matrix. So that means we will have fewer clusters. For example, if C2 and C5 are merged, instead of C2 and C5, you will have C2 union C5. But now the question is, how do we update the distance matrix?

As I mentioned, we have different types of approaches. One way is the minimum distance approach, the distance between the closest samples from different clusters. Another is the maximum approach, the distance between the farthest samples from different clusters. We have group average, where you find the distance between each sample from one cluster and all the samples from the other cluster, and then take the average of all those distances. The most popular one is the distance between centroids. And also, we can have something based on objective functions. You can define the distance based on some objective function. And also, Ward's method uses the squared error or SSE. Based on the different approach that we use for finding the distance between clusters or samples, the overall results would be different. And the final cluster will include all the samples.

Is it possible when you are doing it, if you do the agglomerative approach and the divisive approach, would you get the same results? Are they the inverse of each other? The question is, if we are using the same distance matrix and the same approach, should the agglomerative and divisive methods give the same results? Most probably, yes, we would get the same results. If you use the same distance matrix and the same data points, applying the agglomerative approach and the divisive approach should give the same results.

What are the most important applications of clustering? Outlier detection. That is also important. Think about the centroids, or representatives. What is one of the important problems in machine learning? The curse of dimensionality. So we can represent a set of samples by one representative sample. This means that we can represent high-dimensional data in lower dimensions as well. So we can apply clustering for dimensionality reduction. Also, outlier detection. And it is a useful approach for data compression as well.

Now, let's move to density-based clustering. The most popular algorithm in density-based clustering is DBSCAN. DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. In DBSCAN, as I mentioned before, the clusters are regions of high density that are separated from other regions of low density. That is the idea behind all density-based clustering. DBSCAN is a density-based algorithm. So the question is, what is density here? When we talk about density, we should have something to define or show density. The density is the number of points within a specified radius, called epsilon.

We have three different types of points. First, a core point. A point is a core point if it has at least a specified number of points, called min points, within its epsilon radius. So we say, to define a dense region, we must have at least min points number of samples. If a region has at least min points, we can say it is dense. These core points are at the interior of the clusters.

We also have the concept of border points. A border point is not a core point, but it is in the neighbourhood of a core point. So it is in the epsilon neighbourhood, but it is not a core point itself.

And we have noise points. Any point that is not a core point or a border point is a noise point.

So assume that these small circles are our data objects, and min points here is seven, and this is our epsilon radius. So we say A is a core point. Why is it a core point? Because if you draw a circle with radius epsilon around A, it has at least seven points: one, two, three, four, five, six, seven. And B, is B a core point? No. It is not a core point, it is a border point. For which core point is it a border point? It is a border point for at least one core point. Do we have any other core points here? If you assume that this circle with radius epsilon, we have one, two, three, four, five, six, seven, eight. So yes, this one is also a core point.

Is there a condition where if a point is very close to another core point, one of them would not be a core point? No, we name each data sample as core, border, or noise points. We treat them separately. And based on the distance between the core points, we make the clusters.

So now you have identified what are the core points, border points, and noise points. Is the core point just one or all of them? No, you should check it for each point, for each data sample.

For example, if we assume this is our data with epsilon equals ten and min points equals four, we can separate the points. The core points are the ones that have at least four points within their epsilon radius. The border points are those in the neighbourhood of a core point but not core points themselves. And the red ones are the noise points.

Now, the DBSCAN algorithm. We form clusters using core points and then assign border points to one of the neighbouring clusters. So we make the clusters from core points and then assign the border points to those clusters. The core points determine the clusters, not the border points. Border points are only assigned to existing clusters.

At the first step, we label all points as core, border, or noise points. Then we exclude the noise points. We put an edge between all core points that are within epsilon distance of each other. So we connect the closest core points to each other. We make each group of connected core points into a separate cluster. Then we assign each border point to one of the clusters of its associated core point. This is computationally expensive.

So every border point is inside the neighbourhood of some core point. Then we assign each border point to one of the clusters of its associated core point. You can try it by yourself. If you use scikit-learn, there are modules for DBSCAN and for other clustering algorithms. You can try it and play with different values for min points and epsilon just to see the differences.

DBSCAN can handle clusters of different shapes and sizes, and it is resistant to noise. At the first step, we excluded all the noise points.

Now, moving to distribution-based clustering, also known as Gaussian Mixture Models. This is distribution-based clustering based on probability theory. The idea is to model a set of data points as points that arise from a mixture of distributions. It is very similar in concept. Assume that we have some data samples or data objects, and we assume that the data objects come from different data distributions. But we do not know the parameters of those distributions. For example, if they come from Gaussian distributions or normal distributions, the parameters are the mean and what? The mean and variance. So, assuming we do not know anything about those values, we try to estimate or guess the values for the mean and variance. Then we assign the data samples to each of those groups or distributions.

So, clusters are found by estimating the parameters of statistical distributions using the Expectation Maximization algorithm. It is more like modelling the process that generates the data. So let's say we have data samples, and we need to determine which distribution each sample belongs to. Just by looking at the data, we can say it is a combination of two normal distributions. What we are going to do is find the mean and variance of these distributions, and then assign the samples to clusters based on their probabilities.

So, this looks like a combination of two Gaussian models. This completely describes two clusters. You can compute the probability for each point to belong to each cluster, and then you can assign each point to the cluster for which the probability is highest. We need to find, for each cluster, the probability to see which cluster or which distribution a sample most likely belongs to.

Here is an example. Say we have some data objects. If you know the source of data, it is easy to estimate the parameters. If you know the properties of the data, it is easy to find the mean and standard deviation or variance. If you know the parameters, you could easily assign each point to the distribution with the highest probability. But now the question is, if we do not know the source and the parameters of the distributions, what should we do?

This is where the Expectation Maximization algorithm comes in. It is an iterative algorithm. We start by randomly initializing the parameters. So here, for example, assume that we have two clusters or two distributions. We need a mean and a standard deviation for each. Initially, we randomly assign some values for these parameters.

Now, for each data point, we find the probability: the probability of the point given distribution i. This can be defined as a kind of weight. The weight of belonging to the blue group, for example. If you assume that the weights are between zero and one, then one means that the sample completely belongs to the blue group. And 0.5 means it is between blue and orange. This is the Expectation step. Then we compute the probability for each sample to be in each group or cluster. It is a kind of soft clustering because each sample here can belong to multiple clusters. It is not hard clustering. Then, we update the values of mean and standard deviation to find new parameters. This is known as the Maximization step.

So, let's say we have two distributions or two clusters. Our K here is two. First, we need to find the probability P of belonging to the blue group, and for the orange group as well. Then, we can write this probability using Bayes' theorem from probability theory. Then, we need to update the mean and standard deviation.

For each sample, we found its weight. Using all the samples and their weights, we compute the new mean and new standard deviation. We initially started with random values for the mean and standard deviation. Then in the next step, we updated them. And after updating them, we compute the probability again using the new mean and standard deviation. Then we check which probability is higher: the probability of belonging to the blue group or the orange group. So that is the Expectation Maximization algorithm.

To summarize the EM algorithm: we need to define the number of distributions or number of clusters, and the initial values for the mean and standard deviation or variance for those distributions. Then for each sample, we find the weight. After that, using the weights, we update the initial mean and standard deviation. Then, we check the probability to see which distribution each sample most likely belongs to, whether the probability of belonging to one distribution is higher than the other.

Now, the final topic is about cluster validity, or evaluation of the clustering. How can we evaluate the clusters? If we did some clustering and got some clusters, how can we determine the quality?

One approach is to compare the distance within the clusters, how close the points are inside each cluster, and then compare between clusters. So we can use SSE. We compare both the distance within a cluster and the distance between clusters. Any other ideas based on what you have seen from supervised learning? We have accuracy there. But here, for accuracy, you need to have the gold labels or the true labels, and here you do not have expected output.

One simple way to evaluate the clustering algorithm is to treat the clustering as classification. For part of the data, you label them by yourself or by an expert. Then, you can compare the cluster labels with the true labels using those metrics that you have seen before, like accuracy. For example, if you have some samples, you say, okay, this is true positive, true negative, false positive, false negative. But in clustering, you do not have true or false labels. You can convert it by saying, these samples are in the same group, or they are not. And then use classification metrics.

Numerical measures that are applied to judge various aspects of cluster validity are referred to as cluster validity measures. To evaluate clustering validity, we can use a supervised approach or an unsupervised approach. The supervised approach is like what I just explained: you need to label some of the data by yourself and treat the clustering task as a kind of classification, and then use classification metrics.

In the unsupervised approach, you do not have access to any kind of labels. In the unsupervised approach, we measure the goodness of clustering without respect to external information. External information here means the labels. One important metric that we can employ is SSE, or Sum of Squared Error.

When should we apply the supervised approach and when should we apply the unsupervised approach? It depends on the problem and the sensitivity of the task that we are working on. If it is a very sensitive task, we need to apply both approaches to make sure that everything is fine and the results are reliable. But most of the time, the unsupervised approach would be enough, because the supervised approach is time-consuming and also a bit expensive.

For cluster validity measures, using the unsupervised approach, we measure two aspects: cohesion and separation. Cohesion measures how closely related the objects in a cluster are. It is about the intra-cluster distance. For example, you can compute SSE, the Sum of Squared Error. Cluster separation measures how distinct or well-separated a cluster is from other clusters, which is the inter-cluster distance.

For cohesion, we usually use SSE. The SSE formula is: for all clusters, for all samples inside each cluster, we compute the squared distance between each data point and the centroid of its cluster. So Mi is the centroid of cluster i, and X is a data sample inside the cluster.

For separation, we use another metric named SSB, or Sum of Squares Between. For all clusters, we take the number of samples inside the cluster, Ni, times the squared distance between the cluster centroid Mi and the global mean M. So M here is the global mean of all samples, and Mi is the centroid of the cluster.

For example, let's say we have four data points: one, two, four, five. And the global mean is three. One plus two plus four plus five divided by four equals three. If we have only one cluster with all four points, SSE would be: (1 minus 3) squared plus (2 minus 3) squared plus (4 minus 3) squared plus (5 minus 3) squared, which equals 10. SSB for one cluster would be: 4 times (3 minus 3) squared, which equals zero. Total SSE plus SSB is 10.

Now, if we assume that we have two clusters. Cluster one contains points one and two, with centroid 1.5. Cluster two contains points four and five, with centroid 4.5. Then SSE is: (1 minus 1.5) squared plus (2 minus 1.5) squared, plus (4 minus 4.5) squared plus (5 minus 4.5) squared, which equals 1. SSB is: 2 times (1.5 minus 3) squared plus 2 times (4.5 minus 3) squared, which equals 9. Total is still 10.

So, based on SSE, which clustering is better? K equals 2 is better, because the SSE is smaller. We want to minimize SSE but maximize SSB. The intra-cluster distance must be minimized, and the inter-cluster distance must be maximized.

The total is always the same. So, if you need to have more compact clusters, you should give more weight to SSE. But if the separation between clusters is more important, you should give more weight to SSB.

A graph can also be used. It shows the cohesion and separation. Cluster cohesion is the sum of the weights of the edges within clusters, which is the intra-cluster distance. And cluster separation is the sum of the weights of the edges between clusters, which is the inter-cluster distance.

The last measure to evaluate the quality of the placement of a sample inside a cluster is how good the placement of the sample is. This is the silhouette coefficient. The silhouette coefficient combines the idea of cohesion and separation, but for individual points.
