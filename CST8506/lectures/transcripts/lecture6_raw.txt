And this is kind of... Self-clastering and the... So, each stop, and low school, different lessons, see. So, for example, here, if we assume, let me see, these are our data checks, or the stuff. Then take partition, class games, so we can have, you know, these three clusters. Okay. And there is no overlap between classes. See. But sweet, all right, faster. We've had our podcasting. And say, for example, you say, okay, P2 and P3, point 2 and PG. It can be depending for the people, one faster. Okay, then P4 and P 12 P3. Okay, so it can be another bastard and P1 with no, these classes can make another class. Or, let's say, English, you say, okay, P, one, and Fito can make a cluster. Okay, and also need to, and keyboard and make our faster. Okay? And at the highest level, those 2 clusters can create a bigger cluster. And in each of these traditional heroic and dressing or non-traditional dress, a 100 fantastic, okay. At the lowest level, I below, sir, each sample is considered as exhaustive. Okay, so, for example, here, say, okay, P1 is the SNP2, the classic PT, and the Manifeston. 10 wonders. Oh. Okay, you say okay, P2 and P3 are more piloza or more senior to each other. So, we can put these two in another faster. And then, in the higher element, we say, okay, keyful is much similar to this class. Then I say the one point similar to class. And is cited, Claska, we have different data points. So when we say simularity, between similarty between poems, is clear. If you have two points, two data points, you can simply find a similarity between those two points, right? If my breaks up on, let's say apply nutrition distance. When we stay here, P4 is similar to this cluster, P2 and P3. How we should find this similarity. And you are teaching people, and P2, or P4, and P3? Or none of them. Something else come here. Yeah, we said your refers and tip of the cluster. The similarity between point and the representative of class. So this representative can be for... in any way. For example, if you can take the mean of the objects inside the after, we can, you know, take the maximum point, the biggest point inside the garden. So, okay. So. That wasn't a different types of contrast. So over the clustering, I've written, all classing, or class, 2 troops, can we get your ideas to be? Or hierarchy? Okay. No, what are the types of class terrors? The clusters, plasters, the groups. The groups, by themselves, can be in one of these four factories. The best separation pastors from the type being investor, but... I mean, it used to be clusters, density-based clusters, and... So now, what are the... for groups? So, 14 minutes, separating clusters. Very, very, exactly. Safety plaster, military, it does, it is a set of foods such that any point is that certain importantly to history are fasters. Okay. It's closer to every other point you can be classed, then to 8 point nothing disaster. So, now, to see, all the points in these 3 clusters, they are closer to only on inside those clusters. Right? So, is there a weird separate? Uh... You sicken types of the classes? There is prot high. So... So, the prototype is, as I say, the prototype based class, the class, is a set of objects such that not chick in the surf, is closer to the prototype. Or centre of a cluster, prototype or centre, it's the same as the representative of a classroom. Then to the centre of any other clusters. Okay, so all the points inside these clusters are close to the centre of this cluster, rather than you know, central business, or... And similarly here. Okay. For the representative of the master, we can take the... object in the centre of the... clusters, we're calling as century. Just simply by tasting the mean of the objects, you can get the sentry on class or order representative of the classes. Or we can take the... Maybe... In that case, there gives a tough surfers, you encounter... We are about stuff, you know. You're a decent way. So you can take the, as I said, the maximum value, okay, the biggest object inside the faster, or the, you know, the smaller subjects, is kind of guided, so it's something. But it is separate, it's made of the objects, and we both, that is what I'm most... So forward, continuity base... Sometimes of the nearest major organ, say, faster, just sit up, please. So, at the point, in class, it is closer to one or more other toys in class, and to any point, nothing disaster. So here's the distance between between the data samples, between the data samples, and other data samples is likely saying, so is much. Okay. Not the distance between the data samples and sent through it. Okay. So here we have 8 different clusters. One, three, four, five, six, seven, eight. That's. Those, we are... separate trust. Okay. Any question? It's not that difficult. So now we have the the city-based clusters, so which is the base for the deepest kind of algorithm that you can see in the next. Here. And then, see, take this. Clusters, the clusters are based on the density of the points, and they are separated by lowness regions. So, for example, here, you see the density of the objects, the, of course, here is hard. Then we have low density space here. No, there's space here. So, there's region. With the high density. okay? As we're here, as we were here, so, and all of them are separated from each other by low... Okay. Okay. Any question? No question. Okay. Now, let's have a look at different classing algorithms. Actually, no, the classic Albertans are not limited to these few ones. Yeah. There are lots of other algorithms, you know, ranging from the very simpler ones to very, very advanced ones. Okay. But these are the most famous and most popular algorithms that you can at least use them as a kind of baseline. Hey, so, we are going to have a look at kings and hire a product, yeah, actually, this is the only thing. So, we won't, we really need variants, like, kings, plus, plus, and by 6 kings, AI, but, and then you will have a look at the, team. Clustering, education, as a kind of harder, and in, it is kept for density based, and finally, distribution-based networks, or PN expectation, maximization, factory. Okay. So the most simplest fasting and the basic algorithm, basic classing, our version is K means. So I believe you all are familiar with canes. So you know it. It's a partitional clustering approach. So there isn't applied the kidneys. There will be no order that between the different class. There's 15 to be that resulted classes. You have to define the K. Just finding K as the number of clusters that you want to see at the end. It's okay, is it parameter or hyper parameter?? Kate? Hi. Hi, um, K is a hiker from. Do you specify you? Okay. Uh, each class is associated with this century, or... And each point is signed to the cluster, if the closes a trade or you really see, the basic average is very simple. So I key policeial et cet How we usually selling those K-points? Sure. The first one up. Okay. So, the synchronous brain is just to select him and run. Okay, so, and also you can use the heuristic algorithms, some heuristic algorithms. to find the optimal values for optimal of data objects as the initial values for these centuries. Okay? So then you already agree, the form case masters, by assigning orphans to the closest country. The computer centre of each cluster, argue with, century, no change. So, it's really that. Our data distribution is something like, And we said, okay, pay is free. Right? And... That's gonna be, select these tea objects. Like... Then, at the first explanation, all the samples or the other samples must be assigned to one of these triple, based on the existence to be certain, right? And in the 2nd tradition, the 2nd tradition is central. You should recomputer century, how it should be computed. Okay, okay, okay. So, yeah, simply by taking the average of the log chicks in terms, in each cluster, in the 1st situation. So then a 3rd tradition. We should do the same thing, forward, eight, six. And, yeah, after 6 iteration. It can be east, I know, east class terrace. Okay, so... Now... Okay, the look means, like, in 1st situation, he said, okay, the send, the initial centric, he said, these 3 are our initial centric. So, this means that At the end, if I put the sentries in any herd, after some iteration, I will get to this pastor. I will give these clusters. Or it depends, the final results depends on the initial centuries. If you don't select the closet, it may cause some issues, it may not converge, but like if it's, if it does, then it won't get to the location. Okay, but it depends on the, you know, the initial century. Okay, so if you select different centrates, okay, it's not a heat that after 6 or 7 iterations, you get the same results. Yeah, maybe, maybe, okay, maybe after, if you say, okay, we don't have any limits on the number of iterations. At the end, it will come right. okay? But since usually we have some medication, okay? Because of that, we say the final results depends on the century, the initial century. Right? It's, you understood? For the, it's similar for the optimization algorithm. So, if we don't have any limit in computation powers in time, maybe say, okay, any kind of optimization, at the end, will the results in the final or global chemical. But since, usually, we have the limitations, okay, on our resources. So we say, no, No, all the optimization algorithms depends on the initial state. Right? One question. So for this example, we have only one feature, and then we close there. We have 2 pictures. and Y? And those points, they have the values of those XMY. Yes This one, okay. So, one is one and zero. how they found foster. how they do it they do the same thing? Yeah, just basic. So the only thing here, that's the only thing that you do here is just, you know, finding a distance between the points, data objects are separate. And recalculating the centuries. Okay? So for recomputing centuries, it's similar. So you have some vectors, some. Say, A one, A, 2, A, A, 4, A, N, N, A, N, A, N. Yeah And, and, and... So, recalculating a centrate, it's very simple. just Element twice, ending, addition, and then, at the LGY, the number of the data, geez, for example. A. Do I do my everything? Right? I assume that we have any samples here. Then to find the distance between set frames, so you can just simply apply the euplicate descent. between any victim, any language. I used to compare, get the, the, the average of each cost, before you... Oh, sorry. To find this central, if you calculate for each coffee, the average of the points in that cluster. Yes, before you reassign. No, no, no, after I started it. So at the at the purchase of you just select some random century. But then you make the clusters, right? You know, assigning the other object, data objects to one of those centrates. Then recomputing the new centric by arranging the old samples in inside the cluster. Then, again. Okay, so after finding the, you know, they said, right, you need to assign you all the other sample to the new centuries. And similarly, you need to repeat this process. Okay. So, this is, Caitney, lasting algorithms. Any questions? Any other questions? a broader question, but especially at higher dimensions, you start losing a lot of, like you start, distance will start to kind of average themselves, even wilder, low distances. sort of start. Or it's much harder to different choose than anything becomes almost equities apart. And at that level, are there any alternative algorithms that we foundation to be able to deal with this better than Canada? Actually, I think any questions. So essentially, I'm hard mentioned, okay, the distance between points becomes almost like my points become almost equitous to each other because there's so many dimensions, so are you clear? It could be sense. They're kind of becoming political, essentially, I mean, think, no, I agree, because, you know, by increasing the number of directions, is, you know, when you take them, are sketchy, they are going to be sketchy. So be. I mean Can I tell you one movie? Usually we're on the word that I mentioned, the, like that, at higher dimensions, it's, like, volume start rapidly and it becomes a question of... So, what they usually send the words, it become hard to solve just a little hard. It's hard to tell differences between. Eifference, I want, different, we can, different, electronics. Essentially, it's, if you have, if you think of, It's, it's hard. easy to see separation between individual points, but it's harder to know whether an individual points are partly cluster air supposed to be, just usually something like completely distance. That's exactly. Like, would it be potential? Could it be potential? That that difficult. So, you know, the case is, is, is... But, but it's not the most efficient. There are lots of other algorithms, not trusting, and we think that you can apply them. For, you know, from different scenarios. for the different data, different natures, from different properties. But in any case, in any case, for any kind of dental, in any direction, you cannot die being king exactly. But the reason is natural, you get the best reason... Have you got a problem with the kidney? So that's only one problem. Everything depends on the initial, except for its initialization or the initial situation. Select a number of K is? So what was the 1st time problem of the cake means is the cake. Choosing the this magic for key. Okay. So, we don't have any other information about the data, okay? You can advise. Do not... say any... you know, any... you cannot find, no, any... proper or good values for key. So the key, the value for key. is also problem. Okay. So, it's a centric stretch of algorithm, shows the initial centro, it's repeats reputation, and... So, see, algorithm. Asia centuries are often chosen in a movie, we talked about them, class and produce can rate from one, around to another, you know, Since we are choosing the central, it's rather the initial central level. By run by run, you will give different, you know, the, pretty good. The sentries, technically, the main of the police. Canings and converge for how many stunts measured, it's completely defined central. If you don't have it, if you don't have any limit, the time on, you know, the iteration, at the end, everything would be covered. So, but most of the time, most of the time, the convergence happens in the 1st few iterations. And... The complexity of the pain is, oh, wow. Any hires, K, K, I, I, D. There's only 3 trainings, here, the number of thetology. So, in case, the number of us, there's, like, you know, graduations, a number of HGB switching it up, that is cool. Instant title consume so. So, but, okay, now, assume that we applied to K means that we cannot, and we got some, you know, some clusters. Okay, you use K equals to tree, your friend use K equals control, K, some other one, and you also, you know, select in the different initials and toids, and finally, most of you, okay, so you get some results. Now, how you can compare the results. Which one is better? Okay. Here, there is enough, there is a function called some square. Okay. South is square, so... It is some squared earth. We can go find the sound store, you know, for different class drinks. My thought, the results are different. It can only compare these some square errors for different clustering algorithms to each other to find which one is... So, for each for me, the arrow, here is the distance to the nearest cluster, centre. Okay. So, it says it all, but it's not at all, in fact. Okay, just literature, konit, error, but it's not error, because for the, for talking about the errors, we need to have some gold, something. Right? But here we don't have any phone labourers or uniform data. Okay, but it says, say, hello. So to get SSE or SunSquared error, we squared these arrows and sun, so that would be sunny. And see, the X is data point. So CI is cluster I. And K1 is the centrate of the cluster. So for all the data samples. So for all the classes, you have cave. So for all the, for all the samples inside, each plaster, we find in square and all, or square distance between the sample and the responding between samples inside the cluster, and the corresponding central. I will get some very... Are you square each distance or are you swearing the sum of the distance? You don't reach these things. Scoring each. What are you doing with the air? Are you adding it to the distance? What are you doing with SSE? Yeah, just we wanted to compare. Okay, so let's say. You are Let's see, you applied, came in as algorithm, and you got something like this. And I applied to changes that, which, and I... Now, I want to compare these two trusting results. So how can I say, okay, which one is better? What can I say? And you need to have me three. So as I say, you basically... Yeah, basically, SSC, just after we were just heard, we need to find SSC for this one, then while we SSC for this one, then we compare them. Okay, so which? So, if you want to sing, is, is, is, is, is, this one, the SSC must be high. Other than this, the SSC for this one should be higher than this one or the street sword. Smaller models, because they're increasing the distance. Should we... Smaller than... Okay, we are looking for the smaller values of the. Any questions? No question. shit. Okay. Now, let's come here and take you... Eric is just... Okay, as I mentioned before, you have a hard class in my location. Produce and set up on the nest and gasters, organized, and a hard car treat. I became visualize a hierarchy called clusters within program. Please start, you know, as general. So, tree life, diagram, that record the sequence of marriages or experience. You see, for example, here, and first of all, each of each sample by itself is a cluster at the lowest level. Okay. Then, the next, for example, here, in this example, one did suffer, one, a little country, may be faster, one. Then also data suffered through and filed. They create the cluster, too. And we offer that it, 4 and the 2nd the cluster, too, create the other cluster. And then these 2 clusters, 3 and one, make the other faster, and finally, get the sound of 6. And, you know, to the forest caster, make the... Much bigger chest. Okay. So, it can visualize these... Okay, so I see. I mean, 1st step, all of the all of the data samples are Clustered by itself. In here, we could be men, one, three. Okay, based on this similar, to the recent day, is that... So, it means that, you know, this, uh, 3 days, one and 3 is less than distance between, you know, any other samples with the other side. Okay. So, also here, two and five, in least 12 points, each other. And, and we, I've heard of it. We met before and for we discussed, or we discussed. Maybe, man, you know, he's trusted, he's trusted. Okay. So, in the search of the hierarchical classes, it's exterior, so... Do not have to, we didn't assume, you don't take any assumptions on the number of clusters. So we didn't have, we didn't see any kinds of K or the number of clusters. Okay. So, that's one of the experiences of being... Her, her blessing. So, a desired number of plastic can be contained by cutting individual drum at the proper level. Okay, so for example, if you cut the... You will get six clusters. If you cut it at Islam, you will get one, two, three, four. If you cut it here at this level, okay, you will get one, two, three. it. It depends our... us... in which level we want to investigate the disasters or samples inside the clusters. They may correspond to a lean for testimony, so we'll be in bio, bio science, or head science, you know, or we use this kind of data. Or, you know, reconstruction or in DNA analysis or something like these final patterns. So, it's different now or higher, I think things are much popular than the other kind of partition. Okay, now, how can get, or... I can get these, those kind of hierarchical clusters. So, we have 2 main types of hierarchical plastics, so, running in a, a girl, very cute. Red holes and noisy metals. We will have a look at this, I don't know. So, in this metro, or in this approach, we start fixing points as video clusters.. And if the closest pair of glasses, aren't you only one faster or K fasters lift? least such small each and future faster. Okay. Combine them, things... It helps each other, too, aren't you, well, until we got only one... So, the widely approach is something reverse. Okay. So start with one all-inclusive clusters. So at 1st we have only one cluster. including four the samples. Oh. And at each step, we split the cluster on each classes, contain an individual, so the aggravative approach is... Bottom of approach, this device approach is tapped up. Uh, traditional Hungarian burtons uses similarity or beast asmetrics, okay? For the merging or splitting the cluster. Okay. Okay. For me, I will, I will relative classing. I mean, the key, I get a success, if you manage, trust this class. By the way, sometimes for the castle, maybe I'm writing classing, we permit any kind of, you know, the approaches, with any kind of approaches. Usually we do kind of post-processing. to improve the results. So in the class stream, talking, the post-processing is dies, just combining these similar orchosis. Clusters with each other. or separating or spiriting the big clusters to, you know, to, to multiple clusters. Okay? So one of the usages of these aggravated algorithms, agglomerating, regions or approach, is for doing the post-processing on fasting. Okay. So the basic algorithm is very simple. Compute the distance project. Sometimes you may see something like proximity matrix in literature. So you know what's the proximity?les. Yeah, it's kind of distant, so it's a kind of distance project. So... Then computer distance market, then each data point will cluster, then repeat the nurse, 2 closest clusters, update the distance. Until only a single plaster, reading. So the important thing here, or the key operation here, is to find these terms of two translators. How to fight the distance between 2 passes. So, and there are different approach to the final vistas between the masters. And in those deep front approach to finding this, you know, the distance between different clusters can result in different clusters as well. Step one and two. Let's think we have these samples here, see one or two. You can find these fence matrix, like the... SO, for each pair of the samples, you have to find their distance, just simply a, like, you can be done. So for example you want. Zero, okay. he on YouTube here So this is the 1 food. Then after finding, after finding the, you know, the distance between each data sample, you need to combine the closest samples. Okay, so in intermediate situation, you know, have something like this. So, some suffers, we've been merged, to construct different clusters. So, like, C, one, C, 2, C, 3, C, 4, C, the button. Then we need to, for example, here, let's say something, like, if you want a P2, C one, PTM, PTMP, for, results in C2, and so. Today, you need to do what? We need to recalculate this distance, but fix, but, no, not for itself. Four flashes. Okay. So, when I say faster, so... But it's not if we have... And I say, finding the distance between these two clusters, and you see each cluster has different samples in different positions. Okay. So to find a distance between these two clusters, there are several ways. The simplest one that you know is what? Can you understand? Okay. Yeah.'re just probably because this is between food. Like they say, you're just a bar median, you know. Okay, so the simplest way is just to find the centre of this one, a central, and this one, and then finally discuss between those 2 centuries. Taking the average of samples, it is crushed, there's an average of samples in discretion, and finding these, he starts looking to... I mean, it's the simplest one. But there are some other approaches, but, okay, for example, the distance between the closest samples from two... If he stands between the far, the sun... And depending on the types of this test, this test, calculation approach that you take. The results would be different. Right? Now, we want to, for example, here, we want to match two closest clusters, people, and symphony. Okay, and update the distance project. Okay, so that means that we will have 4 incident, 4 blood classes, to make system. You will have 4 clusters. C1, C2, C3, sorry, C1, C3, C4, C 0 N, C4. The top five plasters, you have four plastic, C2, union, C5, instead of C2 and C5. Okay, but now the question is, how do we update this test market right now? Okay, or for not here, or, you know, investor, that's what, how to obey, no. The value is on this table, this is tense. So, you see, that we have these two plasters, with these some in each cluster. Okay. So, as I mentioned, we have deeper types of the drum each 4 or 5 pieces. One way is to is our main approach. Okay. The minimal distance between the, you know, the closest samples from different, the distance between the closest samples from different clusters. Right. Sit about it. And seriously, having marksuit marks approach. So, the farthest sample, the established in the farthest, that comes from different... We have group average, Okay. Just finally, you know, we distance between the samples, between each sample, and all the samples between each suffers from one cluster and all the samples from the other classes. Okay, and then take the average of all the expenses. Sorry. And they, most popular, like, distance between central. And also we have, we can have something based on objective functions, you can define some objective functions. You can define the distance based on some objective function. And also the work method, use the squared error or SSP. So... here... for people, I mean, let's say... You know, the examples for the other one, especially.. So, based on different approach that we use for finding a distance between the classes or sample, the overall reasons... would be different. Okay, so, for example, so, we have, we have, maybe that's a lot, this time, okay? And the end... see that... Different, okay? So, um, and... No... The final cluster, we include all the surface. Any question? Is it possible when you're doing it to your purchase, the device and Apple? If you do one after the other, would you get the same, are they invert for each other? Do you get a place of food? You just seem... He's trying to copy the distance. Yeah, okay, so the question is, if we were saying distance, perfect, as, say, approach should find the distance between the points, and then you go like... Yeah. Yeah. Most probably, even, we get the same results. If you do the same distance projects, and same at point. So we get these things. But applying the agnovated approach, and also the device. Okay, any other questions? No. Uh... What is the most important applications of the class terrible? Outlet detection? Okay, so it's a complete centre. It's also important, no... The application. Think about these centuries, or representatives, and, uh, there are potentially, like, money here. But it's the definition of the industry. It's the mystery of just, we just find, you know, a similar, we just put a similar object in the same fish. But you know, in reality, what would be the important application of the clustering? But think about the centuries or representative trusts are representative. What is the one of the important problems in machine learning? I give you... Hairself. Hairself. Care yourself. Care yourself. Curse off dimensionality. Oh, is that a... Care yourself dimensional, okay? Right? So, but we, so, really, we can, you know, maybe we can reach and set up samples, but by one sample. Okay, so it means that we can represent, you know, high dimensional data, low dimensions as well. Right? So we can apply it for the dimensionality reduction. Also, the outer machine, I mean, is that. So what approach for data computation? Is important. So we apply in Singapore. Okay. Any questions? No, questions, reaction. No. Okay. Now, lips moving to the density base, clustering, I'm going to, which is now as DB, the most popular algorithm, and density based clustering, okay. So, very simple, in this TV discussing, as I mentioned before, the classes are regions of high density. that are separated from moder river regions on lowensity. That's the, you know, the idea behind the old density-based classroom. The important or the famous algorithm in this category is TV scan, okay, or density based, special class, you know, application with noise. Okay. So, this is now as the... It's a density-based algorithm. So, now, authority is, what is the one density here? Okay, so when we talk about it, density. We should have something to define or show it density. right? So, indeed, the density, in a number of quoting, we can specified radius or zero. Right. So, we have 3 different or different types of samples. First of all, it's a cold food, of course. To say, it points to a four point, if it has activities, it specify the number of points, min points. So we say, okay, we say, okay, they, let's say to define the, to define it, we must have at least 5 samples. To say, okay, this region is dense. If it has, at least, distributor, number, points, we can see what order you can use. So these are points that are at the interior of the classes. And fund the points quite themselves. We also have concept of before people. So the borderoint is nothing for. So all the points, other than 4 points, then we are important, one or noisy, okay? Is not a core point, but it is mixing the neighbourhood of a core point. So it's in the neighbourhood, you will see the next slide. Okay. It's in the neighbourhood, but it's not the corporate. Um, we have a noise point. Any point that is not corporate or border foot. Okay. So assume that these, you know, small surfers are our data objects. Okay. And the main points here is seven, eight. And this is our coseros. The range is... Let's say, okay, so we say corn point, okay. This one is A, A is core point. White, corporate. Because it has, if it assumed, if you know, your circuit, with reducive zero, okay? So maybe I have at least 7.0, one, 2, 3, 45, 6, 7. And B is core point. Is corporate. We cold, cold, cold, cold. No. It is nothing, something, it's the border point, for one, for which cost or for which circle?. It's a border point for this one. At least one. Hey, on this one. There are no good. Right? Do we have any other core point, you know, the 8 here? Three? No.. The, if they assume that, no, this circle with your, you're speaking, is the zero? Yeah. We have only one, two, three, four, four, maybe the point just in your name? Kind of awesome. Yeah, good. This one... This one. One, two, three, four... Seven, two, five, six, seven, eight. Is there, is there a condition where it's like, if a point is to just very close to another, or, like, is there a place that, like, reduces core, not our whole money? Muzz, you say that A, and then that point right next to A, our most 4 points is that, like, because they're so close together, that one of them would not be a 4 point. You know, we call, you know, we named each data sample as coreboard or order portion of noise points. They treat them separate. And based on the distance between the core points, you will make our faster. Okay. Okay. So now you have to stage what are the core points for your point and it's still on, uh, main point. So is the core point, is the best one or all the? No, no, no, you you should check it for each point for each data something. And the blood is it just for representation for? Okay, so for this one, you should check it as well. Can you buy it during the second, previous, you know? Okay, I can't now is somewhere in its neighbourhood. Its very computational expensive. Yes, in. So... For example, here, it assumed that this is our data example, original points, okay? With FC Roman goes to 10 and main points, it goes to four. So we can separate before us. Okay, I please, in point types, It will know, says 44 times, and order. So, I mean, the red one. I think the news for 4 points. Yeah, the green must be... And the red one could be... Nice. No, no. Okay. Now, the, I was going to, it's a, a tedious scan. So, we formed a cluster, using four points and assigned border points to one of its neighbouring clusters. So we make the clusters by core points and then we assign the order points to the clusters. Okay. So the core points determining the clusters, not the border points, border points are the only points that we assigning to the existing clusters. Right. So at the fair system, we labour all points as home owner or English points. Then we neglect the noise points. And then we put an edge between all four points, between these turns, see all of each other. Okay, so this week connect the closest core point to each other. Make each group of connected core points into a separate cluster, we put the closest core points to make cluster, keep different. They assign each quarter point to one of the glasses of its associated for... So, Every border points is inside a neighbourhood. Okay. So, then we assign each border point to one of the, uh, sense of associated. Um, and we will have some, like, these, so you can try it by yourself, just do it. If you use the scalar, you can learn. So. There are modules for the EML, so for DB scan or for other, for other, you know, the classing algorithm tennis, you can try it. Okay? And you can play with different values for the main points of cylons. Just to see the difference between the principles. Okay. Uh, maybe assume it's our personal points is, is, so... Okay. If we not discuss clusters, would be something... Like... So it can handle classes of people, change their size, or resistance to, you know, is weights, resistance to noise. The 1st system, we left all the noises. Okay. Okay, any question? No question. If you want to, have you know, it is called LDA, LDA, or station, one civilization, probably this distribution based, classic, everything based on some, it was recently, probability, theory. So, yeah, the idea is to model a set of data points. With the points that arise from a mixture is... Okay, so it's similar. It's very simple, okay? So assuming that, just assume that we have some data samples or data objects, okay? And we assume that the data objects comes from different data distributions. Right? But we don't know the parameters of those distributions. For example, if they come from the mostly, we use the quotient distribution or normal distribution. So the parameters are the mean and what? Okay. So, assuming we don't know anything about those values. Okay. Because we try to estimate or guess the values for the, you know, for the mean and variants. Okay? Then, you know, A sign, you know, a sign, the data samples to each of those groups. Go distribution. So, classes are found by estimating improvement, statistical distribution, you need expectation, vaccination, and... So, it's more like justice. It's like, I've seen their modelling, the books, that generate the 4 weeks. So let's say we have we have data samples. And we need a sample on data, engineering key side of distribution. Okay, just by looking at these solutions, okay, can say, oh, okay, it's a kind of, it's a combination of 2 normal distribution. But... Oh, what we are going to do, we are going to, you know, find the... variance of, you know, these distributions. Okay. And then aside the samples to, you know, just assume these are 2 clusters. K2 clusters. And aside, you know, the samples to one of these, to placer based on their program. So now... Okay, I think it looks like a combination of 2 more models, you should support the estimating unit... which normal decision. So, this completely describes 2 clusters, so you can see it's 2 clusters. You can compute it from ability, which each point learn to each cluster. Right? And then you can assign each point to cluster, for solution, for which is most troubled. I mean, just, we need to find, you know, for each cluster, we need to find, you know, this probability, to see which cluster or which distribution, our summer, must, you know. Oh. Just bring an example. Here it is, say, have some data that some data objects, like these here, okay? If you know the source of data, it is easy to estimate the parameters. If you know the data, if you know the properties of the data, okay? So it's easy to find the... Let's say, we mean, and start our division variant. Okay. If you know parameters, you could easily assign each point to the closest distribution. Okay, that's what this probability. In highest probability. But now the question is, if we don't know the source and the parameters of the distribution, we don't know the source of the data, we have no information on our data, and we don't know anything about the parameters of the distribution. In that case, what should be? Now, L, Ian, Edward, tell me... It's a tourist algorithm, expectation, and maximization. Okay, so... Do you have other ways to say start to K randomly based ocean? Okay, so here, for example, is assume that we assume that we have 2 clusters or 2 distributions. Okay, okay, question. We, so we need to mean and to start our division already. Okay. So, initially, initialize, you know, we initially we randomly put some value on this, give some measures for these furnitures. But Now, for each point, it's odd, but which point did we have here? We find this probability, probability of... Given this, I... Okay. So, does it look like it runs from Plumo? Or? Just, let's say, we can be defined as a kind of weight. Okay? Wait, you know, very tough belonging to blue group. Right? So if you assume that the weights are between 0 and one, okay, so the bottom means that one means that the sample is completely belongs to the blue group. All between, let's say, 0.5. It's between blue and orange. Eight, expectation. Then we compute it from ability to be in group class or, or are you just... It's a kind of soft clustering because each sample here can belong to 2 clusters. It's not the hard classy, but... Then, we update, you know, these values, main, and external deviation, to find a new one, to feed coins, aside, to hear, it's known as maximization. Okay, now here, just... Let's say assume that we have 2 distribution or 2 clusters. Our cave here is... First of all, we need to find this probability. We need to find this from P of H, I, even, P, or long to the blue group, okay? A course for this one. hey. Then, we can write this, we can write, you know, this P of HI given B, C, in this P, you know, B, given this, it goes to this strategy, probability theory. Last week, we sold it, right? And we heard this from ability and weight. of XI, belonging to... blue clusters. No, we need to update the, what, we need to update the mean and standard deviation. Okay, 40 minutes. For each sample, we found the, from the, in its way. I mean, all the samples, if you want, you love these ones, too, you know, and, okay. So, if you need, what we did, the new meat. Uh, new solar deviation. Can we computers? Right? No, we, we initially, we started by random values for the, Me, for these values for me and standard division. Okay. And then the next step, the next step, we updated them, we updated it. And then after updating them, we compute this probability, which is probability, this is using the remote, you mean, and start activation. Okay, and then check, check, which one is higher. Okay, the probability of belonging to the blue group is higher or the orange. So that's the Expectation, like the musician, algorithm, any question? No question. Uh, why do you look, uh, people's basin? No, be computer-based. Okay, we, we, we, we, and we only need to define the number of distributions or number of clusters, okay? And the initial values for the meat and the sterner deviation or variance for those distributions. The initial values, okay? Then for each for each sample, for each sample, we find the weight. Okay, we find a way. And after that, using the veins, we update the initial mean and summer deviation, okay? Then, the check be, is, the probability. Check the problem, okay? To see which one is higher. No, the probability of the samples belonging to distribution is higher or belonging to the probability of the samples belonging to this distribution is higher. Okay? So, that's more about the... Class stating argaritas. Now, the finery topic is about, it's the cluster, validity, or evaluation of the dust. But before that, so... In your opinion, how we can evaluate clusters. Did we did some class? we got some classes. We want 2 now, then we, the quality of our, how can we? Thank you, and compare the the point between the car, like the inside car, how close they are, and then when we fit. Okay, so we'll see SSD. So I'd say compare both compare both the distance from within a cluster, but this is involved with that cluster versus this is, oh, um, each cluster from this stuff. Yeah, let's say, any other... idea or specific certain based on what you have seen from the supervised learning stuff? So we have the accuracy there. Yeah, yeah, yeah. But here, so for accuracy, you need to have the gold labels or the food labels. But here, you don't have control, expected output. The one of these, the one simple way to evaluate the clustering algorithm is evaluate treating the crust change and classification. Okay, so for part of the data that you have for part of the data that you have here, let's see. You label them by yourself or by the experience. Then, you see, see, the 50 labels, or, let's see. You label them, and even with the results of class, students by using those metrics that you have seen before the accuracy. So, for example, okay, so, for example, if you have some samples, So, you say, okay, this is true, true, arms. But how can now find the accuracy? And trusting, you don't have anything, like these true, true, follow, follow.. You can just convert it, okay? So you say, okay, this is true, this is true, this is true. So it means that all those 3 samples are in the same room. For vice versa. And these 2 are. that we can So... Numerical visions that are referred to get various aspects of the crest, right? PTR, just pointing to, follow, touch, so... to evaluate the crest string, very tea, we can read it to a push. Otherwise, uh, once you provide the supervised approach, you're doing it. on that. I'll supervise of as internal... Supervised is, like, the one I experience. Just you need to label some of the data, some parts of the data by yourself, and treat the grasting task as a kind of classification. I'm then employed to do things that... But in unsupervised approach, you don't have access to any kind of labels, right?. And the unsupervised. So we ask more about approach, you measure the goodness of classing, transactions. We don't respect to external information, external information here means the labels. So... The one important, matrix, that's which an employee is SSE or some of a square error. Then we should apply the unsupervised approach and when we should apply the supervised learning, supervised approach. It depends on the problem. Okay, I, I, I, I, sensitivity of the problem. Okay, that we have the sensity of the task that we are working on. If it's a very sensitive task, okay? So we need to apply both approach. Okay, to make sure that everything is fine and the results are reliable. But sometimes most of the time, So, it's a supervised approach would be enough, one, because supervised approach, the game is 1st time consuming and also a bit expensive. So for us, for blood measures, we have two metrics, to be forbid unsupervised approach, we measure 2 aspects of the, a cohesion and separation. Okay. So, the closer pollution measures how crozy rated are objects in clustered. So, just, it's about the intra claster distance, right? The 1st... So, example, you can fly, assist your sun squared error. And cluster separation, measure how distinct or where separated the cluster is from other cluster or intercluster distance, right? Here, for collision, we usually use the SSV, okay? And for separation, We usually apply distance between the clusters, okay? But it is for, uh, 18, 3rd self squares. This is for SSE, okay? So, SSC is... Okay, so is there a question on SSC formula? Is it curious? Okay, so MR. So MI is the centroid of cluster, the body, and our data sample inside the cluster. Okay, so for this, this is for, this is week. Uh, for for this separation, we add another, another metric named as SSP. So, she is her beautiful industry. Okay, so for all clusters, for all classes, cinema. Number of samples inside the cluster, or the size of the one of the clusters. N minus MI. Okay, so what is M here? And here is the global mean of the south house. Okay? And the MI is again the, but centroid or the representative of the cluster. After finding these two, maybe just some... Let's see, we have these four points, one, two, four, five, season. We have four data police here, okay? And 3 is grow on me. Here, okay. One plus 2 plus 4 +5 divided by 4 equals to three. It turns up one cluster, if you have only one cluster, you have already one. SSE would be something like this one minus... Three, in one cluster, it is sent, it means that all the samples are inside one cluster. And the century is the mean, right? Is it clear, or it's clear? Difficult? So, one cluster, you have one class said, one, two, four, five, or one cluster, and the century is cheap. Okay, so then it was, by the necessity, one, minus three, expired plus, two, minus three, squared plus, one, three, squared equals to 10. Right? Now, SSP, for one plus two, for one test. Normal cluster is for how many samples do we have inside the classroom? Oh, you have one pass. A, one... And A minus MI. Joe Biden, media. minus the century of disaster. It's 4 times G minus 3 squared, equals to 0. In total No, if you assume that we have 2 clusters. You have two. So from two clusters, we need to have two centroids. And one, and two. Right. So, assuming that M1 is one.5, I hear, actually, is 4, 5? Okay. Then you can find the SSD. This is a one minus for the 1st one. One minus, one might be, this grows to disaster, right? One minus one.5 squared plus 2 minus one.5 squared. Plus, now, please for it, 5 rows of east cluster. Right? Four minus 4.5 is square +5 minus 4.5 is squared. Right? SSE, it goes to, what, just based on, essentially, which clustering are which is better? Which one is better? All the same? Yeah? Oh, I mean, by the total. Just for you, just, no, just by looking at your system. Which one is? That's the good. K2, K2, what? Yeah, because, you know, it's smaller. essentially, it's in the world, the total is highly taken on with the low SSE. Sweet. If, like, in this case, we're capable of capable, so the total of 2 goals, but, you know, the same. No, no, no, no. There was a scenery, SSP, whatever. the same. But yeah, yeah, especially for coalition, if you only need to look at the equation, so you need to already look at this. So, like, isn't she... So just looking for Yeah, if we only want to know if the only cohesion is important because, okay, so, if only in setting, in trust or distance is important for us. Okay, so we should look at this. So now for SSP here, this is, we have 2 clusters. Normal first class day is true. f is too. The global mean is three minus... three... six with a 1st test there, one coming... Bodies, both, plus... normal piece of cake to us, sir. Two... time... Central, Mississippi, Festa, 4.5 minus global... goes to So here, SSV, it goes to 0 here, it is very close to 9. And in total, total is the same. So based on the SSP, based on the SSP, which one is better?. We need to... Oh, my God. You are tired, including me. So, We need to minimize SSP, but maximize SSP. The inter, intra-class, distance must be minimized, inter, inter-class, distance, okay? Intrust, distrust must be minimized, intertrust, distance must be... Yeah, they have to just, just to show you, she totally is saved, huh? Okay. So, so it's usually, uh, whenever you wait, I'm like, have one message. I don't know, maybe like, you know, he picks up, he picks up, you know, the trouble. If, if you need to have no most strength clusters, so, you need to give some mortgage to SSP, rather than SSP. But if you, if the, you know, the density of the clusters is important. So we should check the system, we should give the more weight to the assist. Uh, let's see, the distance craft, based on... No, it shows the cohesion, and it feels for the separation, all the distance between different places. Okay, so all classic originally is the sum of the weights of the rings between classes or classes separation, in the sum of the weeds, we can know in the classes, and knows outside the classes, that's the intrastraster distance and interfaster distance. Okay, last one. That's one, last measure to evaluate the quality of the samples. inside the, you know, if the sample is placed in the right cluster or not, okay? How good is the place of the sample? So, just evaluating the quality of the place of the summer. So, now, it's the similar coalition, similar coefficient. So, this sealed coefficient combine, it combines the idea from most coefficient and corporation. But for individual for you. I'm referring to my master time laborator... Okay, good. I'll start...