# NLP - Quiz 4

### Question 1

The self-supervision method in neural language modeling avoids the need for hand-labeled supervision signals by using surrounding words as implicit training data for classifiers.

- True
- False

---

### Question 2

While TF-IDF is useful for some applications (like search engines), its high-dimensional nature can make it difficult to use efficiently for tasks like deep learning-based NLP.

- True
- False

---

### Question 3

Word2Vec consists of two main techniques: CBOW (Continuous Bag of Words) and Skip-gram.

- True
- False

---

### Question 4

Most modern NLP algorithm does not use embeddings as the representation of word meaning.

- True
- False

---

### Question 5

One advantage of GloVe over other word embedding methods is that, it is global in the sense that it considers the entire corpus to learn the relationships between words, and local in the sense that it considers the co-occurrence of words within a limited context window.

- True
- False

---

### Question 6

What is the default dimensionality of word embeddings in the Gensim Word2Vec method?

- A) 100
- B) 4000
- C) 120
- D) 10

---

### Question 7

Suppose you learn a word embedding for a vocabulary of 1000 words. Should the embedding vectors be 1000 dimensional to capture the full range of variation and meaning in those words.

- True
- False

---

### Question 8

Is the goal of the Skip-Gram model to determine the central word based on its surrounding context words?

- True
- False

---

### Question 9

Which of the following equations do you believe should hold for an effective word embedding?

- A) e⃗_boy − e⃗_girl ≈ e⃗_sister − e⃗_brother
- B) e⃗_boy − e⃗_girl ≈ e⃗_brother − e⃗_sister
- C) e⃗_boy − e⃗_brother ≈ e⃗_sister − e⃗_girl
