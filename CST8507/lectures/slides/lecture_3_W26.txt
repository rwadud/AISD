CST8507: NATURAL
   LANGUAGE
  PROCESSING

    LECTURE#3
      TEXT
VECTORIZATION &
   SIMILARITY
    DEVELOPED BY
   HALA OWN, PH.D.
Lesson Agenda
â‘Assignment 1 & Lab 2

â‘Introduction to Feature representation

â‘Text representation techniques
  â‘ One-Hot Encoding
  â‘ Bag of Words model
  â‘ Bag of N-Grams model
â‘ TF-IDF model
â‘ Word similarity
                                          Page 2
NLP Development Life Cycle

 Requirements
  gathering




         Gather more         Improve the
         data                model




                                           3
Feature Representation

                               speech feature representation
Image feature representation




                                                               4
Text Representation Techniques




      Image source: An automated approach to aspect-based sentiment analysis of apps reviews
      using machine and deep learning September 2023Automated Software Engineering , then      5
      modified using ChatGPT
Vector space Model
â€¢ Mathematical and algebraic model transforming and representing
  text document in numeric vector.

â€¢ Each word = vector

â€¢ Similar words are "nearby in semantic space"




                                                                   6
Vectors : An introduction

â€¢ A vector is an object that has both a magnitude and a
  direction.




                                                          7
  Vectors : An introductionâ€¦
x = (x1, x2, x3, ..., xn) is a vector in an n-dimensional vector space
Length of x is given by (extension of Pythagoras's theorem)
        |x|2 = x12 + x22 + x32 + ... + xn2
       |x| = ( x12 + x22 + x32 + ... + xn2 ) (L2 Norm)
If x1 and x2 are vectors:
Inner product (or dot product) is given by
       x1.x2 = x11x21 + x12x22 + x13x23 + ... + x1nx2n


                                                                         8
Text Vectorizing

â€¢ Vectorizing
  â€¢ Process of encoding text as integers to create feature vectors.


â€¢ Feature Vector
  â€¢ n-dimensional vector of numerical features that represent a text
    object.




                                                                       9
 Text Representation :One-Hot Encoding

Each unique word in a text converts into a binary vector.
Only one element is "hot" (set to 1) and all others are "cold" (set to 0),
indicating the presence of that word.

                                 â€œThis is an exampleâ€
   Split Text Into Words                                Numerically Encode Words
  [â€˜Thisâ€™,â€™isâ€™,â€™anâ€™,â€™exampleâ€™]
                                                          This             [1,0,0,0]
                                                          is                [0,1,0,0]
                                                          an                 [0,0,1,0]
                                                          example
                                                                            [0,0,0,1]

               Tokenization                                         One-Hot Encoding


                                                                                         10
One-Hot Encoding for Documents




                                 11
One-Hot Encodingâ€¦



Discussion




                    12
Bag of Words(BOW)




Document is represented as an unordered collection of its tokens,
disregarding word order, and syntax etc.., while keeping track of
word presence or frequency

       Image source: https://sep.com/blog/a-bag-of-words-levels-of-language/
                                                                               13
BOW Technique: Count Vectorization




                                     14
    Count Vectorization for Documentsâ€¦
      Input:
      import pandas as pd
      from sklearn.feature_extraction.text import CountVectorizer

      corpus = ['This is the first document.',
              'This is the second document.',
              'And the third one. One is fun.â€™]
      cv = CountVectorizer()
      X = cv.fit_transform(corpus)
      pd.DataFrame(X.toarray(),columns=cv.get_feature_names())
      Output:
                            terms


                                                                    Document-Term Matrix
document




                                                                                           15
Bag of Words(BOW)â€¦

  John is quicker than Mary.            Mary
                               is
                               John       quicker
                                       than




                                       is      quicker
  Mary is quicker than John.          Mary
                                              than
                                       John



                                                         16
Bag of Words(BOW)â€¦




                     17
Bag of Words(BOW)â€¦
           Advantages                                Disadvantages

 â€¢ Simplicity and Interpretability      â€¢ Ignores context and word order

 â€¢ Works well for text classification   â€¢ High-dimensional feature space

  and information retrieval.            â€¢ Sparsity

 â€¢ Computational Efficiency             â€¢ Lack of semantic information
                                        â€¢ Out Of Vocabulary(OOV )
 â€¢ Language Agnostic




                                                                           18
Bag of N-Grams(BON)




                      19
Bag of N-Grams(BON)â€¦
Unigrams are the unique words present in the sentence.
Bigram is the combination of 2 words.
Trigram is 3 words.

                  I am learning NLP

Unigrams: â€œamâ€, â€œ learningâ€, â€œNLPâ€
Bigrams:    â€œam learningâ€, â€œlearning NLPâ€
Trigrams:    â€œam learning NLPâ€

                                                         20
Bag of N-Grams(BON)
           Advantages                                Disadvantages

 â€¢ It captures some context and word-   â€¢ Sparsity
  order information                     â€¢ Computationally expensive
 â€¢ Simple and efficient method for      â€¢ Ignores the overall structure and
  representing text data                  meaning of a text.
                                        â€¢ Choice of N
                                        â€¢ Out Of Vocabulary(OOV )




                                                                              21
Term Frequency-Inverse Document Frequency
TF-IDF Intuition:

 â€¢   TF-IDF assigns more weight to rare words and less weight to
     commonly occurring words.

 â€¢ Tells us how frequent a word is in a document relative to its

     frequency in the entire corpus.

 â€¢   Tells us that two documents are similar when they have more
     rare words in common.


                                                                   22
Term Frequency-Inverse Document Frequencyâ€¦



     TF-IDF score = TF * IDF




                                             23
Term Frequency-Inverse Document Frequency
Term Frequency
 â€¢   So far, weâ€™ve been recording the term (word) count

     â€œThis is an exampleâ€               This      is        an      example
                                         1        1          1         1


 â€¢   A better way to compare them is by a normalized term frequency, which is (term
     count)/ (total terms).


                                            ğ‘¡ğ‘’ğ‘Ÿğ‘š ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦
                             ğ‘‡ğ¹(ğ‘¡ğ‘’ğ‘Ÿğ‘š) =
                                        ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘  ğ‘–ğ‘› ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡



                                                                                      24
Term Frequency-Inverse Document Frequencyâ€¦

 â€¢   Besides term frequency, another thing to consider is how common a

     word is among all the documents

 â€¢   Rare words should get additional weight

 â€¢   Measures the importance of the term across a corpus.




                                                                         25
Term Frequency-Inverse Document Frequencyâ€¦

Inverse Document Frequency
 It aims to quantify the importance of a given word. Words that
appear in many documents get a low IDF score, while words that
appear in only a few documents get a high IDF score


                                           ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ 
                     ğ¼ğ·ğ¹(ğ‘¡ğ‘’ğ‘Ÿğ‘š) = ğ‘™ğ‘œğ‘”10 (                               )
                                         ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ ğ‘œğ‘“ ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  ğ‘¤ğ‘–ğ‘¡â„ ğ‘¡ğ‘’ğ‘Ÿğ‘š




                                                                           26
Term Frequency-Inverse Document Frequencyâ€¦



     TF-IDF score = TF * IDF




                                             27
    TF-IDF :Example
â€¢   The sky is blue.
â€¢   The sun is bright today.
â€¢   The sun in the sky is bright.
â€¢   We can see the shining sun, the bright sun




                                                 28
TF-IDF :Exampleâ€¦




                   29
Group Work: Compute TF-IDF

 D1 Dog bites man.
 D2 Man bites dog.
 D3 Dog eats meat.
 D4 Man eats food.




                             30
Count Vectorizer vs TF-IDF Vectorizer
import pandas as pd
corpus = ['This is the first document.', 'This is the second
       document.', 'And the third one. One is fun.â€™]

# original Count Vectorizer
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X = cv.fit_transform(corpus).toarray()
pd.DataFrame(X, columns=cv.get_feature_names())

# new TF-IDF Vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
cv_tfidf = TfidfVectorizer()
X_tfidf = cv_tfidf.fit_transform(corpus).toarray()

pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())




                                                               31
Count Vectorizer vs TF-IDF Vectorizerâ€¦
Count Vectorizer Output:




   TF-IDF Vectorizer Output:




                                         32
Why we need log()




                    33
TF-IDF Limitation
â€¢ Miss information about the relationships between words

â€¢ Sparsity

â€¢ Lack of semantic understanding

â€¢ Not well suited for small corpora

â€¢ Out Of Vocabulary OOV words.




                                                           34
  Text Similarity Measure (lexical similarity)
Computational measure of the degree to which two or more documents
are semantically or lexically alike.




 â¢ Applications:
    Speech Recognition                     Machine Translation
    Plagiarism Detection                   Information Retrieval
    Text Classification                    Search engine

              Image generated by ChatGPT
Text Similarity Measuresâ€¦


â‘Jaccard Similarity

â‘Cosine Similarity

â‘Euclidean Distance

â‘Hamming Distance

â‘Levenshtein Distance


                            36
Text Similarity : Levenshtein distance

Levenshtein distance: Minimum number of operations to get from
one word to another.
 Levenshtein operations are:
 â–ªDeletions: Delete a character

 â–ªInsertions: Insert a character

 â–ªMutations: Change a character

  â–ª Example: kitten â€”> sitting

     â–ª kitten â€”> sitten (1 letter change)       Levenshtein distance = 3

     â–ª sitten â€”> sittin (1 letter change)

     â–ª sittin â€”> sitting (1 letter insertion)



                                                                           37
Text Similarity :Euclidean Distance




      Photo Credit: http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-
      implementation-in-python/                                                                 38
Text Similarity :Cosine
Measures how similar two vectors are by comparing the angle
between them

          cos(Î¸) = (A Â· B) / (||A|| ||B||)




              39

                                                              39
Cosine Similarity: Example
 Cosine Similarity calculation steps:
     â€¢   Step 1: Put each document in vector format
     â€¢   Step 2: Find the cosine of the angle between the documents




                                   i               love   you   nlp

                        Doc 1      1                1     1     0
         â€œI love NLPâ€
                        Doc 2      1                1     0     1
         â€œI love youâ€
                                                                      = 0.667
                                a = [1, 1, 1, 0]
                                b = [1, 1, 0, 1]



                                                                                40
Your turn
                                                           pie   data computer
                              Ã¥i=1 vi wi
            ï² ï² ï² ï²               N
    ï² ï² v Â·w v w                                cherry     442   8    2
cos(v, w) = ï² ï² = ï² Â· ï² =
            v w v w                             digital    5     1683 1670
                            Ã¥i=1 i Ã¥i=1 i
                             N 2        N   2
                                v         w
                                                information 5    3982 3325



                                      ?

                                      ?



                                                                                 41
 Your turn
Consider you have the following documents
 document        text
 d1            ant ant bee
 d2            dog bee dog hog dog ant dog
 d3            cat gnu dog eel fox

Use cosine measure to compute the pairwise similarity between d1, d2
and d3 using Count Vectorizer, BOW and TF-IDF
                  d1           d2            d3
     d1
     d2
     d3


                                                                       42
Document Similarity: Example
Here are five documents. Which ones seem most similar?



â€œThe weather is hot under the sunâ€
â€œI make my hot chocolate with milkâ€                 With Count Vectorizer,
â€œOne hot encodingâ€                                  these two documents
                                                    were the most similar
â€œI will have a chai latte with milkâ€
â€œThere is a hot sale todayâ€




                                                                             43
Document Similarity: Exampleâ€¦
Output:
[(0.40824829, ('The weather is hot under the sun', 'One hot encoding')),
 (0.40824829, ('One hot encoding', 'There is a hot sale today')),
 (0.35355339, ('I make my hot chocolate with milk', 'One hot encoding')),
 (0.33333333, ('The weather is hot under the sun', 'There is a hot sale today')),
 (0.28867513, ('The weather is hot under the sun', 'I make my hot chocolate with milk')),
 (0.28867513, ('I make my hot chocolate with milk', 'There is a hot sale today')),
 (0.28867513, ('I make my hot chocolate with milk', 'I will have a chai latte with milk')),
 (0.0, ('The weather is hot under the sun', 'I will have a chai latte with milk')),
 (0.0, ('One hot encoding', 'I will have a chai latte with milk')),
 (0.0, ('I will have a chai latte with milk', 'There is a hot sale today'))]



                                             â–ª These two documents are most similar, but itâ€™s just
                                                because the term â€œhotâ€ is a popular word
                                             â–ª â€œMilkâ€ seems to be a better differentiator, so how we
                                                can mathematically highlight that?




                                                                                                       44
 Document Similarity: Example with TF-IDF


[(0.23204485, (â€˜I make my hot chocolate with milk', 'I will have a chai latte with milkâ€™))


(0.18165505, ('The weather is hot under the sun', 'One hot encoding')),
(0.18165505, ('One hot encoding', 'There is a hot sale today')),
(0.16050660, ('I make my hot chocolate with milk', 'One hot encoding')),
(0.13696380, ('The weather is hot under the sun', 'There is a hot sale today')),
(0.12101835, ('The weather is hot under the sun', 'I make my hot chocolate with milk')),
(0.12101835, ('I make my hot chocolate with milk', 'There is a hot sale today')),
(0.0, (â€˜The weather is hot under the sun', 'I will have a chai latte with milk')),
(0.0, ('One hot encoding', 'I will have a chai latte with milk')),
(0.0, ('I will have a chai latte with milk', 'There is a hot sale today'))]




 By weighting â€œmilkâ€ (rare) > â€œhotâ€ (popular), we get a smarter similarity score

                                                                                             45
Discussion

What are the ethical concerns
related to frequency-based text
presentation?




       PowerPoint Presentation Title
       Date                            46
Summary

  Today we discussed :
  â¢ Traditional methods for text representation.
       â‘ One-hot encoding
       â‘ Bag of words
       â‘ Bag of N-grams
       â‘ TF-IDF
  â¢ Similarity measures
       â‘ Lexical Similarity â€“ Levenshtein
       â‘ Lexical Similarity â€“ Cosine
       â‘ Lexical Similarity â€“ Euclides distance




                                                   47
Q&A




      48
