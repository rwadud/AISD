CST8507: NATURAL
   LANGUAGE
  PROCESSING

     WEEK#4
WORD EMBEDDING
    DEVELOPED BY
   HALA OWN, PH.D.
    Lesson Agenda
‚Ä¢ Prediction based Text representation (word Embedding)

     ‚Ä¢ CBOW, Skip-Gram and SGNS

       ‚Ä¢ Word2Vec

       ‚Ä¢ FastText
‚Ä¢ Count-Based / Matrix Factorization Methods

       ‚Ä¢ Glove

                                                          Page 2
NLP Development Life Cycle

 Requirements
  gathering




         Gather more         Improve the
         data                model




                                           3
Text Representation Techniques
‚Ä¢ Frequency based Text representation:
  ‚Ä¢ One-Hot Encoding
  ‚Ä¢ Bag of Words
  ‚Ä¢ Bag of N-Grams
  ‚Ä¢ TF-IDF
‚Ä¢ Prediction based Text representation (word Embedding)
‚Ä¢ Universal Text Representations




                                                          4
Comparing Feature Representations For Audio,
Image And Text




     https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-
     learning-methods-text-data.html                                             5
Frequency based Text representation: Limitation

    ‚Ä¢ High-dimensional representation

    ‚Ä¢ sparse                            Poor
                                        Performance
    ‚Ä¢ OOV words                         On majority of
                                        NLP Tasks
    ‚Ä¢ Lack of semantic meaning




                                                         6
   Relation between Word Senses: Word
   Similarity
‚ùë Cat is not a synonym of dog, but cats and dogs are certainly similar
  words

  ‚ùë A semantic field is a set of words which cover a particular semantic
     domain
     ‚ùë Restaurants: waiter, menu, plate, food, chef
     ‚ùë Houses: door, roof, kitchen, family, bed
                                                                           7


One way of getting values for word similarity is to ask humans to judge
how similar one word is to another
  WordNet
a large lexical database of English words, where words are grouped into sets of

synonyms called synsets. These synsets are connected by various semantic relationships, such as
synonymy, hypernymy, and hyponymy,etc.




                              https://wordnet.princeton.edu/
WordNet‚Ä¶
Database of lexical relations for English




        https://www.scaler.com/topics/nlp/wordnet-in-nlp/
 WordNet ‚Äì Important Concepts
‚ùñ Synset: A set of synonyms that share a common meaning.

‚ùñ Hypernym: A general term that encompasses more specific terms (e.g., "animal" is a
   hypernym of "dog").

‚ùñ Hyponym: A specific term within a broader category (e.g., "dog" is a hyponym of
   "animal").

‚ùñ Meronym: A term that denotes a part of something (e.g., "wheel" is a meronym of "car").

‚ùñ Holonym: A term that denotes a whole of which the meronym is a part (e.g., "car" is a
   holonym of "wheel").

‚ùñ Antonym: Words that have opposite meanings (e.g., "hot" and "cold").
‚ùñ Troponym: A verb that denotes a specific manner of doing something (e.g., "run" is a
   troponym of "move").

‚ùñ Entailment: A relationship where one verb implies another (e.g., "snore" entails "sleep").
WordNet Applications in NLP Tasks: Semantic
Test presentation




The context of each synset is tokenized into words, with each word mapped to a vector representation via the
learned embedding matrix. The synset vector is the centroid produced by averaging all context word
embeddings.

          Reference: Text classification with semantically enriched word embeddings
          Published online by Cambridge University Press: 06 April 2020                                        11
WordNet Applications in NLP Tasks: Query
Expansion



                                              Convert words
                                              to numerical
                                              format




     Image generated by ChatGPT with some updates             12
WordNet: Limitations
‚ùëLimited Coverage and Static Nature

‚ùëNot Computational

‚ùëDomain Specificity

‚ùëLanguage Limitation

‚ùëManual Curation Challenges




                                      13
  Key Terms

‚Ä¢ Distributional similarity: the meaning of a word can be understood
  from the context

‚Ä¢ Distributional hypothesis: words that occur in similar contexts have
  similar meanings.




                                                                         14
    Vector Semantics(Word Embedding)
Ccomputational model that learn the linguistic units (words,
phrases, or documents ) representations based on distributional
properties of these units in a large corpus.
      ‚ùë Representation linguistic units as vectors in a multi-
        dimensional space.
      ‚ùë Encoding semantic information using mathematical
        vectors.
      ‚ùë Standard way to represent word meaning in NLP.
                                                    15
   Word Embedding
 Representation of words as vectors
 of numbers in a high-dimensional
 space. It captures semantic and
 contextual information about the
 word.

Input: large number of corpus,
Vocabulary V, and vector of
dimension d
output
  ùëì: ùëâ ‚Üí ùëÖùëë
           Image source: real world natural langue processing book
                                                                     16
Word Embedding‚Ä¶
Analogy




vector(‚Äòking‚Äô) - vector(‚Äòman‚Äô) + vector(‚Äòwoman‚Äô)   ‚âà vector(‚Äòqueen‚Äô)



                                                                       17
Prediction based Text representation
Big idea: self-supervision, Bengio et al. (2003) and Collobert et al. (2011)

‚Ä¢ Popular embedding method

‚Ä¢ Very fast to train

‚Ä¢ Code available on the web

‚Ä¢ Predict rather than count


                                                                               18
 Prediction Based Text Representation(Word2Vec)
Word Embedding (creating dense vector representations of words)




       Source: https://dataaspirant.com/word-embedding-techniques-nlp/
                                                                         19
 CBOW
‚Ä¢ Goal: Predict the middle word given the words of the context




                                                                 20
 CBOW..
Window size=2




     https://medium.com/co-learning-lounge/nlp-word-embedding-tfidf-bert-word2vec-d7f04340af7f
                                                                                                 21
CBOW: Simple Example




                       22
Skip-gram
‚Ä¢ Goal: Predict the context words given the middle word




                                                          23
      Skip-gram‚Ä¶

the training objective is to minimize the summed prediction error across all
context words in the output layer.
Skip-gram: Example




    Image source:
    https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_mod
    eling                                                                               25
Skip-gram Prediction:Example



                     the cat sat on the mat

                                     wt-2 = <start-2>
                                     wt-1 = <start-1>
          wt = the     CLASSIFIER    wt+1 = cat
                                     wt+2 = sat




context size = 2
Skip-gram Prediction :Example‚Ä¶



                     the cat sat on the mat

                                       wt-2 = <start-1>
                                       wt-1 = the
          wt = cat       CLASSIFIER    wt+1 = sat
                                       wt+2 = on




context size = 2
Skip-gram Prediction: :Example‚Ä¶



                     the cat sat on the   mat

                                           wt-2 = the
                                           wt-1 = cat
          wt = sat          CLASSIFIER     wt+1 = on
                                           wt+2 = the




context size = 2
Skip-gram Prediction :Example‚Ä¶



                     the cat sat on the mat

                                  wt-2 = cat
                                  wt-1 = sat
          wt = on   CLASSIFIER    wt+1 = the
                                  wt+2 = mat




context size = 2
Skip-gram Prediction :Example‚Ä¶



                     the cat sat on the mat

                                     wt-2 = sat
                                     wt-1 = on
          wt = the     CLASSIFIER    wt+1 = mat
                                     wt+2 = <end+1>




context size = 2
Skip-gram Prediction :Example‚Ä¶



                     the cat sat on the mat

                                       wt-2 = on
                                       wt-1 = the
          wt = mat      CLASSIFIER     wt+1 = <end+1>
                                       wt+2 = <end+2>




context size = 2
Skip-gram vs CBOW

 ‚Ä¢ CBOW is comparatively faster to train than skip-gram and better

   for frequently occurring words

 ‚Ä¢ Skip-gram is slower but works well for smaller amount of data

 ‚Ä¢ CBOW is an easier classification problem than Skip-gram




                                                                     32
Skip-gram Negative Sampling(SGNS): Approach

1. Treat the target word t and a neighboring context word
   c as positive examples.
2. Randomly sample other words in the lexicon to get
   negative examples
3. Use logistic regression to train a classifier to
   distinguish those two cases
4. Use the learned weights as the embeddings


                                                            33
      SGNS : how to learn vectors
‚Ä¢ Given the set of positive and negative training instances,
  and an initial set of embedding vectors
‚Ä¢ The goal of learning is to adjust those word vectors such
  that we:
   ‚Ä¢ Maximize the similarity of the target word, context
     word pairs (w , cpos) drawn from the positive data
   ‚Ä¢ Minimize the similarity of the (w , cneg) pairs drawn
     from the negative data.


                                                               34
SGNS : how to learn vectors‚Ä¶

    ‚Ä¢ Training sentence:
      ... lemon, a tablespoon of apricot jam a pinch
    ...
                      c1        c2 t       c3 c4




                                                       35
Pretrained Word Embeddings Models


‚Ä¢ Word2vec (Mikolov et al.) 2013
‚Ä¢ https://code.google.com/archive/p/word2vec/
‚Ä¢ Fasttext http://www.fasttext.cc/ 2016
‚Ä¢ Glove (Pennington, Socher, Manning) 2014
‚Ä¢ http://nlp.stanford.edu/projects/glove/




                                                36
Google‚Äôs Word2Vec

‚Ä¢ Gensim package :Google‚Äôs pre-trained Word2Vec model
  in Python.
‚Ä¢ This model is trained on the vocabulary of 3 million
  words and phrases from100 billion words of the Google
  News dataset.
‚Ä¢ The vector length for each word is 50,100,300.




                                                          37
Google‚Äôs Word2Vec
 ‚Ä¢ Install genism library
    conda install -c conda-forge genism

‚Ä¢ Genism word2vec Model Training

 model = Word2Vec(text, min_count=1,vector_size= 50, window =5,
 sg = 1, negative=5)




                                                                  38
Demo

‚Ä¢ inclassCode




                39
The GloVe (Global Vector for word presentation)

‚Ä¢ Unsupervised learning model
  that can be used to obtain dense
  word vectors.
‚Ä¢ invented in Stanford by
  Pennington et al.
  https://nlp.stanford.edu/projects/
  glove/
‚Ä¢ https://github.com/stanfordnlp/Gl
  oVe

       PowerPoint Presentation Title
       Date                                       40
Glove Algorithm




                  41
GloVe :Co-Occurrence Matrix
   I love Programming. I love Math. I tolerate Biology.




     Window size = 1

                                                          42
The GloVe (Global Vector for word presentation)
Load the embeddings and use them as fixed word vectors in your
application.
   ‚Ä¢   https://nlp.stanford.edu/projects/glove/
   ‚Ä¢   Download pre-trained word vectors
   ‚Ä¢   Pre-trained word vectors. This data is made available under the Public Domain Dedication and
       License v1.0 whose full text can be found
       at: http://www.opendatacommons.org/licenses/pddl/1.0/.Wikipedia 2014 + Gigaword 5 (6B tokens,
       400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip
   ‚Ä¢   Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB
       download): glove.42B.300d.zip
   ‚Ä¢   Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB
       download): glove.840B.300d.zip
   ‚Ä¢   Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB
       download): glove.twitter.27B.zip


       glove.6B.300d , glove.6B.200d , glove.6B.100d , glove.6B.50d


                                                                                                       43
Dealing with OOV

‚Ä¢ Use a Default Vector
‚Ä¢ Fallback to a Similar Word
‚Ä¢ Train Your Own Embeddings


   Better Solution: The FastText Model




                                         44
The FastText Model

‚Ä¢ The FastText model was introduced by Facebook in 2016 as
  an extension and supposedly improvement of the vanilla
  Word2Vec model.
‚Ä¢ FastText is a framework for learning word representations and
  performing robust, fast, and accurate text classifications

     https://fasttext.cc/
    FastText n-gram embedding model (Bojanowski et al., 2017): Enriching Word Vectors with Subword Information




                                                                                                                 45
The FastText Model‚Ä¶

    Sub-word generation




     Download English vector:
     https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz
Access from Kaggle: https://www.kaggle.com/facebook/fasttext-wikinews


                                                                              46
     FastText: Subword Generation
For a word, we generate character n-grams of length 3 to 6 present in it




Two-step vector representation updating
  1. First, the embedding for the center word is calculated by taking a
     sum of vectors for the character n-grams and the whole word itself
  2. For the actual context words, we directly take their word vector from
     the embedding table without adding the character n-grams



                                                                           47
Advantages of FastText

‚Ä¢ Capture fine level more gradual information
‚Ä¢ Solve VOO
‚Ä¢ Open-source, free, lightweight library.
‚Ä¢ Handle text data in various languages .
‚Ä¢ Has a simple and intuitive API



                                                48
Word Embedding: Benefits

‚Ä¢ Dimensionality reduction

‚Ä¢ Semantic meaning                   Improved
                                     Performance
                                     On NLP Tasks
‚Ä¢ Handling Out-of-Vocabulary (OOV)

‚Ä¢ Transfer learning



                                                    49
Word Embedding - Limitations

Context Insensitivity
Bias
Limited Semantic Adaptation
Dimensionality

Resource Intensive
OOV words
Universal Text Representations

‚Ä¢ contextual word representations
‚Ä¢ Advanced neural language models
‚Ä¢ complex architectures involving multiple passes through
  the text and multiple reads from left to right and right to
  left to model the context of language
  ‚Ä¢ ELMo , BERT, ULMFiT




                                                                51
  Word Embedding - Evaluation
1. Intrinsic Evaluation

    ‚û¢   Assessing the quality of word embeddings independently of any
        specific task.
    ‚û¢    They focus on the internal properties of the embeddings. Word
        Similarity, Analogy Tasks, ‚Ä¶


1. Extrinsic Evaluation

    ‚û¢   Assessing the quality of word embeddings based on their
        performance in downstream NLP tasks like: Text classification,
        NER, etc.
‚ùñ WordNet and word senses

‚ùñ Distributed representation

‚ùñ Word Embedding

     ‚ùñ Word2Vec

     ‚ùñ Glove

     ‚ùñ FastText

‚ùñ Evaluation of Word Embeddings

‚ùñ Problems with Word Embedding
Q&A




      54
