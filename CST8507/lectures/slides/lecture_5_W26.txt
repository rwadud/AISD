   CST8507: NATURAL
LANGUAGE PROCESSING

        WEEK#5
   INTRODUCTION TO
   LANGUAGE MODEL
      DEVELOPED BY

      HALA OWN, PH.D.
    Lesson Agenda
‚Ä¢ Lab 3

‚Ä¢ Text Collection(overview)

‚Ä¢ Language Model
  ‚Ä¢ N-gram

  ‚Ä¢ NN Language model
     ‚Ä¢ Recurrent Neural Networks RNN

     ‚Ä¢ LSTMs

                                       Page 2
NLP Development Life Cycle

 Requirements
  gathering




         Gather more         Improve the
         data                model




                                           3
Data generated in one minute on various social platforms




  Image source: HTTPs://localiq.com/blog/what-happens-in-an-internet-minute/
                                                                               4
Text Collection

‚Ä¢ Tweet Collecting
  ‚Ä¢ X API




                     5
Create X Developer Account




https://help.rssground.com/articles/233141-how-to-create-x-twitter-
developer-app


                                                                      6
Web Scraping:Extraction of data from a website
Python libraries are widely used for parsing HTML:
   1. Beautiful Soup: A popular library for parsing HTML and XML documents. It
     simplifies extracting data from web pages and has an active community with
     detailed documentation.

   2. lxml: Known for its speed, lxml is one of the fastest parsing libraries available. It
     receives regular updates, with the latest released in July 2023.

   3. html5lib: A pure-Python library designed to conform to the WHATWG

   (Web Hypertext Application Technology Working Group) HTML

   specification, ensuring compatibility with major web browsers.


                                                                                              7
Demo

‚Ä¢ Inclass code




                 8
Reminder

PROBABILITY THEORY


                     9
Basic Probability Theory: Sampling with
replacement
        Pick a random shape, then put it back in the bag.




       P( ) = 2/15         P( ) = 1/15       P(   or ) = 2/15
       P(blue) = 5/15      P(red) = 5/15     P(   |red) = 3/5
       P(blue | ) = 2/5    P( ) = 5/15
Sampling with replacement

 Pick a random shape, then put it back in the bag.
 What sequence of shapes will you draw?
                             P(                 )
                             = 1/15 √ó 1/15 √ó 1/15 √ó 2/15
                             = 2/50625
                             P(                 )
                             = 3/15 √ó 2/15 √ó 2/15 √ó 3/15
                             = 36/50625
P( ) = 2/15         P( ) = 1/15            P(       or ) = 2/15
P(blue) = 5/15      P(red) = 5/15          P(       |red) = 3/5
P(blue | ) = 2/5    P( ) = 5/15
Conditional Probability                                           P(X, Y )
                                                    P(X|Y )     =
                                                                   P(Y )
The conditional probability of X given Y, Probability that one event occurs
given that another event has already occurred.
  Chain Rule of Probability

The chain rule expresses a joint probability as a product of conditional
probabilities.
             For a sequence of events ùëøùüè , ùëøùüê , ‚Ä¶ , ùëøùíè
LANGUAGE MODELING




                    14
Language Modeling: the task of predicting what word comes next
                                             books

                                                minds
  the students opened their-------
                                                 exams

                                                laptops
Given a sequence of words
compute the probability distribution of the next word


    Where          can be any word in the vocabulary

  ‚û¢ A system that does this is called a Language Model.

                                                                 15
Popular Usages
Goal of Language Modeling

learn patterns in text and predict the
next word (or sequence of words)
based on prior context.
N-gram Language Modeling




IDEA: Collect statistics about how frequent different n-grams are, and use these to
predict next word.

            Image source: https://devopedia.org/n-gram-model
                                                                                      18
    N-gram Language Modeling‚Ä¶

‚Ä¢ For example, if we have sequence of tokens         ,    then the probability to see
  these tokens in this order is:

Using chain Rule




                           This is what our LM provides




                                                                                    19
      Language Modeling: n gram‚Ä¶
                          n-1 words

Our assumption
                                      Recall the definition of conditional
                                      probabilities
                                      p(B|A) = P(A,B)/P(A)

                                      P(A,B) = P(A)P(B|A)




                                                                             20
n-gram Language Models: Example using 4- gram

        as   the proctor   started   the   clock   the   students   opened their
                       discard
                                                            fixed window




   For example, suppose that in the corpus:
‚Ä¢ ‚Äústudents opened their‚Äù occurred 1000 times
‚Ä¢ ‚Äústudents opened their books‚Äù occurred 400 times
   ‚Ä¢ ‚ûî P(books | students opened their) = 0.4
‚Ä¢ ‚Äústudents opened their exams‚Äù occurred 100 times
   ‚Ä¢ ‚ûî P(exams | students opened their) = 0.1

                                                                                   21
N-grams : limitations and challenges


‚Ä¢ Data Sparsity

‚Ä¢ Computational Complexity

‚Ä¢ Context Limitations




                                       22
Neural Network Based Language Models




                                       23
     A Quick Review Of Neural Nets
              Hidden Layer              ‚Ä¢ Input layer is a set of features; each arrow represents a
                                            weight (float number) that tells us how much each input
                                            contributes to each following step.
Input Layer
                                        ‚Ä¢ Each node in the hidden layer is some combination of all
                                            the inputs. The hidden layer acts as the ‚Äòinput‚Äô for the
                                            output layer.
                             Output Layer
                                        ‚Ä¢ Backpropagation allows us to adjust the weights to improve
                                            accuracy and find the ‚Äòcorrect‚Äô way to combine the inputs
                                            and hidden layers to get the best possible results.

                                                                                                        24
    NN basic element: Perceptron or
                Neuron
  bias x0 = +1        w0
                                                       Activation
            x1        W1                               function
                                             v
Input
Attribute   x2        w2         ÔÉ•                     ÔÅ™ (‚àí)
                                                                    Output
values           ÔÅç     ÔÅç                                             class
                               Summing function                       y
            xm        wm
                                      m
                     weights
                                v=   ÔÉ•w x
                                      j =0
                                                 j j


                                w0 = b
Language Model: Neural Nets

  as   the proctor   started   the   clock   the   students   opened their   -----?-----
                 discard                           --

                                                      fixed window




                                                                                           26
Language Model: Neural Nets ‚Ä¶
                                                                                                  books
                                                                                                          laptops

               output distribution

                                                                                                  a                 zoo



                hidden layer



               concatenated word embeddings




               words / one-hot vectors                                            the          students     opened        their



  These slides are sourced from Stanford's "Natural Language Processing with Deep Learning" course.
                                                                                                                                  27
Feed Forword NN: Limitation...

  as   the proctor   started   the   clock   the   students   opened their   ------------
                 discard
                                                      fixed window




                                                                                            28
Feed Forword NN: Limitation

         ‚ÄúThe food was good, not bad at all‚Äù

         ‚ÄúThe food was bad, not good at all‚Äù




                                               29
Feed Forword NN: Limitation‚Ä¶

 ‚ÄúJust watched the new movie. Loved it!     #entertained‚Äù


  ‚ÄúThe storyline was captivating, the characters were well-

developed, and the cinematography was impressive. Overall,

 a fantastic movie night!       #movienight #recommend‚Äù

                                                              30
Sequence Modeling: Motivations


‚Ä¢ Handle variable length sequence data

‚Ä¢ Track long term dependency

‚Ä¢ Maintain information about order

‚Ä¢ Share information across the sequence




                                          31
DNN: Universal Approximation Theorem (UAT)




     proven by George Cybenko in 1989


                                             32
Core idea of Recurrent Neural Networks (RNNs) RNNs
                Stateful computation

                              yt



                              ht

                              xt

                 y t , h t = f (x t , h t - 1 )
Core idea of RNNs ‚Ä¶
                Stateful computation

      yt                             y1   yt



      ht                h0           h1   ht



      xt                             x1   xt

           ‚Ñéùë° = ùëäùë• ùë•ùë° + ùëä‚Ñé ‚Ñéùë°‚àí1 +b
                   = negative log prob
                      of ‚Äústudents‚Äù
                                                                                                 How we train the
     Loss                                                                                        model
Predicted
prob dists



                                                                                                     ‚Ä¶




                           the          students          opened             their           exams       ‚Ä¶


         These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
         course.                                                                                                    35
                                                                                                     How we train the
                                = negative log prob
                                   of ‚Äúopened‚Äù
      Loss
                                                                                                     model
Predicted
prob dists



                                                                                              ‚Ä¶




  Corpus                the         students          opened             their           exams           ‚Ä¶
 36

             These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
             course.                                                                                                    36
                                          = negative log prob
                                              of ‚Äútheir‚Äù
                                                                                                     How we train the
      Loss                                                                                           model
Predicted
prob dists



                                                                                        ‚Ä¶




  Corpus         the         students          opened             their           exams             ‚Ä¶
 37
             These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
             course.                                                                                                    37
                                                                = negative log prob
                                                                    of ‚Äúexams‚Äù                       How we train the
      Loss
                                                                                                     model
Predicted
prob dists



                                                                                             ‚Ä¶




  Corpus              the         students          opened             their           exams             ‚Ä¶
 38

             These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
             course.                                                                                                    38
        Loss             +                +                +               +‚Ä¶        =

Predicted
probability
distribution                                                                                               How we train the
                                                                                                           model
                                                                                      ‚Ä¶




   Corpus      the           students         opened            their           exams             ‚Ä¶




               These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
               course.                                                                                                        39
                                                                      output distribution
Language                                                                                                      books
                                                                                                                      laptops


Model: RNN
                                                                                                              a                 zoo




  hidden states

      is the initial hidden state




  word embeddings




                                                                               the          students        opened     their


                These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
                course.                                                                                                               40
Difference between NN and RNN




    Traditional NN for LM                                RNN for LM


       Image source: NLP in Action text book, O'Reilly
                                                                      41
Fun With RNN Language Model

‚Ä¢ https://medium.com/@samim/obama-rnn-machine-
  generated-political-speeches-c8abd18a2ea0




                                                 42
Back Propagation in RNN




         Backpropagation Through Time (BPTT).


                                                43
RNN Vanishing Gradient Intuition




       These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
       course.                                                                                     44
Vanishing gradient intuition




       These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
       course.                                                                                     45
Vanishing gradient intuition




       These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
       course.                                                                                     46
Vanishing gradient intuition




       These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
       course.                                                                                     47
Vanishing gradient intuition




       These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
       course.                                                                                     48
Vanishing gradient intuition




       These slides are sourced from Stanford's "Natural Language Processing with Deep Learning"
       course.                                                                                     49
Why Vanishing Gradients is Problem
Vanishing gradients occur when the values of a gradient are too small
and the model stops learning or takes way too long as a result




                           Learning Rate



  Input Layer                                       Output Layer
Vanishing Gradients Problem‚Ä¶

Example
When she tried to print her tickets, she found that the printer
was out of toner. She went to the stationery store to buy more
toner. It was very overpriced. After installing the toner into the
printer, she finally printed her-------------------

RNN-LM needs to model the dependency between ‚Äútickets‚Äù on
the 7th step and the target word ‚Äútickets‚Äù at the end


                                                                     51
  Long Short-Term Memory (LSTM)

‚Ä¢ Hochreiter & Schmidhuber (1997) solved the problem of getting an
  RNN to remember things for a long time.

  ‚Ä¢ At each timestep t, the LSTM maintains two key components:

     ‚Ä¢ Hidden state ‚Äì captures short-term dependencies.

     ‚Ä¢ Cell state ‚Äì acts as a memory unit, storing long-term information.
  Long Short-Term Memory (LSTM)
Key Concepts:
‚Ä¢Unlike standard RNNs, LSTMs can control the flow of information through
three specialized gates:
   ‚Ä¢ Forget gate ‚Äì decides which information to erase.
   ‚Ä¢ Input gate ‚Äì determines what new information should be stored.
   ‚Ä¢ Output gate ‚Äì regulates what information is passed to the next timestep.
‚Ä¢Each gate is represented as a vector of size n and can take values between 0
(closed) and 1 (open) dynamically, based on the current context.



                                                                                53
Long Short -Term Memory (LSTM)




                          LSTM at time stamp T
    Image source: https://towardsdatascience.com/lstm-networks-a-detailed-
    explanation-8fae6aefc7f9
                                                                             54
Long Short Term Memory (LSTM): step 1




                    ft

             ht


        xt




    Forget gate: decide what parts of old state to forget


                                                            55
Long Short Term Memory (LSTM):step2


                                              ct



                                   ùëê‡∑ùùë°   it
             ht

        xt




                  Input gate: decide how to update the cell state


                                                                    56
Long Short Term Memory (LSTM):step3


                       ct
                                                ct


                      ot
           ht
                                                ht
      xt




                            Finally, decide what to output as hidden state



                                                                             57
Long Short Term Memory (LSTM)
                              Write some new cell content    Output some cell content
                                                               to the hidden state




Forget some
 cell content



Compute the
 forget gate

     Compute the
      input gate
                                                            Compute the
                    Compute the                              output gate
                   new cell content


                                                                                        58
LSTM Great resources

‚Ä¢ https://colah.github.io/posts/2015-08-Understanding-
  LSTMs/




                                                         59
Keras ‚Äì Simplifying LSTMs in Python
Keras is a Python package that makes building and training TensorFlow neural
networks really simple. We‚Äôll be working with the ‚ÄùSequential‚Äù model which lets you
add layers one at a time. As an example, let‚Äôs see how to build a 1-layer LSTM
model with 10 hidden nodes.

from keras.models import Sequential
from keras.layers import Dense, Activation, LSTM

model = Sequential()
model.add(LSTM(10, input_shape=(TIMESTEPS, FEATURE_LENGTH)))
model.add(Dense(NUMBER_OF_OUTPUT_NODES))
model.add(Activation('softmax'))
Evaluating Language Models
‚Ä¢ The standard evaluation metric for Language Models is perplexity.


                                                                   Normalized by
                                                                  number of words

     Inverse probability of corpus, according to Language Model

    Low perplexity ‚Üí the model predicts the text well
    High perplexity ‚Üí the text is unexpected for the model

Perplexity (PPL) measures how confused a language model is when predicting the
next word in a sentence.


                                                                                    61
Summary

‚Ä¢ We introduced the concepts of recurrent neural networks
  and how it can be applied to language problems.
‚Ä¢ RNNs can be trained with a straightforward extension of
  the backpropagation algorithm.
‚Ä¢ How LSTM used for text generation
‚Ä¢ Applications of LSTM for sequence-to-sequence
  modeling



                                                            62
Q&A




      63
