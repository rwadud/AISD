   CST8507: NATURAL
LANGUAGE PROCESSING

        WEEK#6
  SEQUENCE TO SEQUENCE
     MODELS (SEQ2SEQ)

       DEVELOPED BY

       HALA OWN, PH.D.
       Lesson Agenda
‚Ä¢ Midterm(week 7)

‚Ä¢ Bidirectional Long Short Term Memory (Bi-LSTM)

‚Ä¢ The encoder-decoder framework

‚Ä¢ Attention mechanisms




                                                   Page 2
Midterm
 Midterm is on Tuesday, Feb. 23, at 2:00 pm.
‚Ä¢ The exam will consist of 30 questions, including multiple-choice and
  true/false questions, with no essay questions.
‚Ä¢ Material includes from week 1 ‚Äì week 6
‚Ä¢ You will have 60 minutes to complete the exam.
‚Ä¢ The exam is closed book. However, you may bring one cheat sheet: a
  single letter-size page (8.5 √ó 11 inches) that may be used on both sides.
‚Ä¢ Ensure that you leave a 5 cm by 5 cm space in the top-left corner of
  each side of your cheat sheet for the proctor's signature. If this specific
  area is missing, you will not be allowed to use any cheat sheet during
  the exam.
‚Ä¢ Try to arrive early to allow sufficient time for setup.

                                                                                3
Midterm

‚Ä¢ Read the instruction before starting your exam.
‚Ä¢ Write your name and ID number on the spaces provided on the
  questionnaire and Answer sheet.
‚Ä¢ Make sure to have your ID.
‚Ä¢ Read carefully the ICT exam conduct outline
‚Ä¢ Please do not forget to bring your HB pencils and eraser.
‚Ä¢ Scantron answer sheets will be provided to you before the start of
  the exam together with the questionnaire.
‚Ä¢ Submit both the questionnaire and the Scantron answer sheet

                                                                       4
How to Prepare
‚ñ™ Lecture summary slides are a good place to start:

     they don‚Äôt have all the details, but make sure you understand the details underlying the

     main points mentioned.


‚ñ™ Do the labs! Make sure you understand the answers you get.

‚ñ™ Code-Examples demonstrated during the lecture (check lecture materials

  folder on Brightspace).

‚ñ™ Hybrid work


                                                                                                5
Questions




            6
Recap :N-garm




                7
Bigram Probability

I have a dog whose name is Lucy.
I have two cats.
 they like playing with Lucy.




                                   8
Recap :RNN
Designed to handle sequential data by maintaining a hidden state
that captures information from previous time steps.




         Source of image: deeplearning.ai
                                                                   9
   Recap: LSTM
LSTM networks are a type of RNN that can learn long-term dependencies. They use
gates (input, forget, and output gates)to control the flow of information, making
them effective for tasks requiring memory over long sequences.
 RNN vs LSTM cell

RNN
                    LSTM




                           11
         Types Of Sequence Problems in NLP Task


output



RNN/L
STM



 Input


             Image                Image                      Sentimental             Stock Market   Translation
          classification        captioning                    analysis                prediction




                   source: http://karpathy.github.io/2015/05/21/rnn-eÔ¨Äectiveness/)
                   (
Bidirectional Long Short-Term Memory (Bi-LSTM):
Motivation




      The movie was terribly exciting!
Bidirectional Long Short-Term Memory (Bi-LSTM):
Motivation             positive




                        Sentence encoding




             the             movie      was   terribly   exciting   !


        Slide credit: Daniel Jurafsky
                                                                        14
Bi-LSTM‚Ä¶
                                                        This contextual representation of ‚Äúterribly‚Äù
                                                        has both left and right context!




  Concatenated
  hidden states




  Backward RNN



  Forward RNN




                            the           movie   was        terribly      exciting         !

                  Slide credit: Daniel Jurafsky
                                                                                                       15
Bi-LSTM‚Ä¶




    Source of image: deeplearning.ai
                                       16
Bi-LSTM‚Ä¶




           17
Bidirectional RNNs
On timestep t:                      This is a general notation to mean ‚Äúcompute
                                    one forward step of the RNN‚Äù ‚Äì it could be a
                                    vanilla, LSTM or GRU computation.



               Forward RNN                                                     Generally, these
                                                                               two RNNs have
             Backward RNN                                                      separate weights

Concatenated hidden states




               We regard this as ‚Äúthe hidden
               state‚Äù of a bidirectional RNN.
               This is what we pass on to the
               next parts of the network.

          Slide credit: Daniel Jurafsky
                                                                                                  18
Bidirectional Long Short-Term Memory (Bi-LSTM)‚Ä¶




  Single forward LSTM layer   Bi-LSTM model


                                                  19
     Bi-LSTM model Architecture for Classification
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional,
LSTM, Dense
model = Sequential([
      Embedding(input_dim=vocab_size,
                output_dim=embedding_dim,
                input_length=max_len),
      Bidirectional(LSTM(n_lstm)),
      Dense(1, activation='sigmoid')
])
                                                                20
Multi-layer                                                                    The hidden states from RNN layer i
                                                                               are the inputs to RNN layer i+1
RNNs\LSTM
    RNN layer 3




    RNN layer 2




    RNN layer 1




                           the             movie   was   terribly   exciting         !

           Slide credit: Daniel Jurafsky
                                                                                                                    21
Multi-layer RNNs‚Ä¶




                    22
Types of Sequence Problems




    source: http://karpathy.github.io/2015/05/21/rnn-eÔ¨Äectiveness/)
    (
Introduction to Sequence-to-Sequence
(Seq2Seq)


‚Ä¢ Seq2Seq is a type of model used to transform one

  sequence into another sequence.

‚Ä¢ Commonly used in tasks where the input and output are

  sequences of varying lengths.

                                                          24
(Encoder-Decoder) Model
solution to Seq2Seq task




      Image source: https://pradeep-dhote9.medium.com/seq2seq-encoder-decoder-
      lstm-model-1a1c9a43bbac
                                                                                 25
Machine Translation (MT)
‚Ä¢ The sequence-to-sequence model is an example of a
  Conditional Language Model.
   ‚Ä¢ Language Model : task is predicting the next word of the target
     sentence y
   ‚Ä¢ Conditional :predictions are also conditioned on the source
     sentence x

‚Ä¢ MT directly calculates                 :


                                                               Probability of next target word, given
                                                           target words so far and source sentence x



                           Slide credit: Daniel Jurafsky
                                                                                                        26
              Training a Neural Machine Translation system                            = negative log
                                                         = negative log                                     = negative log
                                      ùëá
                                 1                        prob of ‚Äúhe‚Äù                prob of ‚Äúwith‚Äù        prob of <END>
                            ùêΩ=     ‡∑ç ùêΩùë°
                                 ùëá                   =         ùêΩ1 + ùêΩ2 + ùêΩ3 + ùêΩ4 + ùêΩ5 + ùêΩ6 + ùêΩ7
                                  ùë°=1




                                                                                                  ùë¶‡∑ú (5)   ùë¶‡∑ú 6   ùë¶‡∑ú 7




Encoder RNN                                                                                                                  Decoder RNN


                  il    a        m‚Äô       entart√©         <START> he            hit       me      with     a      pie


              Source sentence (from corpus)                        Target sentence (from corpus)

                                          Seq2seq is optimized as a single system.
                                          Backpropagation operates ‚Äúend-to-end‚Äù.

                                                    Slide credit: Daniel Jurafsky
                                                                                                                                           27
              Neural Machine Translation(Testing)
              The sequence-to-sequence model
                                                                             Target sentence (output)
                Encoding of the source sentence.
                  Provides initial hidden state
                                                          he           hit        me    with          a       pie <END>
                       for Decoder RNN.

                                                      argmax       argmax      argmax   argmax   argmax   argmax   argmax



Encoder RNN                                                                                                                 Decoder RNN


                   il     a    m‚Äô   entart√©         <START> he                    hit    me      with          a     pie


                    Source sentence (input)                    Decoder RNN is a Language Model that generates
                                                                  target sentence, conditioned on encoding.

                   Encoder RNN produces
                     an encoding of the                          Note: This diagram shows test time behavior:
                      source sentence.                         decoder output is fed in     as next step‚Äôs input

                                              Slide credit: Daniel Jurafsky
                                                                                                                                          28
              Sequence-to-sequence: bottlenecks problem
                    Encoding of the
                   source sentence.
              This needs to capture all                                     Target sentence (output)
               information about the
                  source sentence.                               he   hit      me    with    a     pie <END>
              Information bottleneck!




Encoder RNN                                                                                                      Decoder RNN


                     il     a    m‚Äô       entart√©         <START> he           hit   me     with       a   pie


                     Source sentence (input)
                                                                       Problems with this architecture?

                                 Slide credit: Daniel Jurafsky
                                                                                                                               29
Sequence-to-sequence: Limitations




                             Pair of RNN used for translation


      Natural Language Processing with Transformers, O‚ÄôReilly Media, Inc, 2022
                                                                                 30
Solution with Attention




        Image source:https://www.directenergyregulatedservices.com/blog/kw-
        vs-kwh-whats-difference                                               31
What is attention?

‚Ä¢ Attention is a weighted average over a set of inputs

‚Ä¢ How should we compute this weighted average?
   Compute pairwise similarity between each encoder hidden
   state and decoder hidden state.
    Convert pairwise similarity scores to probability distribution
    (using softmax) over encoder hidden states and compute
    weighted average

                                                                     32
     Attention
Solution to the bottleneck problem.
Benefits
     ‚û¢ Improved handling of variable-length input sequences.
     ‚û¢ Enhanced modeling of long-range dependencies.
     ‚û¢ Better performance in tasks where certain parts of the input sequence are more
       relevant to specific parts of the output sequence.


Core idea: on each step of the decoder, use direct connection to the
encoder to focus on a particular part of the source sequence




33
Sequence-to-sequence with attention
                         dot product




    Attention

     scores

    Encoder
      RNN



                il   a      m‚Äô     entart√©           <START>


                Source sentence (input)

                     Slide credit: Daniel Jurafsky
Sequence-to-sequence with attention‚Ä¶
                             dot product




     Attention

      scores

     Encoder
      RNN



                 il      a      m‚Äô      entart√©       <START>


                  Source sentence (input)

                      Slide credit: Daniel Jurafsky
35
     Sequence-to-sequence with attention‚Ä¶
                                    dot product




         Attention

          scores

         Encoder
           RNN



                     il         a       m‚Äô     entart√©    <START>


                     Source sentence (input)

                          Slide credit: Daniel Jurafsky
36
Sequence-to-sequence with attention‚Ä¶
                               dot product




   Attention

    scores

   Encoder
     RNN



                  il       a      m‚Äô     entart√©   <START>


                   Source sentence (input)

               Slide credit: Daniel Jurafsky
Sequence-to-sequence with attention‚Ä¶
                                               On this decoder timestep, we‚Äôre
                                               mostly focusing on the first


    Attention Attention
                                               encoder hidden state (‚Äùhe‚Äù)



                                                                Take softmax to turn the scores

     scores distribution
                                                                 into a probability distribution




    Encoder
      RNN



                           il        a       m‚Äô     entart√©     <START>


                           Source sentence (input)

                                Slide credit: Daniel Jurafsky
Sequence-to-sequence
               Attention
                         with attention‚Ä¶
                                                                        Use the attention distribution to take a
                                                  output                weighted sum of the encoder hidden
                                                                        states.

      Attention Attention                                               The attention output mostly contains
                                                                        information from the hidden states that
                                                                        received high attention.

       scores distribution


      Encoder
        RNN



                             il      a       m‚Äô     entart√©   <START>


                             Source sentence (input)
                             Slide credit: Daniel Jurafsky
39
Sequence-to-sequence with attention‚Ä¶
                                                  Attention         he
                                                   output
                                                                            Concatenate attention output with


      Attention Attention
                                                                    ùë¶1^     decoder hidden state, then use to
                                                                            compute ùë¶1^as before




       scores distribution


      Encoder
       RNN



                             il       a      m‚Äô     entart√©       <START>


 40                          Source sentence (input)
                                  Slide credit: Daniel Jurafsky
Sequence-to-sequence with attention
                                                Attention                hit
                                                 output



     Attention Attention
                                                                        ùë¶2^




      scores distribution


     Encoder
       RNN



                            il      a      m‚Äô      entart√©       <START> he


41                          Source sentence (input)
                                 Slide credit: Daniel Jurafsky
Sequence-to-sequence with attention‚Ä¶
                                                    Attention                 me
                                                     output



     Attention Attention
                                                                              ùë¶3^




      scores distribution


     Encoder
       RNN



                            il          a      m‚Äô     entart√©    <START> he   hit


                            Source sentence (input)

42                               Slide credit: Daniel Jurafsky
Sequence-to-sequence with attention‚Ä¶
                                             Attention                               with
                                              output



     Attention Attention
                                                                                     ùë¶4^




      scores distribution


     Encoder
       RNN



                            il   a      m‚Äô     entart√©            <START> he   hit   me


                            Source sentence (input)
                                  Slide credit: Daniel Jurafsky
43
Sequence-to-sequence
              Attention
                        with attention‚Ä¶
                                                                                       a
                                                  output



     Attention Attention
                                                                                      ùë¶5^




      scores distribution


     Encoder
       RNN



                             il      a       m‚Äô     entart√©   <START> he   hit   me   with


44                           Source sentence (input)

                            Slide credit: Daniel Jurafsky
Sequence-to-sequence with attention‚Ä¶
                                             Attention                                          pie
                                              output



     Attention Attention
                                                                                                ùë¶6^




      scores distribution




                                                                                                      Decoder RNN
     Encoder
       RNN



                            il   a      m‚Äô     entart√©           <START> he   hit   me   with    a


                            Source sentence (input)
                                 Slide credit: Daniel Jurafsky
45
Attention Mechanism Benefits vs Challenges


 How does attention address the temporal bottleneck in
 sequence-to-sequence models?




                                                         46
Transformers(2017)




       *https://arxiv.org/abs/1706.03762   47
 What is Transformer

‚Ä¢ The Transformer in NLP is a novel architecture that aims to solve
  sequence-to-sequence tasks while handling long-range dependencies
  with ease.
‚Ä¢ The Transformer was proposed in the paper Attention Is All You Need *.


‚Ä¢ Relying entirely on self-attention to compute representations of
   its input and output.



           *https://arxiv.org/abs/1706.03762
                                                                           48
Transformer Architecture




                           49
Q&A




      50
