We have a huge source of text in our world. This text comes from books, articles, news, or even websites. So we call it a document. Any source of NLP text is a document. This document has specific characteristics. We have structured representation and unstructured representation. And this is the main challenge of dealing with text. The text is unstructured. Articles, there is no structure here. Ambiguous, the main characteristic of any natural language, English, French, Arabic, Chinese, any language, is ambiguous, and I will show you what that means. And we have a huge source of this data.

On the other side, we have knowledge. What do we mean by knowledge in this context? What is knowledge representation? Can you give a definition of what knowledge representation is? A set of rules, and the programmer sets the rules, and the machine figures out the plan or tries to execute. So it's very good. The rules that are then interpreted by a machine to be able to act in the world. So all the purpose of knowledge that we know, structured in the brain, is that we can be able to extract information from that knowledge. So knowledge representation is to represent any data in a form that the computer can understand. This is the main objective of knowledge representation. It can be in a network, like logic, or in a formal way to represent structured data in a form that the computer can understand and work with. So this is the knowledge, and the specific characteristics of any knowledge are that it is structured, precise, actionable, we can build many applications from it, and specific to the task in a specific domain.

Now, what is the role of NLP? NLP aims to convert this unstructured information from documents to knowledge that we can use to build many applications. So the main characteristic is that documents are something humans can read, but slowly. We cannot memorize everything. We read it very slowly. But when you represent it in a knowledge representation format, it will be fast, easy to find the information, and it has all these characteristics of the formal representation for any application. So this is the role of NLP. How do we collect data from different resources, restructure it to something that can be used, and then build applications on top of that?

Now, the formal definition of natural language processing. There are many formal definitions. If you open any book, you will find each author defines natural language processing in a different way. But I like this definition. It's a subfield of linguistics, because we use language, natural language, so linguistics is very important. And computer science and artificial intelligence. The main objective is to analyze and understand human language. So how to program a computer to process and analyze large amounts of natural language data. So this is the main definition of what natural language processing is.

So what is our motivation? Why do we have natural language processing, and why is it important especially now? Starting in 2017, for every year, if you make a research study from 2017 till now, how many papers were published in this field. If you compare it with another field, you will find NLP is at the top.

Our motivation is to understand and process human language. We can build many applications if we understand language. Translation, we can translate if we understand language. We can do summarization. We can build many applications if we are able to understand and analyze human language. Also, industry in many areas is using NLP applications. They are now able to analyze the feedback of customers and improve their sales based on customer feedback. So it has many applications in the business area as well. And this is very important: anyone with a disability can easily contact devices by voice. Anything related to speech is related to NLP. And computers can access information, extract insight, and make sense of data. The common application is sentiment analysis. We have a huge amount of data in social media: X (which was Twitter before), Facebook, and other social platforms. How can we find the trend in social media? How can we find the feedback of the people regarding a specific topic on social media? As you see, this application actually became very visible during elections, based on how people spoke on social media.

This is part of our motivation. We have many reasons to learn natural language processing. Career opportunity: now there is high demand for people who understand natural language processing. With the increased use of large language models, there is a need for people who understand what natural language processing is.

What is the difference between AI and machine learning? So I agree with most of you that you find that the difference between AI and machine learning is that AI is broader. It's not just one thing. I believe the definition here is that AI is the umbrella that includes machine learning and many other fields. So natural language processing, speech recognition, computer vision, all these applications fall under the umbrella of AI. The general definition of AI is just how to make a computer act like a human. AI is about the behavior of intelligence. When you ask a very small child to get something from the fridge, this is intelligence. This is human intelligence. How to make the computer mimic this behavior, this is what we are trying to do. To make the machine imitate or simulate human behavior.

And machine learning is a subfield of AI where we let the computer learn from data itself. We used to write programs to give a computer instructions for a long time before. With machine learning, there is no need for that. Just feed the computer with the data, and the computer understands or learns from this data and tries to perform any machine learning task: classification, clustering, forecasting. What are the other tasks? Association, outlier detection, anomaly detection. So you can find these main tasks. These are the main tasks of machine learning. One of them is association. What is this? Association between elements. I know the application. You know, when you go to the supermarket, they can offer two different items together. They learned from data that if you put this item with this item, they sell well together. So this is the idea behind association. How they use it to make an offer.

So now, what is the relationship between these terminologies? AI, artificial intelligence, machine learning, deep learning, and natural language processing. So this is the relation. As I mentioned, AI is the umbrella for all the applications related to how to make the computer intelligent. And then as you see, we have machine learning. And as a subset of machine learning is deep learning. So deep learning performs the same tasks as machine learning, but instead of using normal machine learning techniques, we use deep neural networks. That's why deep learning is a subset of machine learning. And then we have natural language processing. As you see, it intersects with machine learning and deep learning, as natural language processing uses all the machine learning techniques and deep learning techniques.

So the idea is, in machine learning, what is the format of the data? You have to create a table. And this table has some instances, which are in rows, and here in the columns are the attributes or features. So we have instances and features. And then at the end you have the decision, or we call it the class. So based on the values of the attributes, we can assign a class based on the application. And usually this information is numeric. And if you have categorical data, as a preprocessing step, you have to convert it to numbers. Why? Because the computer only understands numbers.

We will do the same thing. We will convert the text to a numerical format. And this is the big preprocessing step for any text. How to convert the text to a form of numbers. There are many techniques to do this, and at the end, we have a table of numbers. And then we can apply any machine learning technique based on this numerical data, or deep learning. So our preprocessing focuses on how we represent text in a form of numbers. And then once we have a table of numbers, it will be ready for applying any machine learning techniques. That's why NLP has an intersection with machine learning and deep learning.

Now, a brief history. The starting idea in natural language processing was in the 1950s, where they had a very simple translation machine. They call it a machine from the early days. They started to have a very simple, very limited translation machine. And in 1960, the first chatbot, a very simple chatbot. If you search about it, they call it ELIZA or something like that. It was a rule based chatbot. And then actually the main advance of the field happened in the 1970s, where we started to use statistical models to represent language. And then while there was improvement in machine learning techniques, the next step was how to use these machine learning techniques in natural language processing and enhance the applications.

In my opinion, the main step actually was in 2013 when Word2Vec came out. It made a lot of difference and enhanced NLP applications. And the biggest step was in 2017 when the researchers published the very famous paper. What is the paper name? "Attention Is All You Need." Very famous paper. And starting from this point, this was the year of large language models, because a few years after, we heard for the first time about GPT, which is built on top of the idea of the transformer. The transformer is a building block for all modern NLP applications. So it is a basic architecture. And then they updated and developed it in different formats: encoder only, decoder only, encoder decoder. All large language models are built on top of this architecture. The transformer architecture has been implemented in different forms. It completely changed everything.

So this is a very big step, because all upcoming applications are developed on the foundation of the transformer architecture. The transformer architecture comes in different forms: encoder only, decoder only, encoder decoder. All large language models are built on top of this.

Before we talk about applications, there is a very famous test in our field of natural language processing. This test was devised in 1950 by a British scientist called Alan Turing. He invented this test, and this test is now considered as a benchmark for all AI applications. It's still used now. So what is the idea of this test? We have three components in this test. We have a judge or evaluator, a human. And then we have a computer system, and we have another human. Now, the idea is the judge asks a question. The judge is not able to see who is the computer and who is the human. The judge asks a question, and both the computer and the human answer this question. If the judge is able to differentiate which answer came from the computer and which answer came from the human, the application does not pass the Turing test. But if the judge is not able to differentiate which answer came from which, in this case the machine, the computer system or AI system, passes the test. So it's used in AI applications, and it's common to use it also in natural language processing.

Now, applications of natural language processing. Can you give me an example of an application? Forensics, for detection, yes. Any other application? Spam detection. Spam or not spam. This is a natural language processing task. Your iPhone autocorrect, when you write something and it corrects a name, that's natural language processing.

So we have applications of natural language processing in many areas. Before I talk about the applications, I want to differentiate between types of applications. If you divide natural language processing applications, we can see that they are divided into two main parts or categories. One is natural language understanding, and the second one is natural language generation. Some applications use only natural language understanding. Some applications are based on natural language generation. Some applications combine both understanding and generation.

For example, pure natural language understanding. Can you give me an application that uses only understanding of the text? Sentiment analysis. Yes, it requires understanding, but it's a deep understanding. Translation? Translation is understanding and generation, because you understand the source text and then you generate the translation. Classification? Classification is based on understanding. We can classify text based on understanding. Spam or not spam, for example, is classification based on understanding of the text.

Another example of natural language generation, like translation, is another example. Summarization. You understand the text, then generate a summary. So this is a different type that combines both. Pure natural language generation, just generating language. Can you think about an application that is just natural language generation, no understanding? Speech to text. For speech to text, do you have to understand? There is no deep understanding. They try to understand the signals, then convert the signal to words, then combine the words to generate text. Another application: any reporting generation, automatic report generation, is natural language generation. For example, an automatic medical report is generation. We have some features related to the patient, measurements, analysis, and then automatically based on this information, a report is generated. So any automatic report generation from data is natural language generation. But most modern applications actually combine natural language understanding and natural language generation.

Now, NLP is not purely NLP by itself, but NLP plays a very important role in speech systems. A speech recognition system, the input is voice, and then we convert this voice into text. So it's a combination of different techniques. There is signal processing here, because you have a speech as a signal. Signal processing plays an important role in how to convert the voice into signal. And we have an acoustic model to convert these signals to characters or words, and then we have NLP to convert this into text. So it is not pure NLP, but NLP is an important part.

For example, auto translation in real time. How does it generate the translation? And we have another application: conversation agents and chatbots. We have speaking chatbots or text based chatbots, where the computer can answer some questions from the user. They use chatbots in customer service and technical support. When you try to make a conversation with Amazon or similar services, a bot will reply to you. They use speech recognition here. For example, Alexa or Siri takes the speech and then there is language understanding to understand it as part of speech recognition, to convert the language, and then the system generates a response and does the opposite of speech recognition, converting the text into words. So as you see, it's a combination of many techniques. One of the techniques is dialogue management, which tracks the state of the conversation and controls the flow between question and answer. So it's a very complicated system, but natural language processing is an important part. It's involved in all these systems.

Another application, which is more purely natural language processing, is text classification, and we have many applications of classification. Sentiment analysis, spam or not spam, language detection. I can feed the computer text in a specific language, and then it can predict which language it is. We have document detection, paragraph detection, where you can feed the computer text and then it can classify if it's news, sports, business, or whatever. So there are many applications in classification.

And we have sentiment analysis, which is very important. Sometimes people mix up text classification and sentiment analysis, but actually there is a huge difference. In text classification, you need to understand or have a shallow understanding of the language. Usually, how text classification works is they try to find a specific word in the text and then based on this word classify the text as spam or not spam, for example. So it doesn't need a deep understanding of semantics. But in sentiment analysis, we need a very deep understanding of the text, because we are not just classifying the text. We try to find the emotion behind the text, the attitude behind the text. The difference between classification and sentiment analysis is the deep understanding of semantics, because we need to extract insight, trends, emotions, and attitudes. But again, it is a very important application of natural language processing.

Another important application is text summarization, where you have a text and then your computer system summarizes this text into a short text. Actually, text summarization is one of the hardest NLP applications. It does not work as easily as you might think. Because there are many challenges in text summarization. How do you identify important information? How do you make the produced summary coherent, where the words and phrases are connected to each other? And dealing with multiple challenges depending on the source that you want to summarize. For example, summarizing an article is not like summarizing a research paper. They are different. So there are a lot of challenges.

One challenge is redundancy. In some sources like articles or books, we can get repeated content. Another is complexity. Some texts are complex and have many ideas behind them. How do you find a way to deal with this complexity? Another problem is that after you try to solve all these problems and get a very good summarization system, you cannot apply it in any language. So if you can create a summarization system for English language, it's not easy to apply the same for French language. There is no one to one correspondence between different natural languages. So there is no way to have a generic summarization system that can be applied to any language. You have to reconstruct it from scratch. So there are a lot of challenges in summarization.

Question answering. Most probably question answering will be the topic of your project. I used to teach question answering, where you have a question and the system provides an answer. There are two common techniques. The first is extractive question answering, where we have a text and a question, and we just extract the correct answer from the input text. Just extraction: find where the answer starts and where it ends. That is extractive question answering. On the other side, we have generative question answering, where you generate the answer using different techniques. So we will cover question answering more in our course.

ChatGPT is one application of NLP. Of course, there is reinforcement learning inside the architecture of ChatGPT, but it is one of the applications of NLP. It's text generation. You ask any large language model a question, and based on your text, it generates the answer. So it's also part of NLP applications.

So none of these applications are easy. There are a lot of challenges, and all of these challenges come from the nature of any natural language itself. All natural languages have major challenges. So making the computer understand language is not an easy task.

One of these challenges is ambiguity. All natural languages are ambiguous. A word can have different meanings depending on the context. This is true not only in English but in many languages. All languages are ambiguous. And we have different forms of ambiguity.

Words have multiple meanings. For example, the word "bass" has two different meanings. It may refer to a type of fish, or it may refer to low frequency in music. The word "bank" may have two different meanings: the side of a river or a financial institution. So ambiguity is a part of any language. This is an example of ambiguity in any natural language, words that have different meanings.

Another form of ambiguity we call attachment ambiguity. What do we mean by attachment ambiguity? It's about which phrase or word attaches to a specific modifier in a sentence. For example, look at this sentence: "She saw the man with a telescope." Does "with a telescope" attach to the man or to her? So she saw the man through a telescope, or she saw the man who was holding a telescope. It's ambiguous. As humans we can understand from the context, but how to make the computer understand this is a big challenge.

Another type of ambiguity we call coreference ambiguity, where we don't know which entity a pronoun refers to. For example, read this sentence: "My girlfriend and I met my lawyer for a drink, but she became ill and had to leave." Does "she" refer to my girlfriend or to my lawyer? From the sentence alone, it's not clear. So this is one of the challenges. When you build your NLP application, you have to consider what the challenge is and find a way to solve it.

Another challenge is sparsity. What do we mean by sparsity? Take any text and sort the words by their frequency. You count the frequency of each word and then rank them. So you'll find that some words are highly frequent and some words are rarely used in the text. So you have a frequency and you have a rank. The first one, for example, may appear 1000 times, so its rank is one. The second appears 800 times, so its rank is two. And so on. So what is the relationship between frequency and rank? When the rank increases, the frequency decreases. Inverse proportion. So this is what we call Zipf's law. Rank is inversely proportional to frequency. When there is high frequency, there is low rank. When there is low frequency, there is high rank.

With this analysis, any text from any type, from any resource, whether it's a book, an article, a news piece, or a scientific research paper, they all have the same pattern. Some words are repeated very often, and some words appear very rarely. And why is this a challenge? Think about machine learning. In machine learning, you have a table with instances and features. When you find that a value appears only one time, how do you consider this instance? If it just appeared one time, there is no pattern. How do we consider that? What is our usual response? It's an outlier. What is the definition of an outlier? Something not habitual. In machine learning, you make the machine understand patterns. When something repeats many times, the machine comes up with an idea that with this feature, this decision happens. Now, if something just happened one time, how are you going to learn from it?

But in natural language processing, these rare words can be very important for the text. And this is a challenge. As Zipf found in his law, about one third of words are repeated and have high frequency, and then we have the middle range, and then we have rare words which appear very infrequently. And most text follows this same pattern. So the problem is some of these rare words may be very important to identify the type of data, to identify important information. They are important in summarization. A word like "fuzzy logic" may appear only one time, but it's very important. And this is the nature of any natural language, and this is a challenge.

Another challenge is variation. Not between different natural languages, but there is variation inside the same language. The variation comes in many forms. Variation within the same language, geographically. For a huge area like a country, we have this experience here in Canada or in your home country, there are different cities or different provinces where the language varies from one part to another. So geographical variation: people speak the same language, but from different areas they have different vocabulary, different grammar, different ways of talking. This is a type of variation inside the language.

Another variation is social variation. When you talk to an educated person, it's different from when you talk to a very simple person, even in your own language. This is a variation in language. There is also a variation between styles, formal and informal. When you write a formal letter, it's not the same words that you use when you talk to your friend or when you talk to your boss or professor. So formal language and informal language within the same language use different terminology and different expressions.

Generational variation is another type. The language that older generations used is completely different from what young people use now. New technology brings new vocabulary and new ways of expression. And there is variation between different languages. What is the problem here? If you create an application that works perfectly in English, you cannot take that same application and apply it to a different language. It's not feasible. You spend time, money, and effort to create a perfect application, and you cannot simply take it and apply it to another language. You have to do a lot of additional work. You can take some benefit from the effort, but you have to start a lot of the work from scratch because it's a different language. And this is actually a challenge for NLP.

The last challenge is common knowledge. Between humans, we have common knowledge. But how do we transfer this common knowledge to the computer? For example, if I have two sentences: "a man with a dog" or "a dog with a man." As a human, what is our common knowledge? The natural phrase is "a man with a dog." But for a computer, it doesn't know which is the correct or natural phrasing. The sentence structure is fine in both cases. This is common knowledge for us, but how do we convert all this common knowledge to a computer? How do we make the computer understand this common knowledge? Other examples of common knowledge: the Earth is round, the sun is hot. Everyone knows that. But how do you transfer all this common knowledge to the machine? This is another type of challenge.

And there are more challenges. Volume of data, dealing with accents and slang, the complexity of text with many ideas behind it, the evolving nature of techniques. Look at translation 10 years ago compared to translation now. There is a huge improvement as technology improved, as architectures improved. Computation resources are another challenge. To train any model, you need huge resources: time, power, human effort. We need huge computation resources. And security and privacy. When you use ChatGPT, your conversation data can be used to improve the model, so it becomes part of the training data. So security and privacy are big challenges now. There are many challenges in building NLP applications, and you have to consider them when you build your NLP system.

Now, the approaches to NLP to solve these applications. Starting from early techniques, we use heuristic based approaches, like regular expressions. Regular expressions are a technique that can match or extract patterns from text. And then they came up with machine learning techniques, more advanced techniques than heuristic based ones. And then we have deep learning, RNN, and advanced versions of RNN used in natural language applications, and we end up with the transformer architecture, which is the state of the art technique.

Please don't consider that the old techniques are no longer used. For example, heuristic based NLP which uses regular expressions is a very old technique. You might think there is no need to use it now. Actually, it surprised me. I did a lot of investigation in the industry, and I found out that even when you build an NLP application with state of the art techniques and everything is fine, you still have some cases where your system fails on those cases. When you are running the system in production, the system works very well, but it still fails in specific cases. What do you do? Do you rebuild the system from scratch? No. What they do is they plug in a very small rule based system to handle these specific cases. A small system that works alongside the main system. So they still use these old techniques today. Don't say that it's an old technique and no one uses it. They are still used now. Some applications don't even need a transformer. A transformer is good, but you need resources to run a transformer. Some applications just need simple machine learning, and it's successful. So it depends on the application.

Some important resources for NLP: the common ones are NLTK, spaCy, Hugging Face, which is a very important resource for our course. We will also use scikit learn, of course, but I think we will use it at the beginning for preprocessing, for how to clean your text and then convert your text to a table so you can apply any machine learning technique on it.
