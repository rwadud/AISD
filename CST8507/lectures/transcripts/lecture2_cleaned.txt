The main power of a regular expression comes from its metacharacters. In regular expressions, we have a list of metacharacters, each with a specific meaning. The first is the dot. The dot matches any single character. To make it easier to understand, I have an online tool for regular expressions. I usually recommend that my students, when programming, test their pattern using this online application first. Once they make sure that the pattern is correct, they can put it back into their code.

For example, here I have a text that says "natural language processing," and then I can type a dot. What do we mean by a dot? It matches any single character. So I am searching here for "a" followed by a dot, this colored one, matching any single character. Now, how many matches are here? The interesting thing about this application is that it shows the number of matches highlighted, and also gives you an interpretation and explanation. So this is what we mean by "dot matches any single character."

The second symbol here is the square bracket. Let me start with a question mark. The question mark means zero or one. If I replace that with a question mark, how many matches are here? 288. It is approximately all the characters, because "a" followed by zero or one characters. So even if "a" is not followed by a character, it still matches. The other example here is the plus. What do we mean by plus? Match one or more characters. Now, "a" followed by plus means "a" followed by one or more characters. How many matches here? Four matches, because I have "a" appearing four times. Now, if I say "b" followed by one or more, there is no match in this case. So plus means one or more, and question mark means zero or one.

Now, going back, we have the star. The star matches zero or more characters. If you go back and write star instead of plus, you get 288 matches, covering every character, because it matches zero or more characters.

This is the basic meaning of the metacharacters in regular expressions. Now I will go back to the square bracket. What do we mean by the square bracket? It matches any of the characters listed inside it. Anything that contains "a" or "b" or "c" will find a match. It is like an option: "a" or "b" or "c." This matches a list of characters.

Now we have the caret. Based on its position, what does this character do? If it comes inside the square bracket, it means not "a" or "b" or "c." So here we can find the matches that are not "a," "b," or "c." But if I remove it from inside and put it outside the square bracket, it indicates the beginning of the string. We should find "a," "b," or "c" at the beginning of the sentence. Based on this rule, there is no "a," "b," or "c" at the beginning of the sentence.

As the counterpart to the caret, we have the dollar sign for the end. The dollar sign means end of string, so I can search for a specific pattern at the end of the text or the sentence.

This is the meaning of metacharacters. We also have more metacharacters. We can use curly braces to search for occurrences with a specific minimum and maximum. If I go back to my online application and search for "a" appearing two to three times, there is no match. "A" must come together two or three times consecutively. If I add more "a"s, I can find one match. So this is a minimum and this is a maximum. You can also search for just "a" repeated exactly three times.

The next one is the escape character. In regular expressions, certain characters have a specific meaning, like the dot. When I tried to search for a literal dot at the end of a sentence, I put a backslash before it. The backslash is an escape character, allowing us to search for the actual dot rather than the dot as a metacharacter. When we search for a specific character that is also a metacharacter, we add a backslash to differentiate between the literal character and its metacharacter function.

We have the pipe, which is "a" or "b." You can choose between two characters or two patterns. So even "aa" or "b."

The last one is a group using parentheses. There is a big difference between parentheses and the square bracket. The square bracket is an option: "a" or "b" or "c," from "a" to "z," or whatever. Parentheses are completely different. If I choose "abc" as a group, it searches for a pattern inside your string where "a," "b," and "c" come together in that exact order. They must come together as a group, considered as a single unit. Is there any "abc" here? No, that is why there is no match. If I make a small change, now I have a match. This means a group: consider it as one unit, and the characters must come together in the same order. If you switch the order, it will not match. For example, if I make it "acb," there is no match.

Now, some special character classes. Backslash lowercase "s" is completely different from backslash uppercase "S." The negation of each character class has a different meaning. Backslash lowercase "s" matches any whitespace. If I write here backslash lowercase "s," it matches all the whitespace. If I make it a capital "S," it is the negation: match any non-whitespace.

This is the difference between the lowercase letter and the capital letter for these specific characters. The same for backslash "w." Backslash lowercase "w" matches any word character. Backslash capital "W" means non-word character. The same for backslash "d." Backslash lowercase "d" matches any digit from zero to nine. Backslash capital "D" matches any non-digit character. So the negation here is between the capital and the lowercase.

In your textbook, this part is covered in detail. That is why I put it in the required reading in chapter two. They explain regular expressions in a fantastic way, and also provide a lot of examples. These examples come from your textbook. Just practice what we have covered now.

We can also use character ranges. If it is lowercase "d" or "w," it will match. We can write it in a simpler form: [a-z] for any lowercase letter, [A-Z] for any uppercase letter, and [0-9] for any digit from zero to nine. So it gives you a simpler way to write your regular expression.

Negation: when you have a caret inside the square bracket, it means negation. But when you have a caret outside the square bracket, it means the beginning of the string. So it should match at the beginning of the sentence. The same for the dollar sign: we match at the end.

This is a link for the online application that I use. It is good to practice with it. It gives you not just the matching but also an interpretation of why it is matching and details on the result. So it is a very good tool to work with.

In Python, we have a library that works with regular expressions called RE. RE is a Python library that provides functions for working with regular expressions. The first one is match. Match takes two parameters. The first parameter is the pattern you search for, and the second parameter is the string. It returns the object that matches a regular expression at the start of the string. Then we have search, which takes the same parameters, but the output is different. Search returns the match anywhere in your string.

This is a regular expression example where I have code that shows the difference between search, match, and findall. Here I have a string, and I am searching for "123" inside it, repeating the same search using different functions: search, match, and findall. The output is completely different for each. Search finds the match and gives the start and end indices where it occurs inside the string. It finds "123" starting from index 42 and ending at index 45. This is the output of search.

Match returns None here because match searches only at the beginning of the string. If there is no match at the beginning, it returns None. Search, on the other hand, searches anywhere in the string and returns the first occurrence. Even if you have multiple appearances of the pattern, it returns only the first one. Findall returns all occurrences, searching everywhere and returning a list of all matches.

So this shows you the main difference between search, match, and findall.

For writing this code, I have to import RE at the beginning. I import RE, which is the regular expression library in Python.

Another function is sub, which returns a substituted string based on the matching pattern. We also have a very important function called compile. The idea of compile is that you can compile a pattern, and the output of the compile function is a reusable object. You can use this object later in your code whenever you need to apply the same pattern.

We also have group, which returns the matched text as a subgroup, and span, which returns the starting and ending positions of the match in the text. Group and span serve different purposes. Findall finds all occurrences and returns a list of matches, while search returns a match object from which you can extract the span.

There is also finditer. It is like findall, but it returns an iterable object. So you can iterate through this object. Inside finditer, I can use group to return the matched text and span to return the start and end positions for each match. So based on finditer for a specific pattern, I can print the position of all the matching elements. This is the first match starting here and ending here. This is the second match, starting here and ending here.

These are the main functions that we use from the RE library in Python, but there are more that you will discover with experience. We also have split, which allows you to split your text based on a specific delimiter. Here I split on a space, and it returns all the resulting strings. This is helpful when you have a text file and you want to extract the list of words from your corpus. Since words are separated by spaces, applying split gives you the list of vocabulary or the words inside your text file.

Why do we study regular expressions? As you will see in your lab, we can perform a very simple NLP application only using regular expressions. We can use them for text cleaning. For example, if you have a text with a lot of unwanted characters, we can search for matches of those characters and replace them with a space or remove them. There are many uses of regular expressions in text cleaning, when you try to remove unwanted characters. If you have a sentiment analysis task and you want to get rid of all the emoticons, or if you have an HTML file and you want to extract only the text by removing all the tags, we can use regular expressions. It is a powerful tool for all these cases.

Information retrieval: we need to extract information that matches a specific pattern from a huge corpus or a huge document. Regular expressions have many applications in information retrieval.

Sentiment analysis: I am searching for a specific pattern to indicate whether a sentiment is positive, negative, or neutral. You will create a sentiment analysis system based on regular expressions. I give you a huge text divided into different sentences, and for each sentence you have a dictionary of good words and bad words to indicate whether the sentiment is good, bad, or neutral. You extract the good words, count how many matches of good words and how many matches of bad words, and then compare. Not very accurate, of course, but it is a very simple approach. If the sentence contains more good words than bad words, we indicate this sentiment as positive. If the good words and bad words are equal, it means it is neutral. Not very accurate, but this is a first attempt at how to create a sentiment analysis system.

Language detection: given a document, I can search for specific patterns unique to a language, so I can detect whether this is English, French, Arabic, or any other language by searching for patterns unique to that language.

These are not all the applications of regular expressions, but they represent some of the key applications in NLP.

Now, NLP development follows a methodology similar to software engineering. What is a common technique of building any application in software engineering? We have two techniques: the waterfall model and Agile. So this is similar to a waterfall, a set of steps for how we build any system. In software engineering, we call it a development life cycle. NLP also has this development life cycle. I modified this based on my previous knowledge, but it contains the same steps.

We start with data collection. Any NLP application should start with collecting data, and this collection should be huge. You will see later that the more data you have, the more the performance of your application improves, because you give the model more information to learn from. At the same time, the data should be relevant to your task. So data collection is a starting point and a very important one, with two conditions: the data should be large enough, and it should be relevant to your task.

The second step is text cleaning, to get rid of any noise in your data. For example, in machine learning, the first step is preprocessing. What happens in preprocessing? There are many techniques. Removing duplicates is one type of noise removal. Replacing missing data or deleting it based on your decision is another. You might search for all missing data and choose to delete those records or replace them with the average or median. There are many techniques for handling missing data. Feature selection and feature engineering are also part of this process. Changing the format of the data is another task: some data comes in a categorical form, some in a numerical form, so unifying the format is one of the preprocessing tasks. Dimensionality reduction involves getting rid of irrelevant data. If you have a column that is not relevant to your processing, like ID for example, why would you include it in your data?

All of these are techniques used in preprocessing, but not all of them apply directly to NLP. Sometimes I want to remove noise from my data. Some characters are irrelevant, so I clean the data by removing them. Sometimes I have the same word appearing with a capital letter in one place and a lowercase letter in another, so I have to convert everything to lowercase. This is a type of preprocessing and text cleaning. If you are working with HTML files, you want to get rid of all the opening and closing tags and keep only the text. This is another type of cleaning. Cleaning the data, whether in natural language processing or in machine learning, is very important and has a huge effect on the performance of your model. Remember: garbage in, garbage out. If you enter bad data, do not expect good results.

This part of any task is time consuming and takes a long time, but it is very important. I divide it into two parts: cleaning and then preprocessing. After cleaning and removing all the noise, I move to preprocessing. Preprocessing means we standardize the input through the pipeline. We have a huge number of documents in different formats, and I want to create a standard form for all of them.

Then we go to the feature extraction or feature engineering phase, where we choose features and create our feature table, similar to machine learning. After that comes modeling, where we apply the modeling technique. Then comes evaluation, where we evaluate the model. Finally, deployment, where we put the model into a production environment. This is important because you may even need to integrate the model with another system in production. All of this is part of the deployment phase.

Then comes monitoring, which is a very important step. You do not build a system and let it run for its entire lifetime without oversight. You monitor it to make sure that the performance does not drift. Remember model drift: sometimes a model works fine initially, but over time it drifts. It may introduce bias, or the performance may degrade. Think about the sentiment analysis example. Would it work 10 years later with the same performance? No, of course not, because new terminology, new language, and new ways of expression are introduced over time. So we have to monitor our system to make sure that it works as expected.

This is not a linear process. There is an evaluation step where we may find that the results are not satisfactory. In that case, we can go back to preprocessing or go back to collect more data. It is not a linear process going in only one direction. During monitoring, we may also need to add more data to keep the model performing as expected.

There is something missing in this pipeline. Before you collect data, you need to understand the problem. This is what I added to the pipeline. The original pipeline comes from a specific book, but I added this part based on my experience in software engineering. You have to understand the problem at the beginning. You should not go and collect data first. Sometimes the task may not need NLP at all. Understanding the problem and gathering requirements from the user are basic steps. If you have a client who wants to create a system, gathering information and understanding the problem is essential to deciding whether NLP is the right approach. What type of task will you perform? Classification? Clustering? Or it may be that NLP is not the right choice, and there is a simpler solution. In my opinion, gathering requirements and understanding the problem is a very important step before you start collecting your data.

In this lecture, I will focus on two steps: text cleaning and preprocessing. We will go to the other steps in upcoming lectures.

What is the motivation for text cleaning and preprocessing? As we discussed, garbage in, garbage out. We need to clean our text by removing all redundancy, irrelevant characters, and irrelevant information to improve the performance of our model. We need clean, standard text data suitable for our NLP task. We also need to convert the data into a standard form. Usually, when we gather information, we get it from different sources: PDFs, text files, and web resources. We need a preprocessing step to make all of this uniform, creating a standard form regardless of the source or format. This is an important task, and it is done during preprocessing. Clean input in a standard form will improve the performance of your model.

The text preprocessing pipeline: I have to mention that this is my opinion. It may differ from others. Some people or textbooks combine normalization with noise removal, or divide it into more steps. But in my opinion, these are the main steps to preprocess your data. Sometimes the order may vary based on your task. You can perform noise removal before normalization, or normalization before noise removal. It depends on your task, but normally, in most NLP applications, this is the logical sequence of steps in a preprocessing pipeline.

You enter a document; this is your data. The first step in preprocessing is tokenization. Tokenization is the input for any NLP model. If you have experience with NLP models, even the transformer, the input of the transformer is a token. The technique that produces tokens for each model may be different. For now I will talk about the classic techniques. Later on, when we come to transformers, we will cover more advanced tokenization techniques. But tokenization is the first step.

The second step in our pipeline is noise removal, removing any noise in your data. Then we come to normalization. I will go through each of these and discuss the specific terminology.

The first topic is the building blocks of any language. What do I mean by building blocks? The fundamental units that make up any natural language. Our example is English. If you consider English, we have these building blocks.

What are phonemes in the English language? They are small units of sound that make up our language. Does a phoneme have any meaning on its own? No. It is just a small unit of sound. When combined, phonemes produce meaning. In English, we have 44 different phonemes, which are the basic sounds of the language.

On top of phonemes, we have morphemes or lexemes. What are morphemes and lexemes? They are the smallest units of language that carry meaning. For example, "untangling" can be divided into a set of morphemes. A morpheme is the smallest unit of language, not sound.

On top of morphemes, we have syntax. What is syntax? It is how morphemes are arranged according to a set of grammar rules. The structure of the sentence, how we build the structure of any sentence in a language, is governed by a set of rules we call grammar rules.

On top of syntax, we have context, which consists of a couple of sentences or a paragraph conveying a specific meaning. These are the building blocks of any language, in our case English. Each block has a specific application in NLP.

In a previous lecture when we talked about speech recognition systems, phonemes play an important role. When we speak, we divide the signal into sounds and then combine the sounds to form a word. So phonemes have applications in speech recognition, speaker identification, and text to speech.

Morphemes have applications in tokenization, where we divide our text into chunks of tokens, which are different from words. We need to tokenize for many applications: for embeddings, as we will discuss later; for part of speech tagging, where we identify the part of speech in any sentence; and for many other applications.

Syntax is used for checking if the sentence is correct according to the grammar rules of the language. It has many applications in morphological analysis and even in testing the performance of applications like machine translation. When you translate, you check if the translation is correct according to the grammar rules of the target language. So syntax plays a very important role in such applications.

Context is used to understand meaning and to resolve ambiguity in language. Each building block is needed and applied in different NLP tasks.

Another important concept in NLP is the corpus. What is a corpus? It is your data, a collection of documents from various sources. This is what we refer to as the corpus in NLP. We also have words, the smallest unit of language that carries meaning. Usually, we identify words in English by the spaces between them.

What is tokenization? Very simply, tokenization is dividing your text into a set of tokens, not words. We do not divide the sentence based only on spaces. A token is a small unit of language. There are many tokenization techniques. Splitting the sentence on spaces is one technique, an early and simple one. Tokens play a very important role in all NLP applications, even modern ones. You have to break your text into a set of tokens, and there is a difference between tokens and words.

Semicolons and other special characters in your text are also considered tokens. That is why there is a difference between a word and a token. Whitespace is not the only criterion for generating tokens from your text.

If you want to generate a vocabulary, you split on spaces. A vocabulary is a set of unique words in your text. But tokens are different. Some words like those joined with an apostrophe are treated differently. For example, "can't" is a word, but if you use a tokenizer, "can" is one token and "'t" is another token.

We will talk about tokens in detail and show you some techniques later for how to tokenize. But do not worry, every NLP library has a tokenization function. You just feed the text and call the tokenization function. You need to know which tokenization technique the specific library uses, because there are many tokenization techniques, including more advanced ones.

In NLTK, we have a word tokenizer, a sentence tokenizer, and a regular expression tokenizer. These are all different techniques. For example, I can use NLTK's sentence tokenizer. If I have a block of text, the sentence tokenizer splits it into individual sentences: the first sentence, the second one, and the third. The tokenization technique is implemented internally and is a black box for us. For simple cases, they most probably use rule-based approaches. Some advanced tokenization techniques actually use neural networks, but the simpler techniques usually rely on regular expressions, cutting the text at specific delimiters.

In NLTK, we also have the word tokenizer. Here we convert the text to tokens using the word tokenizer. As you see, this is a token, this is a token, this is a token, and so on. There is a difference between tokens and words.

After converting to tokens, I perform some cleaning. I clean after tokenization. SpaCy has a very advanced tokenizer, and Hugging Face models use their own tokenization techniques. The transformer has its own tokenization technique as well. The key point is that the input for any NLP pipeline is a set of tokens. The preprocessing step that converts raw text into tokens is a very important step.

This is the difference between tokens and vocabulary. When I ask you to create a vocabulary, it means the set of unique words in your text. We need it for statistical techniques to convert words to numbers. So vocabulary and tokens are completely different concepts.

We also have an online application where you can feed your text and see the tokenization output. Here you can enter your text and see the results using different NLTK techniques. You can try different tokenizer techniques in NLTK to see the differences between them.

What is noise removal in the preprocessing pipeline? Removing numbers is one type of noise removal because, in some NLP applications, the text does not contain any meaningful numbers. We can remove them. This is not a standard step for every task; you need to consider whether numbers are important in your task or not. Commonly, numbers in text do not carry much meaning, so we can replace them with a space using regular expressions.

Removing punctuation is another type of noise removal. In the English language, we have many punctuation characters that do not carry meaning in most NLP preprocessing, so we can remove them. In NLTK, we can identify the set of punctuation for the English language. I can access the string punctuation property and print all the punctuation characters. In a very simple step, I can remove all punctuation and return the text without any punctuation characters. This is an important type of text cleaning.

The main objective here is cleaning the text so that we feed the model only the most relevant information. At the same time, we try to reduce the dimensionality of the text. If you feed the model text full of noise and unwanted characters, it increases the computational cost. So we clean to improve performance and reduce computational cost simultaneously.

Removing stop words is another important step. In any natural language, there are stop words that do not carry meaning. You may recall this from the previous lecture on Zipf's law: the frequency of a word in a text is inversely proportional to its rank. In any natural language, including English, many words appear very frequently without conveying important meaning, such as "the," "and," and "is." These are the top N most frequent words and constitute the set of stop words.

We can remove these stop words and identify them based on the specific language. If I want to print all the stop words in English, NLTK allows us to load them. I counted them: there are 179 stop words. We need to get rid of them. Their frequency is very high, yet they are not important to the actual context of the text. If you are doing document classification to identify whether a document is about news, sports, or fashion, does "the" help with this identification? No. It is extra and not relevant to the task. This is one of the preprocessing techniques: removing stop words. All NLP packages have this functionality.

How do we remove stop words? After identifying them, I search for all the words that are not stop words. The tokenizer runs first, and then I clean after tokenization. For example, "can't" is not one token; it becomes "can" and "not." I tokenize first, then clean after tokenization. We extract the set of stop words and simply iterate through the text, keeping only the words that are not in the stop word list. Look at the text after cleaning: only 40 words remain, all relevant to the meaning. The dimensionality is reduced, and the model receives only the most important words. This type of preprocessing also involves removing punctuation and converting to lowercase. Some books consider converting to lowercase as a normalization technique, but I consider it a type of noise removal.

In other applications, we need to keep emoticons and replace them with their word equivalents, especially in sentiment analysis, where emojis are very important. Preprocessing is both an art and a science. You have to understand your task. That is why, in all the NLP assignments, I mention cleaning your text and ask you to justify what type of cleaning you applied. Not every preprocessing step applies to every application or every text. In some applications, certain steps will not work. It depends on the application and the type of your data.

Noise removal in general: we can consider URLs in any text as noise. We need to get rid of them. We may consider social media handles or hashtags as a type of noise. It is easy to use regular expressions for this. Sometimes they are noise, but sometimes they are not. Compound words need to be handled as well: how we deal with compound words is part of preprocessing.

The general steps for noise removal: first, prepare your dictionary with the set of noise you want to remove. Then iterate through your text to remove it, like we did with stop words. We identified the set of stop words in English, then iterated through the text to keep anything that is not a stop word. This is how we handle any type of noise. For special characters, you prepare the set and iterate through the text to get rid of them.

Compound word extraction works the same way. Some words should come together, like "New York" or "machine learning." We follow the same steps: prepare your dictionary and then iterate through your text to keep these as one word. Usually, before preprocessing, we go through the text and read it to identify what type of preprocessing we need to apply. Look at your data first. Explore your data, just like in machine learning where the first step is to explore your data.

What is normalization? Normalization is the task of putting your text into a standard form. For example, if I have "love," "loves," "loved," and "running," these are variations of the same base words. In some applications, these variations do not convey additional meaning. "Run" and "running" are essentially the same. This is what we mean by normalization: we return each word to its base form. This is the objective of normalization, and it depends on your application.

In some sentiment analysis tasks, we need these variations because they convey meaning. But in applications like text classification, does it matter whether it is "run" or "running"? No. In document identification? It does not matter either. So normalization depends on your application. The core idea is to convert your words into their base forms by removing variations.

In NLP, we have two normalization techniques. The first is stemming. What is stemming? It is the simplest technique. It chops off the end of the word. Any "ing" at the end is removed. Any "ed" at the end is removed. Any "s" at the end is removed. This is stemming: getting the base form of any word. So "love," "loves," and "loved" are all converted to "love" by chopping the end of the word. Stemming uses a list of rules. Based on this list, if you find "ing" at the end, you remove it. If you find "ed" at the end, you remove it.

In NLTK, we have three stemming techniques. The Porter stemmer is the most common one. We have the Snowball stemmer. The difference between Snowball and Porter is that Snowball is built on top of Porter, works the same way, but can be applied to different languages. Porter only works with English, but Snowball supports multiple languages. And we have the Lancaster stemmer, which applies a more aggressive technique to cut the end of the word. Usually in an educational environment, we use Porter.

Understanding the application of stemming: even though it chops the end of the word, having just the base form is sufficient for classification tasks. In clustering and information retrieval, stemming is used in search engines where you enter a search query. The search engine uses this technique to match the base form and reduce dimensionality when searching its database. In these specific applications, losing the inflectional part of the word does not matter.

The other technique is lemmatization. In lemmatization, we also try to find the base form of the word, but in a different way. Lemmatization uses a dictionary of the language and requires an extra step. It reduces the word to its base form and then checks the dictionary to verify that the produced word is a valid word in the language. This is the key difference between stemming and lemmatization. Stemming just cuts the end of the word regardless of whether the result is valid. Lemmatization includes a check against a dictionary to verify that the resulting word exists in the language.

For example, stemming may produce a word that is not in the language. If you apply lemmatization, it checks against the dictionary. This is the basic difference between stemming and lemmatization.

Lemmatization is more accurate, so why would we ever use stemming? Because stemming is a lot faster. Lemmatization requires more computation. The choice between them depends on the trade-off. For example, in SpaCy, there is no stemming; SpaCy only offers lemmatization.

Choosing between lemmatization and stemming is a practical decision. What we try to do is check the performance of the model. Is the additional computational cost worth it? Adding a computational step takes time and storage and affects performance. If it significantly improves the performance of the model, then we accept the cost. If it does not, there is no reason to overload the system. This is the practical approach.

For lemmatization tools, we have different options in NLTK and SpaCy.

When do we not use lemmatization or stemming? In certain tasks, as I mentioned, the variation of a word is important. In morphological analysis, where I study the morphology of the language, I need the variations because they convey meaning. Computationally, if the benefit is not worth the cost, we skip it. In social media analysis, the variation of a word may convey meaning that indicates the emotion, behavior, or attitude of the person.
