For example, anything comes for what is the end ofiness? I local life, okay, so I will direct it to the student for anything from the department, a directed to the department, any other, I just directed to other fers. So we use ignor separation in our daily life and in so part of many, many systems. Okay? Right. So the main power of a regular expression come from its metacar. So in the regular separation, we have a list of metacorrupter, which have a specific meaning inside the rigoric separation, when you come in rigory separation. So in the first is blood. So that match any sinking character. Right, so to make it easier for you to imagine what it is, I give you, I usually have an homeline. Oh, make it. This is a online me of suppression. Usually, I recommend it my to get in a wel programming to test the z pattern using this online application and then, if they make sure that this pattern is correct and imagin their enexer correct as a pattern, then it can toate it back to your coat, okay? So, for example, here, what is the So, I have a text here that your language processing, then I can say a thought. So, what do you mean, my thought? It mat any same gale character. So I'm searching here for a v starting. This A, coloured one, any single party. Now, how many matches here? So you can find it yet. The interesting thing of this application, actually, the number of matches, highlighted, and also gives you here an interpretation and explanation of or why. So I like this application for this reason. So this is what we mean by means match any single character. The second,emble here is.. The square bracket. Let mean the square bracket is unponsored. So let me start with a question mark. So question mark, it means zero or more, okay. So if I replace that he ways, that question mark, So it's how many matches here? 288. It's approximately all the characters, because A, followed by.. zero or more. So even if it's A is not in the it's a pattern, it matches, okay? So the other sample here related to this is, I will go to plus. So I will show you that sl. So what do you need by plus? Match one or more character? Now, A, followed by plus means A, followed by. One or more. So how many matches here? Four, Three, four matches. So here, eight, one, two, three, four, because I have A, four, two types. Now, if I say B. 0, one, word more, there is no match in this case. So plus means one word more. That means any single corrupture and question mark means zero or one, one. Okay? Now, going back, we have the star. So what is the store is matches here or more correct. So here, if you go back and right here, instead of plus, I can say start 28, so every cur. Because it's one. It's zero or more corruptory. Okay? So this is a meaning, the basic meaning of the metadata in regular expression. Now I will go back to this one, which is a group of characters, the square product. So what do you mean by square bracket? I choose matches between any of these Pomineish. So anything to start has A or D or C. Find a match. This is match, okay? So this isn't like an option. So A, or B, or C. Now, if I use. So this is a match a list of the character. Now, we have a face. Okay, so at the beginning, based on the position, what is this character for? So if I go here and then I just not. Okay. So it's it's come into interpretation, actually. So this, it's come inside, the pattern, it means not A or P or C, okay? So here we can find the match which is not A or B or C any match. But if I remove it from inside and put it inside here No much. Outside means at the beginning. We should find A or D or C, okay? So, in the beginning of the sentence, A or B or C. Okay, so based on this rule, it's not there is no A or P or T at the beginning of the sentence. loads or world of the sentence, okay? And as equivalent to at the beginning, we may have.. What do we call it the end. So here, beginning, and we have an envelope that string. So here I can search for. I don't. We updated. Maybe I write slash dot. And then... Okay, so if I have not here, it will not match, because I'm searching for. This is not at the end, not at anywhere in my desk, where I have in the med, but there is no match, because this dollar side means we search for a specific better at the end of the best or the sentence. Right, so this is the meaning of meta character. So any questions if that I pneon, okay? So this is important to meet a character based in a regular expression, and then we have more metaarter here, we can use a group, part where is M&M. B I searching for an Oas with a specific minimum and the maximum, okay? So if I go back to my online application, so I am searching for A 2 to three times. There is no match, okay? So A, come together for two or three ties. So if I made some changes here, I add more A here, I can find one match. Okay? So this is a minimum, and this is a max. Another way, if I am here.. Now, I find it this match. So here, it's a minimum or maximum, or even, there's no need for minimum, and maximum, you can search for just the air for 3 types. Okay, so I'm searching for A repeated for S. So this is a meaning of this characters. Now, the other one is a scape character, if you remember that, because in regular Exorpation, I have character with a specific meaning, okay? Like don't. You remember when I try to search for adult in at the end of any sentence, I put a slash. So slash you, is a escape characters, so use this, not as a one of the meta character. I amm searching for adult, the actual adult, not dot related to the meta character, okay? So this is why this is why we have a flash when we search for a specific character, related to the metanata, then you can add slash to make sure differentiate between them. All right, so, and I have Orf, which is A orP. For example, you can choose between two character, two pattern, so even AA, or P, for example, so I have here, AA two times I don't have So it means four. And now the last one is a group. So there is a big difference between this one and the previous one here. So it remembers the previous one we have, it's an option. A or B or C, from A to Z or whatever. This is it's completely different. So if I choose A,.. Now it was at his normal match. What do you mean by this? So I have a search for a pattern inside your strang or your status, which APC come together. It should be come together as a group. So it's a considered like a yolette, okay? Too much. As there is any APC here? No, that's why it is not, it is more match. So if I make a little bit of change here, so I see for something. Now I have this match. Okay, so this means a drop, consider this as i, and it must clim to ger. in the same board. So if you switch is a border, it will not match. So for example, here, if I make it ACP, in this case, there is no match. Okay? Is that key? Any question? Okay? I'd. Now some special character classes here slash/ small S is completely different from slash. So the negation of each slash character is has a different meaning. So slash small Sens match any white space. Okay, so if I write here slashs small sash. So it match all the wider space, okay? Why, if I just did make it a capital S, it's aation. Match any nonw space. Oh. Okay? So this is the difference between the small letter and the capital letter for. This is specific characters. Now, the same force slash that is that match any alphabetic character, slashita W means non alphabetic character. So here, if I just to make it the stap you, non alphabetical character, or alphabetical characters, so but if I make it, non alphabetical character, includ, the same four/D. So slash match any from 0 to 9 slash cutters match any. So if I just add here one, to three, so slash match 1 to three, but if I just need it cabinet D, it it would match any non desit character. So the negation here between the capital and industry. Okay? All right. So, go back. I mentionedication. All right, so, in your Rob actually, this part, I'd like it in your book, and that's why I put it in there required reading in chapter 2. They explain very goodation in a fantastic way, and also they have they provider who is a lot of examples. These examples come from your textbook. So, yeah, just practice of what we just did we've heard now, for, okay, so this is optional, so if it is small d or W, it will match, so it match all the water to start a W or small W, fling one to nine, so it match any digit or we can write it in a simple form from 8 to Z. So any digit from starting from capital A to. Okay? And here from B or starting with mat is A, from small A to a small bed, and his lower case from any capital call in instead of I think like this, I can say from zero to light. So any digit from zero to. So it gives you, more sim way to write your regular expression. And this is example of the metac characterter that has three metacorter, you can go through it and it's easy to same like what you did. All right, so negation, this is an example of anegation. When you have a head inside. The factor it means angation. But when you have have a public side, in the bottom, it means as beginning. So it it sh should be magic at the beginning of the sentence. The same four dollar sign we match at the end. All right, so this is a link for the applications that I use. It's a good to practice with it. Again, it gives you also not the matching, but it gives you an interpretation. while it matching and delays on ritual. So it's a very good tool to work on it. in our work programming, I ask for money students when they create a batch, a pattern, I use an airport, I pastor their battery. So once they tested a p and everything is fine, they can move into the in a demry. Okay? Any question for a research? Is that clear? All right? Okay, in Bison, we have a function that won't in a expression, like here some function we call RE So, RE it's bison diary that work in a regard expression, impro us is a fastic function that you can improve it in your c. The first one is match. Match you take two parameters. The first a parameter is the patter, you search a core, and the second parameter, that takes all the string. Okay? It's returned, the opt that match a regular exemation at the start of the string Okay? W search, when you take the same parameter, but theput this. So here it return the match anywhere in your strength. So for this. I can show you the difference in the coat,. Some LK for actually for every election, I have mual in plus Portland demonstrate the concert. Usually after the election, you will post it in the bright speed. So all these qual be posted in the brightesmiths. Okay? So, some impression, okay? So this is a regular expression. where I have a c that show you the difference between search, match and the fine dog. So here I have a string, and I'm searching for one, two, three, okay? Inside, this is string. And I repeat the same, but using different function. Search, match, final. You can see here the output is completely different. So match one to three, it's return. What is sorry, search. So I find a search and they gave me the start and the end of the endex where you find the search inside the street. So I find the match, which is one to three, starting from index 42, and end at the Nex 45. This is a ri of the output of the search. Okay? Why match, return is here, none. There is no match. Why? Because match is search. Beg. Beginning of the string. If there is no match, it returning out here. Why search, it search anywhere in the string, and it returns the fair is occurrence. So even if you have multiple of appearance of the the pattern, it just return in the hairstristoc case. Why and find all return all, searchwhere, and return the list of all. Okay, so this p shall show you what is the main difference between search, match and the find. It's a difference is clear? Any question? Okay? So this is a function, we use it in the pison RB. So for writing this code, I have to import RE at the beginnings. So I importing RE, which is the regular execation, I in Python. Okay. And I have another function here. Sup, so you can return a sub straight based on the matching pattern. and we have a very important function here. we call it compi. So the idea of the compile is you can killate a patter, and then compile it. compile after the output of the combined function is you generate and object. So you can use this object later all in your port. So when you compile a specific pattern, the output of the compile a generator option. So later on in your p, you can reuse this object in your port, okay? I can't remember if I have something of fine here in my coat, you know? check.. It was like 60 something before... Okay, you can try to, okay?. Okay, so also we have a move, so we can return this match it as a subgroup and we have a span when we return the match it at the starting and the end of the matching in your p, okay? So, Room and Span have a different interpretation here. If you vote two here and let's see what is the difference between them. Find all I find the all. it's a lens of merch, so what this is some brain's phone has a globe. So I'm searching for a phone and then I'll return the output as a group, so it will return me here as a group, as a search will not return the actual data, it returning the spam. The first is at the start of the text. So here when I find all it g me a list. So here I can go the alopput spam here starting from. This is output of the search usually, but here I present him as a group, so this is the starting of the group and this is the end of the end. And also I can spend I think here I use a span, okay, match span, so for all the matches, find itator. It's like find all, but it return an irable object. So you can iterate through this object. Okay? This is a fine iterate, okay? Inside the fine iteratum, I can use here a group where I can return where is the start at the end for each match. So based on this, find the tum for a specific pattern, I can use a spell, where it's a print of possession of all the matching element. So this is the first match to start here and the end here. This is the second match, start here and the end the ear hand. Okay? This is the main function that we use it in regular separation from RE library in Fais, but there is more. It comes with your experience to find more functioning. We have also split where you can split your pattern based on a specific dimilit, okay? So here I split is on a space. So it returns all the splic strength based on the space. It helped when you have that power, when you have a textifying, and you want to return the list of vocabler inside your of force. We all know that all the world is separated by space. So if you apply this, you will have the list of vocabulary or the wards inside your textestival. Okay? Right, so this is a second war function, a regular expression. Why we study regular expression as you will see in your laptop that we can create. How can we can perform a very simple NLT application, only using regular exation. So here we can use it for testic lean. So for example, if you have a t and there is a lot of unwanted character inside this text, we just can search for matching of this character, and we can replace it with space or whatever, okay? So many uses of available separation inextically, when you try to remove unwant the character. So if you have a sentiment and an anestase, and you want to get right to all the emotions, or if you have an H ML file, and you want to go right of all the texts, you, and extract only the text. We can use a regular accemation. It's a powerful or all this case. Information retrieve. Okay? So this is an example of information retrieve. We need to accept some information that match our calculated pod from a huge purpose for a huge document. So it has many, manyPN information retri, sentimental analysis, okay? I'm searching for a specific port to indicate this sentiment is positive or negative or neutral, which, the application of this, I think I included in Lo, Sou, you will create a sentimental analysis system. based is the regular ex separation. I give you an, a huge, so it's divided into a different sentence, and in each sentence, you can cover it, you have a dictionary of a good word and a bad word, to indicate this sentiment is good or bad or neutral. So you extract the good word, search for how many matches of a good work, and how many matches of that bad work, and incomete. Very Spen worked very accurate, of course, but it's a very simple. So if the sentence compare more good word than the bad word, so we can indicate this sentiment analysis as possible. So the opposite is if they are equal, put what and equal the bad word in the same sentence, it means it is a. Very, not accurate, but this is a first to try, how to create a sentimental an. And you will find an application p in your laptop. Lando detection? If given a document, I can search it for a specific pattern. unique for this language, so I can detect. This is language, is it an English language? This is French language, this is an Arabic language, or so for for a specific better unique to this language. So, this is not all the application of regular acceptation, but this is a part of application of pregnant expression incane. Okay, any question? All right, so. uh, this is some class activity where you can check your knowledge in the regard expression, so I ask you here to check to find a match of all if there is URL exist. How you can aim the ble. How are you, the kid? This is the URL, not you. This sentence, how many URLing? Give me an eye. Very, very, yes. Probably can find can be starting with so any sentence starting with HTP So I'm searching for any HTTP or HTTB S, because I have to take for HTTB, okay? So some we use HTP, which is an old version of.. It's a hypertist., yes. And we have HTPS, which is more advancancing and a secure hypertics.. Okay. So we search, it's very sent me to pattern, I I give it for you as an example, so given a test long as possible suffering that are rer value. So we have the rule for the proper valuable, so each variable should start with. or a small letter. So can we start any variable with susp character? No. No, it's a leg, so hidden your mind, what is that legal rule for variable, and then you can create your budget. You can find in in classhood at the end a solution for this group activity can test your knowledge on this. Okay, now.. N NP is the development to motorcy. Does anyone here has some experience of software engineering? So it's is seminar too. Ner two? What is a common technnique of building any application in a software engineering? We have two techniques? The waterfallers? Waterfallow, perfect. And the other one? A, okay? So this is a se to water flu, a step that how we p any system in a software engineering the v provided as a lif cycle. We call it a life cycle for a waterflow model to create any application, and MMP also, we have this development life cycle. I modify this on my previous knowledge, but it's contained this again steps by flying means steps also. So we start with the data collection. any of the applications should start with a collection of things. And this collection should be huge. You will see later poll as more as your gazard, as more your performance of your application, because you give your more than more information to live. So like in, yes. Does it make a difference if? If we? If we have aatabase inside the data versus the metadata about the dataation to extract information about. For which application for any application? If you have a table, we need find that we have specific the table, but the met we use that to find difference between the name and the type of. I'm not very familiar with the dabase, actually a s. Can you tell me what's a difference did we play? I cannot understand the table in the database. But what do you mean by the database? when you at the colums of the, what you first is another one. I think we can. I think we can use it. So, because you' can expect what information we need. Okay? For any. For example, you need to catch for now, for a basic knowledge. We only add exemation as a technique. I didn't use any machine learning or deep learning technique. But with a regular expression, you can extract all this things. Yes, for methit, okay? It's a type of document. What this documentents, okay? Right, so data collection for this is starting point for any application, b. All right? And this data should be huge. So if you'ology to learn more and the way. And also at the same time, it should be irrevant to your thust. Okay, analysis, you can gather some tweets, all right? So for any scientific application, you have to measure a scientific argument. So the data collection is a starting point, and it's a very important point, and the two conditions, it should be enough, it should be huge, and it should be relevant to your task. The second step actually is a deep text cleaning, just a good ride off any noise in your data. For example, in your machine learning, what is the first step? I don't understand. Before analysis, what's the first system? Pre processcessing. What happened in the pre process? What do you do in the prep process? There is many techniques. What is the comment techniques? What do you do? Remove the application. This is a type of noise. Any other technique.. Right, replace the missing data or delete it. It's based on youression. Maybe your search for all missing data, you may choose to delete all this record or replace it with. maybe average or medium. There is many techniques to replace the data, yes? So let's. Sorry? So that's features. selective feature is in a feature system, which is feature engineering, okay? F. Okay, change for the format of the data, some data come in a particular form, some data come in that number form, so unify the form of the data is one of the processing thing. So even, yes.. Yes, Okay, so theity reduction get right of air relev data. So if you have a column which not included in your processing, like IG, for example. So why I have to include IG in my date, okay? So all this in techniques, but not the same, will be applied in an NP. Sometimes I have I want to remove the noise of my de. Some character is irrelevant. I just clean this data from irrelevant. Sometimes I have the same word, but sometimes it comes with a capital letter and a small letter. So I have to lower case. This is the type of the prep processing and dec cleaning. If you extract that HTML files, we want to get right off all the opening and the closing attack. So I need only the text. This is time of cleaning. So cleaning the data, as you do a natural language of processing, in MLP, or sorry, a machine nicus, is a very, very important, and it's a huge effect in the performance of your model, okay? You remember garbage in, garbage? Oh, so if you enter El.K for un val today, don't expect that will be cooked, okay? So this part of any task, is a time consuming, it takes a long time, but at the same time, it's a very, very important, okay? So, I divide it into two.. In a minute. into two parts here, cleaning, and then that is to prep processing. So after I cleaning, I remove all the noise and I have many types of noise. I come with a preprocessing. Prerocessing here, it means we. We standardarize. Your input through the matter. So we have a huge number of document in a different format. I want to create a standard form for all this end. Okay, I'll talk about this in details, and then we go to the future extraction or feature engineering face, where we can choose some feature career our feature table, like in your machine learninging, then it comes to moderning, apply the modern techque. And then, ifuation, evuation, modern, and then deployment, what's your model in production environment? This is important, and even will not even put your model in in production environment, it may be may need to integorate this modern inner production environment with another system. So all this is a part of their deployment, and then monitor. which is a very important step. will not build the system and they make it run for whole life. Monitor to make sure that the performance will not drift from eight spells, you know, remember? And I think of course, modern draft. So sometimes I the modern work fine now, and after time it directs from as introduce somebody bias or whatever, or even the performance degraded. And think about the sentimental analysis, girl. 10 years ago. It would be open with a warpo with the same performance snap? No. No, of course, because a new terminology introduced a new language introduced many ways, so we have to monitor our system to make sure that it work as. And this is not a linear process. As you will see, there is here an evaluation, things we may find some evaluation is not good. So what we can do in this case, we can go back, who maybe go back to prep process or go back and to collect more. So it's not a linear process and to go in ast direction. And the gave it here in monitoring, we need to add more data to keep the modern work as we expected, okay? So this is not a linear process. Now, please look at this model. Do you think is there is something messing in this spipeline or in this steps? understand the plan? Sorry. Bef before you correct data, you didn't understand. Understand. So. So this is what I added, actually. Thiseline is out of this, it from. I write the source here. Okay, it comes from a specific book, but I add it actually, this and also in this on my experience in software engineering, I added this part. You have to understand this approp at the beginning. You not go at the collective. Sometimes, maybe this task. N LED, it's not about a choice for this task, okay? So understand is a problem. Gers requirement from the user if you have a client and a client wanted to create a system, gathering information from the user and understanding your problem. It's a basic step to decide. If you've always had any LB, okay, which tusk, you perform a classification, you perform a classering, what type of task you perform to solve with this problem? Or it may be any helipable the choice for you. We have another symbol solution for this problem. So, in my opinion, God requirement to gazaline is a very, very important step before. You started to collect your date, okay? So in this lecture, I will focus actually on this two steps, which is text cleaning and the prep process. And we will go to the other steps in upcoming village, okay? Any question here? Is that clear? I All right, so what's the motivation? For the and prerocessing, as we discussed it before, garbage in, carbage out, we need a clean text, remove all redundancy, or are relevant corruptors or are relevant information to improves a performance of our body. So we need a clean and the standard take data, sweet for our p, okay, which in LP does. We need to convert the data in a standard form, usually, when we gather information, we got an information from different resources. BDF, textify, ph resources. Now, we need a pre processing step two uniform, all this, okay? We have a standard form. is that different resources. Different formats, we need to have a dig that is standard, okay? So this is an important task, and this task done during the reprocess, okay? And also this is an improve that performance. When you have a cle input and the standard form, everything is fine, it will, of course, improve your performance of your model. So this is our motivation, why we need, or why we need to focus and display to time in the people that says he was that. Okay? Right, so take it to reprocessing pine pine, I have to mention that, this is my opinion. So it may be different. Some people or some folks that combine normalization with noise energy to remover, or maybe divide it into more different step. But in my opinion, this is the main steps to reprocess your application, okay? Sometimes it may be well, based on your task. So, maybe you can perform a noise remover before organization, or you can make a normalization before token noise remover. Okay? It's a bit on your task, but normally, in most of MNP application, this is the logical steps in a people process. So you enter a document, this is a source of undocuments, this is your data. The first step in a free processing is to make toization. Okay? Which is an input for any MLP model. So if you have some experience with anLP model, even the transformer, the input of the transformer is a to. The technique that produce a token for each model may be different, okay? But now I will talk about the old technique or the phsticnique. But later on, when we come to transformer, we have more, and the more toization decrease, okay? But toization is more first system. The second step in our whiteeline is noise entity removement, any noise in your system, and then we come to a normalization. People going and talking about what is a organization, I will try to look a specific terminology in and enity, okay? So the first is melting kip block of any language. M mean by welding blocks of any language. Any fundamental, you, that makes any natural language. Okay, so in any natural language, it consists of a specific fundamental flux. Now, our example is english. So look at the English. If you consider an English as we have this block, for needs. What is a four names in in English language? Small yon of? Salad. Perfect. That consists of that make up our l. Okay? Does it have any meaning? No meaning. It just a small unique of some. Well, it's combined together, it have anyique. Okay? In English, we have a 44 different volumes, which is a basic ofony in our in English language. Then, in a top of this, we have amorphees or lycimes. What is a morphmes under lyximes? A small unit off. Language that has a me. So here. as you see here, tampling we divide it into a set of morphines. All right? So this is a smallest unit of language, not a sound. It is a language, right? Then in a top of wolfphins, we have the syntax. What is the syntax? Have the mortus and range it according to, or a set of grammar. So the structure of the sentence, how we paint is the structure of any sentence in language. It's a set of rules. We call it grammar rules, okay? And then in a top, we have the context, which have a couple of sentence or a paragraph convey a specific meaning to us. This is a building clock of any language in our case is an English language. And each clock has a specific application in an enemy. You remember in a previous lction when you talk a speech, recognition, system, phenomes, a player, important group? When we talk, we divide the signal into.f, sounds, and then we combine the sound to have a word. So it has an application, in a speech, speaker identification, tickets to speech, okay? And even this promot, it work, we have an application into organization. When we divide our take into chunk of tokens, which is different from words, Okay? So we need to lookize for many, many applications for the embedding, as we will talk later on. Part of speech tagging, when we identify the part of speech in any sentence, many, many application. The sentence, the grammar't if the sentence is correctly according to the grammar rule of this language. And it has many, many application in morphological analysis, and even to improve or to test the informance of some application, maybe by translation. So when you translate, check if this is the translation is correct according to the grammar rule of the language or not. So a play and very important rule in such application. and then the context is, of course, to understand what's the context, to improve the ambiguity of any language. So for each plot, we need it, and we apply it in a different NAP. Okay? So this is a piece of information that we need to know. Another important information. It's a common in NLAP. We hear about corps. What is a corpse? Your data. It's a collection of documentation. Okay? So the corpus is consistes of many documents from anywhere, from different time. So this is what' when you refer to the purpose skin and LP. And we have awards, small limit of language that have a meaning in any language. This is old. Usually we can identify the world in English language separated by. space, okay? Any questions now? What, right? I find one that't I. Okay. Now, what is a organization? Talization very simply is a divide, your sentence to a set of tokens. not words. I will not divide the sentence based on a space. Tken is a small unit of language. So deponization technique is different. We have many, many organization techniques, spling the sentence space on a space, maybe one of that organization technique, an early organization technique. Help with heRL ways text, one by one, toens. Tens again play a very important rule in all NP application, even the moder NLP application. Have to cut our shop, your take and a small set of topics, and there is a difference between tokens and the foc. Yes.. So, I'ing my head. Yeah, so, okay, vocabulary? Yes. There' a question Previous one? one? Yeah. So we have the pieces, and then the same. Yes.. Oh, okay, because I generated by the Okay, okay, semicins, one of the top. Sam Collins, one of the any special character, in your ticket is considered as a topic. That's why this is a difference between a morbed and toics, yes. So he ignores the space, white space? It is not the base that you cut or generate a talking for your text. Okay. Okay, so if you want to generate a vocabulary, I cut this on the space. This is a vocab, a set of unique wall in my text, but token is different. So, some words like, who joined with the hyphen or? So I give you. So for example.. This is a wt, okay? But if you if you use a token, Ken is a token, and is another token. Okay? We'll talk about tokens in details and we show you some techniques later on on how to toonize. But don't worry about organization in every MLET library, they have aization function, just the feed tickets and ask for a toization. That's it. You need to know which organization technique. This is specific library use. Okay? Because there's many, many touization technique, and the more advanced toganization technique, okay? So this is what you need to know. I'm a wise we just call the function, and as you can see now, just to produce it. I will show you here the difference. in a practical example.. I'm Okay, so in MTK, I we use any MTK. So N NTK here, we have awarded organizer, and we have a sentence toonizer. and we have a regular experation tanister. So this is all a different technique, yes. Sorry? This is the one., okay, so you can leave that what? This is an in class coat that in every lecture I have in class coat, and I will publish, but after the lecture, in the same week. So you will see in plus code for week two, and for each week, when I present some code here, I will publish, but after the election, not before the lure, okay? So don't worry, I will publish the same code in the Pri space after the l. Right? So here, I can use an LT scent organization. If I had this sentence or this text, I can use, I can use a sentence stchilizer. And then it just to produce this is a first sentence, and this is the second one, and this is the circuit. Have the implemented which techniques you use, this is headed for us. It's a block, box, okay? So you can spend more time to understand which techniques to use, mostly probably they used. For a simple cases, some advanced organization techniques actually they use a neural network. This is advancing technique, but the same techniques, or any techniques, usually rely on alligulic separation. Okay? So cut the word in a specific to. And here, in MTK also, we have... Okay, so here we convert with the tokens, so token word the toonizer. Just it's converted to token. As you see here, this is a token, this is a token, this is a token and so. So there is a difference between tokens and the word. So here, I think I have an example of difference here between between a w saw here, I rem moves award to chase. I, so we can. This is a type of demistic cleaning. I clean after I contracted to talking, Iformed some cleaning. I will talk about the designniques cleaning in a few minutes, but this is how to convert to. Spacey have a token is that very advanced organization to keep, and have toK use organization technique given the transformer has its own organization technique, okay? But this is what we need by topic. How we feed? Because this is an airport from any LVP pass. toets, set of toets. So the pre processing step is to convert the prototist into toets. This is a very important step. All right, so this is the difference between tokens and the vocabulary. When I ask you to create a vocabulary of your vocabulary means the set of none, a set of unique words and your text, and we need it any statistical, technique to convert numbers, a set of photocas, a set of unique walls inside your desk, so it's completely different between topics. Alright, so also in we have an online application that you can feed your tech and then you can see the output, the documentization output of these teches. I found it actually just days ago. So here you can enter your T and then nice using NFTK to technique using an FTK. So I tried why just a puppy pest. Take this one. I see and defeat it as Then T eyes, Okay, a little bit. Don't tell me that. Okay, so this is the tok. So a divided sentiment by sentence. So this is a set of tokens of different lea. So why is poconizer, word apartment to Dereizer techniques in LFTK. Okay? So if you like to play Wednesday, you can go and check this organizer. to get a difference between them. No energy over. It's far off there. No, free processing pipeline. What is the energyG noise remo, what? Removing a capital letters? Removing capital letter, actually, and converting text into lower case, I think it's a part of the normalization, okay? Not entity, removal, but it's a part of the repcess. So I have the same word, but this world has started with the capital deer and this word, not a. Do you think there is a difference between two words? No. No. Okay? So in a processing, there's no difference between this one. So converting all the tech is to our case, a type of standardze your empput to your NPos, okay? Another MPT entityo here remove a capita letters, removing numbers, suck in some applications, full application. N doesn't contain any. Okay? So we can remove this number. Again, just remove the standard steps, not for any rep processing you' to remove a number. Think about the dusk is the number is important in her task or no? But commonly, the number in a ticket cousin has any. That's why we can just replace as we leave, you can use aular expression to replace any character, any numbers to space. and removing punctuation, what is a punctuation in English language, we have many, many punctuation, letters? It doesn't have any meaning in rep processing. So we can remove this punctuation characters. And we have in MMTK package, we can identify the set of punctuation for English language, for example. So can see here. So I can just explain strength to punctuation, and bring to all the punctuation character in an English language. Okay? So this is a set of punctuation and in very simple step, I can remove all this punctuation and return. The tick was out with any punctuation corruptor. Okay? And this is a time of important cleaning of your text. The main objective here. is what? Yes, cleaning the text. It's a step that we feed the modern was a most relevant information, okay? But at the same time, we try to radios. There dimensionality of your text. If you feed that text with all these mous and all these unwanted character, it. It takes a computation cost of your application. So we clean to improve the performance, and at the same time, we're trying to reduce the computation cost of our computation, okay? So here, it returns. the same digest and just ignoring all the bbituation. Okay? So this is a type of flies. Removing stopboard in any natural lunch, we have some stopboard, which doesn't gain. You have this experience in that one. You remember love one? J through? What is Z? one? The inverse of the. The prevalence of a word and a text is the inverse of its rank. Yeah, the frequency of the words, and very, relation went down. It's rank, okay? You may find in any natural language, even in the English in your case, many, many words't use the frequantry in your text. Okay? Was without any important feeling, like what? There? Um. And your experiment. You see that. It's the top. N, N. So this is a set of the stop world that it's not convey any meaning in your text. So we can remove this stepboard and we can identify this step word based on your specific language. So here. Alright, so if I want to print all the top words in English language, in LTK, we have we can step forward of English law, we can print all the set of the soft word. I count it. It is98. Stopboard. So we need to get right of this stopboard. It's frequently, it's a frequency. It's very high while it is not important to the actual context of the text. If you make a document identification, this document is, is new, it's a sport or fashion or whatever. Do you think that's a support by this meeting? No. So it's an extraernal, and it's not an important relevant to myitos. So this is one of the free processing technique is to remove theboard and all the MNP packages has this functionality, how to remove the stop. So how to remove the So after identify the stopboard, I just search all the word that it's not a stop. Yes. I see the word. I't. Before for here? and there's no. But it is like a. I can come over. Because I only one. Oh, here. You have three tons. W, N poss.. This is you talk about this set?. There's repeated words, but a very, like people. I. Yeah, but it's considered that as a slip. It. Yes. Because I cries at the beginning.. This is like at my example. cannot. It's not. It's a can and it's not. So it's divided into. So the step I took knives, and then I clean after toization. Okay? So we can extract all the set of stopboarding and then very sly, just every day and hold the word that is not in the stop word. Look at the texture. Only 40 w, which is relevant to mine after cleaning, okay? See you have the missionary is reduced and at the same time, I feel my more is the most important word, okay? So this type of the prep processcessing as set.oving punctuation, perverting to lowercase. Sometimes he consider lower lowering removes the capital letter. It's one of the normalization techniques. Some books consider this, but I consider it as an model.
Text. It's one type of the noise. But in other applications, we need to keep emotions, and replace it with the word equivalent to it in sentimental analysis. Emojis is very important, okay? So, again, at preprocessing, it is an art and science. You have to understand your trust. That's why, in all, you know, LLP and all the assignments. I just mentioned that killing your text. And I ask you to give me your decision. What type of cleaning you apply in your text? Again, it's a science and art. It doesn't mean that for any application, or for any tickets, I apply all this pre processing estate. No, in some application, it will not work. Okay? So again, it's based on application, and it's based on the type of your data. All right, but this is how you can do it. Uh, Phil, any other questions? Remind me when it's 12:30, please. Sometimes at home, I don't think, because I think you... Your quiz. Because I said, That's why I don't want to lose it. Yeah, come on. Once you do accommodation, then you can... It should be safety based on our previous discussion, because we will not take a break. Okay. So, I will change it when it comes here. So, I give it to this... It should be certain... It's, like, very old... It's... Wow. Okay. Now, okay, so remind me of to come to searching. All right, so... Okay, nice entities. Again, it's based on application. We can consider a link. A URL in any tickets. It's a house. We need to get right of it. We may consider social handages, hashtag for head. It's a type of noise. So, it's easy to use a regular expression, too. Sometimes it's noise, but sometimes it's not, okay? Compatent. We need to accomplish it, we need to divide it. So how we can deal with the compound temp. This is somehow we need for peer processing. Nice, overgeneral step, so it's based on your tech and visual application, but we go through a three specific, definite steps. The first step is to prepare your dictionary with set of noise you want to report, okay? And then it rates through your text to remove it, like we did, in the stopboard. We identify the set of the stop ward in English, then we emerge through the text to remove. To keep, or remove, stop ward, or keep, anything is not a stop ward. This is how we can, for any type of knots. So, even if you let a go, this... another special character. As nice, what you can do is just prepare this set, and interact you on the tech is advocate right for this. This is the general steps. Combound the board, extraction, so I think if I have here, an example of how to extract the component work, it's the same idea. Okay, so, dealing with a compound. Some words should become to each us, New York, machine learn, okay? Again, we will follow the same step. Prepare your dictionary, and then it rights through your test, and tell you, just keep this test, uh, one word, okay? So this is a general technique to pre process your text, please, on the text. So, usually, we not have a thickest, and then, we start to talk process. We usually go through the thickest and read the text to identify what type of preprocessing we need to apply. So, hunt just the database, look at your data. Explore your data. Like in machine learning. The first step is to make an exploring your data. Yes? How many classes, how many different classes, how many... And this is a conveys of information for you, okay? So this is what we have to do in here. Now what is a normalization? Normalization, it's a task to put your tent is in a standard for. All right? For example, if I have love, love, it is a good, yes? Maybe I have inside, the thing is running. Okay? It's a variation. form of the same width, right? In some application, this variation doesn't convey any meat. I have a round pair, as it is. So this is what we mean by normalization. We need to retail each word to its base in full. This is the objective of the normalization. Again, it's based on your application. If we need a variant form, at the same word or no, In some sentimental analysis, we need this variation. It's convey some meaning. But in some application, take the classification. Does it matter? Run for running? No. Document identification. It doesn't matter, run for money? No. So, again, it's based on your application. But this is the idea of normalization. To convert your focus into its base, any valuation, just removed, okay? And LAP, we have two techniques. The first one is a steam. What is the stemming? The stemming is an easiest technique. It just shot out the end of that world. Any, any, at the end, remove it, any E, at the end, remove it, any S, at the end, remove it. Okay? This is a stem. To have the basic form of any what. Right? So, nothing, love, loves, love, all converted to, love, shopping, the end of the wall. So, it has a list. And then, based on this list, if you find, at the end, if you find, at the end, EG, remove each, and you find, at the end, I energy, just remove. This is the standard. Okay? And editing there, we have three techniques here. They use three techniques, ported is a common one. It's a flexible one, and we have the snowball, the difference between snowball and the portal. Snowball is a band in the top of the border, it is the same way, but snowball, can be applied in a different language. So, that's, this is the improvement in a spoil, okay? Portal, only work with an English language, but snowball supported from natural language, okay? And we have Manchester Steamer, which is applied a more aggressive technique to cut the end of the market. Again, to reduce a dimension, okay? But usually, we use a pottery, yes. Between, portal and snowball. Okay, snowboard, it's the same by portal, but... it supports multiple length. Also, only work with English language. But snowball supports multiple language, okay? And Lancaster is... use a more aggressive technique to cut the end of the world, okay? But usually, we use a portal in an educational environment, we use support, okay? Standing the application? Okay, why it shocked the end of the world, it doesn't matter in classification task, to just have the base form of the world, in clustering, and also eat information, either, which is actually used in a search engine, whereas I use an interquery, that the search engine use this technique to match the pace form, and reduce the dimensionality when searching in its database. So, it doesn't matter. In this specific application, if we stay with systemic, that part of the world, which is important, and doesn't matter in this place, but usually used in this technical application. Another technique is, laminization, laminization, laminization, and, again, we try to find the base form of the word, but in a different way. What is a different way? Latinization, come with... Let's say that the channel of the language. So, levitization needs an extra step. It cuts the end of the world, but it goes to the dictionary at the beginning to check if the produced world is a part of the language or not. This is a difference between it. and climateization. Okay? It is standing just to cut the end of the world. It's part of the language, always, not part of the language, doesn't matter. But levatization, it's coming with the check, in a dictionary, in a coordinate dictionary, in English, that is this word after cutting the end of the word, it's English water. It's related to the language woman, okay? So, for example, you can see here the left. Okay, okay, I think the difference here is working. So if you have... this word, the stemming, we just... Cut, the end... S, E, S, if you find any, S, this is a part of the rules. Study energy. This is some wonderful French, but here it comes with the word, not in the language. But if you apply limitization, it's a terrorist touch. This is the basic difference between stemming and lemonization. Exactly. Okay? Now, yes, limitization is perfect. Why we have to use this thing? By a lot, faster. Computation power. More computation. Levitization is perfect, but it's the end more. Computation step. Use it or not use it. Like zombie in a space, there is no standing in a space. The only limitization. Okay? So, using methodization or using the spicy, the plastic and weight, or nemitization, or stemming the practical, impractical natural. What we try to do is to check the performance of your model. Is it worth it or not worth it? I added a computational stick, okay? Take from my time, take from that. Storage, take the performance. If I did this effect, it affected this. So it's a trade. Okay? If it's high effected. The performance of my model, so, okay, we have to take the cost. If it is not. Why I have to overload my system with this system? Okay? This is a practical help, all right? All right, so, limitization tool, we have, in a space here, ATK. This is a different tool, okay, medicalization. But, again, let me check, I'll give you an example here. 328. Yes, I just, okay. All right, so I, I'm just convinced to finish only this part. When we not use a limitation, or stemming, or a specific task, as I mentioned, some task, I don't, I need the variation of the same world. And we're fourishing analysis. I study some morphology of that, and language, so I need a variation. It conveys some meaning for me, okay? And computation, of course, as I mentioned, if it's not worth it, and in a social media, maybe the variation of the same word conveys some meaning to indicate the emotion or the behaviour or attitude of the person. So I will stop here, and...