During this lecture, we will talk about feature representation. We will talk about common techniques: one hot encoding, bag of words, bag of n-grams, and we will talk about similarity, lexical similarity. This is the same slides from the previous lecture, where we have the NLP pipeline. In the last lecture, we talked about text cleaning and preprocessing, how to clean our text to get it ready for the next step, which is the most important step, even in machine learning, which is feature engineering.

What do we mean by feature engineering? What is feature engineering? It is deriving new attributes to create and represent your object as a set of features. Is that important? Is it as important as preprocessing, or less important than preprocessing? Equally important. Both of them, I think, come under data preparation. We prepare our data for the most important part, which is the modeling. Feature engineering in machine learning, and in natural language processing, is a very important step. You may hear about Kaggle. They publish some data, and then people compete to have a good result. One feature, only one feature, may be used and allow them to win the contest. Just a new feature. So this is important, it plays an important role in the modeling, the result of your model. The feature engineering is a very, very important step. During this lecture, and the next lecture, we will talk about how we create features from our documents or text in our course.

So, how about feature representation? We know in machine learning how to represent our data. Your data in machine learning is numerical, which a computer is able to understand. But what about unstructured types of data? Like images, which are a type of unstructured data. Speech is a type of unstructured data. Text is unstructured data. Now, how does the computer represent this unstructured data? Let us take an example about images, how it represents the image. The computer processes images as a matrix. So, for any unstructured data, we need a mathematical model to represent it. The mathematical model for images is the matrix form. Each value in the matrix represents intensity. So, in terms of gray and white, it is based on your processing. But each value in the matrix represents a value. That is a feature vector, and once we have created this feature vector, we can now enter this feature vector into any model, and then we have predictions for whatever we can do with this. So we need a mathematical model. In image representation, our mathematical model is matrix form.

Now, what about speech? What is a mathematical model to represent speech, the voice? The waveform. Waves, where we have an amplitude. So the amplitude of this wave represents a feature of this sound. This is a mathematical model. Fourier transform and other transforms in the waveform automatically decompose them to represent different waves.

And now we come to text. This is unstructured, and we need a way to represent it. What mathematical model can we use? We can classify the representation techniques. The first one is frequency based, based on how many times a word appears in the text. The more advanced one is what we call word embedding. We will cover it next lecture, including Word2Vec, GloVe, and FastText. And the more advanced one, which we call the transformer based technique, includes how the transformer and large language models represent text. In this lecture, we will focus on the early techniques, and then in the next lecture, we will focus on the embedding techniques, where we actually use neural networks.

Representation based on frequency. The idea comes from statistics and math. You know, computer science is a relatively new field. All the early people in computer science had a statistical background or mathematical background. After that, many people graduated as computer scientists. But before, when they started working in computer science and anything related to it, they came from a mathematical or statistical background. So the early techniques always rely on statistical or mathematical techniques. Like machine learning, for example. What is the early technique in machine learning? Regression. What is regression? Predicting continuous values. It is a statistical technique. If you study statistics, regression is statistics. So all these models come from statistical roots. That is why this part relies on statistics. But with new terminology, they explored how we can use these techniques to represent our text.

So in this lecture, I will focus on frequency based techniques to represent text. What is the idea of representation of the data? The idea is, at the end, we need to use modeling techniques, normal modeling techniques. In machine learning, we have to prepare a table where we have instances, and then we have a set of features represented as numbers. At the end, we come up with the same representation but for text. This is our goal: how to present the text in numbers that we can add to a table, and then we have a feature table. The mathematical model that they use to represent text is the vector space model. What is a vector space? We have dimensions, so each word can be represented in two dimensional space, or multi-dimensional space. This is a vector. So each word has a representation in this space, and by using this model, we can expect that similar words may have similar or near similar representations in the space. So words that are very related, we expect that when we represent them in a vector space, the vector value of each word will be very close to each other. This is the idea.

What is the mathematical model that we can use? All the existing techniques, all the methods, including those used by transformers, are based on vectors. They represent each word and document as a vector in a vector space. So, in linear algebra, what is a vector? A point in a space, where it has magnitude and direction. So a vector has a magnitude and direction. In algebra, this is a vector, and the magnitude of the vector and direction of the vector can be computed using specific techniques in vector space. This comes from physics, where we can represent many physics phenomena as a vector, like velocity and force, which have magnitude and direction. And we have specific mathematical properties and equations related to vectors. So if you have a vector, you represent a vector as an array. From a computer point of view, we can represent a vector as numbers in a space, based on the dimensions, and then we can easily convert it to an array. And then we can perform some mathematical operations on it. One of the mathematical operations is computing the magnitude using a norm, which is a common technique in vector analysis. We can also perform an inner product, and this is the equation of the inner product in vector space, which is a very simple mathematical technique.

What is vectorization? Vectorization is to convert each token in your corpus to a vector. So any word should be converted to a vector, where it has a value depending on the dimensions. So, if I mention a point in a given dimensional space, it can be represented as x1, x2, a point in that space. How do we get these values, x1, x2? This is based on the technique that we use. All text representation techniques produce values, but the difference is what is the value of the vector, and how this value reflects the meaning of the word. This is how to differentiate between the different methods. They all produce values, but how much does each value reflect the semantic meaning, the actual meaning of the word? This is the main difference between the different methods.

Once we have a feature vector representation, we can have a table of feature space. So document one can be represented as a vector, document two can be represented as a vector, and then we are ready to enter this table into any modeling technique. There is no specific model in NLP for the text. We can use any machine learning algorithm, any algorithm.

One of these techniques, an early technique, is one hot encoding. But actually, before one hot encoding, we have another, which was the first attempt to convert text into numbers, and it is very interesting. A group of linguistic experts came together, and they tried to find a vector representation for each word. The first approach was a list of English words, all the unique words in English. And then they had a list of questions. These questions might be things like: is it male, is it a living thing, or can it talk, can we buy it. And then they assigned zero and one based on whether each question is true or false for each word. This was a very early attempt. So for "man," is it male? Yes. Is it living? Yes. Can it talk? Yes. Can we buy it? No. For "woman," is it male? No. Is it living? Yes. Can it talk? Yes. Can we buy it? No. For "tree," is it male? No. Is it living? Yes. Can it talk? No. Can we buy it? No. And so on. How many questions? Maybe a huge number, more than you would expect. So we can represent properties of each word this way. This was the first step. The linguistic people came together and proposed questions, and using these questions, they thought they could identify or create a vector for each word. It is one of the early attempts, but it is based on the number of questions, maybe 300 questions, 3000 questions. It is also language dependent. So we cannot use it for any other language. But it was a first try to have a vector space or vector representation for each word.

Then we come to one hot encoding. One hot encoding is very simple. We can create a binary vector, which is like a computer log, zero and one. Based on whether a token exists in your text, if it exists, you give it a one, and if the token does not exist in a sentence, you give it a zero. This is the idea of one hot encoding. So if you have a sentence, the first step is to identify or split your words in a simple tokenization. And then we go to the next step, which is to create an encoding for each one. The dimension of the vector depends on the number of vocabulary items in the sentence. So, for a document, if you have a set of documents, we can have a list of documents. Again, we have to find the set of vocabulary, and each vocabulary item has a unique ID, and based on this, we can represent the one hot encoding for each document. So, for document one, "like" gets one, "love" gets zero because love does not exist, "NLP" gets one, "AI" gets zero, "fun" gets zero, and so on. And this is how we create a feature space. We can enter this into any machine learning model.

This is an early technique. What is the problem with this technique? In your opinion? The dimensionality is going to be ridiculous. Dimensions will be very high, depending on the size of the vocabulary. You could use an inverted index to have the words point to the document. We can invert it. If you have a vector, it is easy to know what word is inside the vector. And this is one of the advantages.

Let us look at the advantages and disadvantages. One advantage is reversibility. If you have a vector, it is easy to know what word is inside the vector. Another advantage is that we can determine the position of each word, because one hot encoding arranges words and gives an ID for each word, so it somehow preserves the position of the word inside the vector. Another advantage is easy interpretability. You can easily interpret where each value comes from. Another advantage is from a computation point of view, it is manageable. But the disadvantage is that the vector may be huge, based on the number of vocabulary. So consider that in English, we have more than 300,000 different words. So it costs a lot of time and space. Another disadvantage is that the more documents we have, the more dimensions we get, and more zeros. There are a lot of zeros. And if the dimension is huge and you have a short sentence, we have a lot of zeros. This is what we call a sparse representation.

It is a good start, we can now run many machine learning applications, classification, clustering, using this representation. But the actual result is not very good. Then the next step: they started to think about a technique called bag of words. We do not consider any order. We just throw all the vocabulary into a bag. And then, for each word, we just count the frequency of this word inside the document. How many times does this word appear? That is all we rely on. No order, no syntax, no positioning, but just the frequency.

So, this is an example. If you have two sentences, this is the first sentence and this is the second sentence. We list all the vocabulary in our documents. The first step is extracting all the unique vocabulary, and then the number in the vector space represents the frequency of each word in the document. It is an improved representation, a step ahead, and it improves performance. In Python, we have CountVectorizer. It is part of scikit-learn, where we can use CountVectorizer to create a table with documents considered as instances and each column representing a vocabulary item. So we are able to create a table and feed this table to any machine learning algorithm.

Now, what is the problem with this approach? Look at this example. The meaning is completely different, but the bag of words representation is the same. So, another example: we have two documents. Document one: "The child makes the dog happy." Document two: "The dog makes the child happy." They have the same feature representation using bag of words. This is one limitation. Another important limitation, which is common in all frequency based techniques, is dimensionality. It almost depends on the vocabulary size, and at the same time, we have a sparse representation of the document. The other important drawback or disadvantage in all frequency based techniques is that there is no semantic meaning. There is no meaning of the word here. I just represent the word as a frequency. There is no context here, no semantic meaning. This is a common disadvantage in frequency based methods. So each frequency based method tries to improve the technique somehow, but it is not ideal yet.

We still have the dimensionality issue, the lack of semantic meaning, and we also face the problem of out of vocabulary. If you train a model to create a feature space for all your vocabulary, and in the testing phase you introduce some word that is not inside your vocabulary, this is the main problem, what we call out of vocabulary.

The advantages of bag of words include simplicity and interpretability. It is easy to compute, just count frequencies. It works for text classification, where we classify topics based on word frequencies. It is also useful for information retrieval. Computation is simple, and it can be applied to any natural language, English, French, or any other language. But the disadvantages are that it ignores context; we just rely on frequency, there is no semantic meaning, no context. The dimensionality of the feature space depends on the vocabulary size, it may have many zeros, and there is a lack of semantic information. We also have the out of vocabulary problem.

So, one hot encoding in scikit-learn: when you look at the implementation, notice that the number of vocabulary might not match what you expect, because one hot encoding works at the token level, not the word level. So for every token, even if that token appeared before, it creates a separate feature vector. It works on the token level, not the word level. So you may find a lot of redundancy. A word that may be represented more than once if it appeared multiple times in the text. You may find repeated columns.

For bag of words, this is how we create a bag of words representation. Each word gets a unique ID, its index. So the word at index two has its index, each word has its index, and then we can build a document feature space. Where the column represents the documents. So index zero is document one or sentence one, sentence two, sentence three. And this is the feature space for each sentence.

One of the main disadvantages, as you see, is the lack of semantic information. So, in the research area, people study the limitations and then start to work on overcoming them. This is how research improves. If you find a research topic, you research all the documentation related to this topic, study it well, and find how to come up with a new idea to overcome some limitation.

Then comes the bag of n-grams. What is a bag of n-grams? It is dividing your text into chunks of n consecutive words. At the end, it is your choice. So, if you have a sentence, and n is one, we divide the sentence into chunks of one word, which is a bag of words. If n is two, we divide our sentence into two consecutive words. This is what we mean by a bigram. If n is three, then we divide it into chunks of three consecutive words or tokens.

What is the idea of using n-grams? They try to introduce some semantic meaning into our representation. If two words come together many times, how many times is that combination repeated? Or if three words are repeated together, it captures some local context. So they try to improve the idea by introducing some semantic meaning.

Unigram, bigram, trigram. So we can use the same example. Based on your n, this is a unigram, and this is a bigram where n equals two, and a trigram where n equals three. Here is an example. I have a text, and I choose a range of two to two. Two to two means I want to only extract the bigrams. Sometimes you may write one to two, and in that case, it will create a feature space for all unigrams and then for all bigrams. So two to two means I need only the bigrams. Based on this, this is the resulting n-gram division. As you see, "I love NLP" and so on. And this is the vector space related to this sentence. The frequency, but instead of the frequency of a single word, I am searching for the frequency of the n-gram, based on your n. How many times does this n-gram occur? Then it slightly improves the representation, and as a consequence, improves the result of the NLP task, but it is still not the ideal way. It suffers from some disadvantages. If I introduce a new text, there is no representation for a word that was not seen before, because this is a common disadvantage in all frequency based techniques: out of vocabulary. If the model has not seen a word in the vocabulary before, then it cannot produce a representation.

For the bag of n-grams, we have some advantages: it captures some context, it is efficient to represent data, we can interpret it, and it is very simple. But at the same time, computation is very expensive. It has high dimensionality, maybe with a lot of zeros, it is computationally expensive, it ignores the overall structure since we only have local context of two or three words, not the whole structure of the sentence. The choice of n is a challenge. If you increase n to capture more semantic information, it gives you a huge feature representation. So choosing the right n for your application is challenging and comes down to trial and error. And the common disadvantage is out of vocabulary. If the model did not see a word before, it gives you a zero for any new vocabulary.

For more improvement, they thought about TF-IDF. These techniques were invented before NLP. They came from a statistical background, originally used to rank search results. How much does a search result match the user query? It was an old technique to evaluate search engine performance. Then they thought, why not use this method to represent text?

So what is the idea of TF-IDF? In term frequency, we just compute the frequency of each word in a document. But we did not consider the frequency of this word across the whole collection of documents. The idea is, if a word is repeated many times in a specific document, and it is not a stop word, it means this word is very important for that document. But what happens if the same word is repeated many times across the whole collection? Is it significant to a specific document, or is it a common word across all documents? It is a common word. So, using this technique, we add what we call a weight. And the weight comes from two things: how many times is this word repeated in a document, plus how many times is this word repeated in the whole corpus. If the word is repeated in a specific document and it is rare in the other documents, it means it is significant, and the weight of this word should be high. But if the word is repeated many times in a specific document and also many times in the other documents, it means this word is not as significant, and the weight should be reduced.

So TF, we compute the frequency of each word in a document, but with inverse document frequency, we may add more weight for a word if it is rare in the other documents, or we can reduce the importance of this word if it is commonly repeated in different documents. This is weighting. Based on whether the word is rare or not rare in the other documents. And this is the basic idea of term frequency inverse document frequency, which we call TF-IDF.

How to compute TF-IDF: it is a score computed by multiplying term frequency by inverse document frequency. So for each word inside the document, we compute its term frequency. The equation for term frequency is: the number of times a word appears in the document divided by the total number of words in the document. For example, if I have a document consisting of maybe 100 words, and I am searching for the term frequency of a specific word like "Trump," if Trump appears in this document about five times, what is the TF? Five divided by 100, so it equals 0.05. For each word, we compute the TF.

The second part is to compute the inverse document frequency. The idea is that rare words should get additional weight. The equation for IDF for any term is equal to log of the total number of documents divided by the number of documents containing the term. This is the weighting equation, which may increase or decrease the value of TF. If a word is not repeated much in the other documents, the TF-IDF value will be increased. But if the word is repeated too much in the other documents, the TF-IDF will be reduced.

For example, if I have maybe a thousand documents, and the word "Trump" appears in 50 of those documents, what is TF-IDF for the word "Trump"? IDF equals log base 10 of 1000 divided by 50. That is log of 20, which equals 1.3. Now the TF-IDF score is the multiplication of TF, which is 0.05, times 1.3. The result is 0.065.

Now, I will make a small change. Remember this equation. Instead of appearing in 50 documents, if the word appeared in only five documents, what would change? IDF changes. IDF in this case would be 1000 divided by 5, which is log of 200, which equals 2.3. Now the new TF-IDF is 0.05 times 2.3, which equals 0.115. In the second case, the word "Trump" appeared only five times across documents, so it is considered a rare word. That is why the total TF-IDF score is higher, 0.115, with more weight. But in the first case, it appeared 50 times, so it is not considered rare, and the TF-IDF score was lower. Do you understand the idea? Now we have a weight. Not based only on the frequency, we add more information about how important a word is relative to the whole collection of documents.

So, this is the idea of TF-IDF. The score multiplies TF by IDF. Here is an example. If you have four documents, and this is the set of vocabulary. The first step is to compute the frequency of each word in each document. For example, "blue" in the first document occurs one time, and the other words like "sky" also occur one time, after removing the stop words. The second step is to compute TF. TF is the number of occurrences divided by the total number of words or tokens in the sentence. So, if a word occurs once and the total number of tokens is two, then it is one over two. We compute TF for each word. Then we compute IDF. For IDF, if we have four documents, for the word "blue," I compute log base 10 of 4 divided by how many times "blue" appeared in the whole collection. If it appeared only one time, we compute it accordingly. Once we create IDF for each word, then we can multiply the TF by the IDF, and then we come up with the feature space, which is a weighted feature space.

The implementation is very easy. We have in scikit-learn a specific function to create TF-IDF. Using TfidfVectorizer, we can build the representation. In the in-class code, I give you examples of how to implement one hot encoding and how to implement bag of words. We also have scikit-learn methods to automatically compute TF-IDF. You may find some questions in your midterm related to TF-IDF, so you can practice with your colleagues on how to compute TF-IDF.

For the bag of words using CountVectorizer, we have whole numbers, but with TF-IDF, we have real numbers, which introduce weights. The weight is now different for the same document. With CountVectorizer, we simply count how many times a word repeats, but now with TF-IDF we have a different representation based on the weight of each word. Why do we use log? We use log to smooth the difference between the values, to reduce the large gaps between raw frequency counts.

Limitations: we still have limitations. One limitation is that there is no relation between words. We deal with each word as an independent unit, never thinking about the context. Each word is treated as independent, which is not correct when we talk about language. A word has meaning when it comes in different contexts. Sparsity: a lot of zeros, based on whether a word exists or not. Lack of semantic understanding: we just compute the word as a frequency, there is no meaning at all. For example, the word "bank" when it comes in a financial context has no different representation from "bank" when it comes as a side of a river. This is another drawback.

We try to generate vectors that represent properties and attributes of a document. If a word exists or does not exist, it is one of the attributes. It is an important attribute in our case. Even zeros carry information, because a zero means the word does not exist in that document.

TF-IDF is not well suited for a small corpus. If the number of documents is small, then IDF will mislead. It may over-inflate the importance of a word because the total number of documents is not large enough. So it may give a word a high weight when it is not important. Do not use it when you have a small corpus. And again, out of vocabulary. If the model did not learn a vocabulary item, we are not able to represent it. If I feed the model a new document like "the sky and xyz today," it is able to represent "the," "sky," and "today," but "xyz" is not in the vocabulary. So we cannot have a representation for any word that is not in the vocabulary.

These are the main techniques for representation of text based on frequency, which come from statistical techniques. At that time, this was considered a huge step. It allowed us to perform a lot of NLP tasks, but still, it is not a good representation for the text. It suffers from many disadvantages, which in the next step, word embedding, they try to overcome. It is an open research topic. Even with the use of transformers and large language models, we have not come up with the ideal representation. We still work on this. With transformers and word embeddings, we improved, we introduced some semantic techniques, and we consider the context of the word, but still, it is not perfect. It is still a hot topic of research.

Now, I will move to text similarity. What is text similarity? Text similarity is actually considered a benchmark test to measure the performance of any text representation. If you have one method for text representation, and you have another method for text representation, how do we compare them? We compare them using text similarity techniques.

So what is text similarity? It is a method or computational technique to measure the degree of similarity between any words or even any documents. The applications are many: speech recognition, machine translation. This is to improve the performance of any of these techniques using text similarity. It is a benchmark test.

What are the measurements? We have many techniques to measure the similarity. We have Jaccard similarity, cosine similarity, Euclidean distance, Levenshtein distance, and Hamming distance. Do you remember these techniques? We measured similarity using these.

I will give you some information about Levenshtein distance. Levenshtein distance is the minimum edit distance between two words, based on counting the number of specific operations: how many deletions, how many insertions, how many substitutions you need to apply to convert one word into another word. This number represents the Levenshtein distance measurement. For example, if you start with "kitten" and I want to find the similarity between "kitten" and "sitting," we just count how many operations. Starting with "kitten," I can change K to S, that is one operation. Then from "sitten," I change E to I, that is one substitution. And then I add G at the end, that is one insertion. So the Levenshtein distance between these two is three. It is used as one of the similarity measurement techniques.

Then the Euclidean distance. If you have two points in vector space, you can measure the distance between them with the Euclidean equation: the square root of the sum of the squared differences between the coordinates. This is also used as one of the techniques for measuring similarity between two vectors. So if word X is represented as a vector and word Y is represented as a vector, we can easily apply Euclidean distance to get how similar these two words are. 

The next technique, which is used now in NLP, is cosine similarity. The difference between Euclidean distance and cosine similarity is that Euclidean distance does not take into account the direction of the vector. We just measure the magnitude. But in cosine similarity, we consider both the magnitude and the direction. The equation is the dot product of A and B, divided by the norm of A times the norm of B. The idea comes from the cosine function: if two words are very similar, the angle between them will be small. What is the cosine of 0? One. It means two words are identical. If the two words are far apart, there is an angle of 90 degrees between them. What is the cosine of 90? Zero. So the value of cosine ranges from one, meaning the two words are identical, to zero if they are not similar, and sometimes to minus one. This is the behavior of the cosine function.

How do you compute it? First, you work in a vector space. For each sentence, you can get a vector. For each document, you can get a vector. And then once we have vectors, we can apply the cosine function. A dot B is the inner product, and then we divide by the norm of A times the norm of B. Based on this equation, the value might be 0.667, which is considered high, indicating similarity between the documents. It is not identical, but there is similarity. How accurate is the similarity measure? It depends on how good the vector representation is. That is why we use cosine similarity to measure the performance of the representation method. When we have a text representation technique for a word, and we get a text representation for another word, we can measure the similarity. If we know two words are identical or very similar, and we get a high similarity score, it means the method is a good method for representing the text. If we get a low score, it means it is not a good representation. This is why we use it as a benchmark to measure the performance of each text representation technique.

The computation is very easy. You can use any technique: bag of words, count frequency, one hot encoding, or TF-IDF. Then we can compute the cosine similarity. As an example, I just use count vectorization, counting how many times a word is repeated, and then we can measure the cosine similarity between any documents.

As I mentioned, we use text similarity to measure the performance between different representations. During this lecture, we have covered different representations: one hot encoding, bag of words, bag of n-grams, and TF-IDF. This example is part of your in-class code. Consider we have a set of documents, and we need to measure the pairwise cosine similarity between sentences. Using CountVectorizer, the highest similarity was between certain sentences. For example, using count vectorization, "it is hot outside" and "the sun is hot" might get the highest similarity because "hot" appears in both sentences. Now, if I change it to TF-IDF, I represent each sentence using TF-IDF and measure the cosine similarity pairwise. The highest similarity might be between "I make my hot chocolate with milk" and "I will have hot chocolate with milk," which is more logical because they share more context. So which is the better representation? The TF-IDF result is more logical because the sentences are in the same context. This is how we can use cosine similarity as a benchmark for evaluating different representations.

We will continue with cosine similarity in the next lecture, when we talk about the embedding techniques. We will use the benefit of the neural network. So, instead of frequency based methods, where we rely on statistical techniques to have a text representation, in word embedding, which is a huge improvement in text representation, we will use neural networks to generate a vector representation based on learning the context between words.

Now let us discuss some ethical concerns related to frequency based techniques. Do you consider there are any ethical concerns? We use frequency based representation. One concern is that the potential failure to deal with context and meaning may create problematic situations. For example, with text comparison, you may have two texts that are represented similarly but have completely different meanings. This could create situations where, say, someone is checking for plagiarism and the comparison model shows documents as similar, but they are fundamentally different. Or conversely, documents that are fundamentally similar could be rated as different. 

Since frequency based methods prioritize higher frequency words, that creates a term bias about words, topics, or even social dimensions of what is being talked about. Topics that are more common get more priority, and that could have downstream effects.

This is one of the ethical concerns: bias toward common topics. What about minority topics? The frequency is not a good representation for them. It reduces visibility for minority topics and less common topics. Many people now discuss popular topics on social media but ignore scientific or deep knowledge topics. Based on frequency, we would treat these important but less discussed topics as less significant. So it is not a good representation for this, and it will reduce their visibility.

Another idea of bias relates to languages. On the internet, how many documents are written in English versus other languages? Frequency based methods introduce bias related to the types of languages. So there are ethical concerns about representing minority topics and minority languages.

Now we have an idea of how we can feed text into machine learning algorithms. We can prepare a table with features, prepare our feature space, feed it to a model, and then perform any machine learning task: classification, clustering, or anything, because this is the starting point. How we represent text. Up to now, it is not a great representation, it suffers from many limitations, but still, we are able to accomplish tasks. And it is still a good way for specific tasks. For example, word embedding is a good representation but uses neural networks, and neural networks introduce some computational challenges. For a simple classification task, TF-IDF may work very well. For information retrieval, TF-IDF works well. So now it depends on your task. If the semantic meaning and the context are important, then frequency based methods are not a good way to represent your data. But if the frequency is more important, then frequency based methods are a good choice. Not because they have many limitations does it mean they will not work with any NLP task. They are still used, but think about your task, which method is good for your task, and the trade off between accuracy and computation cost.
