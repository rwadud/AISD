By the end of next week. So I'm now in a final patch of assigned one, but left two, it's already published. Okay? So, you can see enough, too, and assignment one, you can respect it by the end of this week. I will talk, during this lecture, about future representation, and we will talk about this common techniques, one more encoding, bag of forth, bag of enagram, and we will talk about the similarity, lexical similarity. This is the same slides from the previous collection, where we have an NVP house, lifestyle. And in the last lecture, we talked about taxi cleaning and reprocessing, how to clean our taxes, to heat, ready for the latest step, which is the most important stuff, even in machine learning cost, which is feature engineering. What we need by future engineering. What is a feature engineer? Driving new attributes from... to create, represent your object as a set of feature. Is that important? Step, like, reprocessing, or less important than prepossessing. Equally important. It's... Equally. Perfect. Both of them, I think, come on to data preparation. Yes, yeah, it is, we prepare our data for the most important part, which is the modelling, feature engineering, and machine learning, and in a natural language processing, it's a very important step. You may hear about what you know, okay? They publish some data, and then the people competes to have a good result, okay, of this thing. One feature, only one feature, may be used, and allow them to win this contest. Aims a new feature. So this is important to play you, important in the modelling, the result of your model. So, the feature engineering is a very, very important mistake. And during this lecture, and the latest selection, we talk how we create a fisher from our documents, or techies, or in our course. Okay. So, how feature presentation, we know in machine learning how to represent our data. Your data in machine learning, it's enough, which is a computer, our airport to understand. But what about unstructured types of data? Like, here, images is a type of unstructural data, speech, it's a type of unstructural data, like a text. It's unstructured data, OK? Now, how the computer represents this unstructured thing. Let us take an example here about the edge, how the, how it represents the image. The computer, so it has processes age. Yes. Matrix. Metrics, model. But, so, for any unstructured data, we need a mathematical model to represent resistant, okay? So the mathematical moment for image is the matrix form. What each value in the matrix result... in thirsty. Okay? So, in terms of gray and white, it's based on your processing. But each value here, in the metrics represent a value. What's a feature vector? And this is a picture vector, we created, we can now enter this picture vector to any model, and then we have protection for whatever we can do with this, okay? So we need a mathematical model, and an image representation, our mathematical model is metric support. Now, what about the speech? What is a mathematical model to present the speech, the voice? The waveland? What is better? The waves. Where we have an amplitude. So the amplitude of this way represent a feature or this sound, okay? This is a mathematical look, okay? For in transform, the screen, transform, and the in the web, it's automatic, and more them to represent a different waves. Okay? And now come to the left. This is unstructured, and we need a way to represent what mathematical model to represent is a first. We can classify the presentation technique. The first one is a frequency test, based on how many behind this world appear in the text, and the more advanced one, what we call it, a world embedded meat, okay? And this is, we will cover it, next lecture, include water to fake, love, and fast taste. And the more advanced one, which we call it, universal technique, based ultra transformer, which include how the transformer and third large language model represents the text. Okay? So, in this lecture, we will focus on this part, at the early techniques, and then in the next election, we will focus on the adding technique, where we actually use tenure. Okay? This presentation, based on the frequency. The idea comes from... statistics and the math. You know, computer science is, it's a new top. So all the people know in computer science has the area in early years. They have the statistical background, all mathematics background. Okay, after that, there is many, many people now cook, okay, graduated as a computer scientist. But before, when they started to touch base, the computer science and even anything related to the computer science, the people started this work as mathematical background or statistical background. So the early technique is always relying on statistical techniques or mathematical techniques, okay? Like a machine learning, for example. What is the early technique in machine? The early sweat. Regression? Regression is everyone. What is a regression? Um, predicting continuous? It is statistical. Technique. If you study statistics, regression, statistics. Okay? So all these models come from, again, starting from statisticality. That's why this part rely on and all the statistics. But with a new term, at that time, not challenge model, they say about how we can use this techniques to represent our kids. Okay? So in this lecture, I will focus on frequency this technique to represent that. So what is the idea of representation of the data? The idea is, at the end, we need to use the modelling technique, a normal modelling technique. So, in a machine, we have to prepare a table, where we have an innocent, and then we have a set of feature, represented systems. And at the end, we come up with the same presentation, but for the text. This is our end. How to present the ticket in, in a number, that we can add it in a table, and then we have kids table two. So, the mathematical model that they think, how to represent the game, that is, then we use a very full space model. What is a bit from space? We have a dimension, so each word can be represented in two dimensional space, or much dimensional space. But this is a vector. So each word has a representation in this space, and by yielding this model, we can expect that the similar model may have a similar or near to similar presentation. in the space, okay? So, here, this is a very related, so we expected that we are presenting this world in a factory space, a vector value of each model of each world will be very related to each other. And so on, this is their idea. Have we had, what is the mathematical moment that we can use it for? And this is yours to tell now. So all the inviting technique, all the methods use, the body transformer, is based on vectors. Okay? represent each word and a document as a vector space based on the vector space. So, in linear algebra, what is a vector? Is a vector, this? What is a vector, a linear, a chapter? Point, yeah, the space? Where it has... Perfect, magnitude, and... Direction. So this is a bit, has a magnitude, and direction. So, for algebra, this is a vector, and this is the magnitude of the vector and direction of the vector can be computed using the ideal of a specific technique in a vector space. Okay? And this comes from the FedEx, but, but I went where we can represent many physics phenomena, it was in a vector, like velocity, and, like, force, but it has a magnitude and voyage. Okay? And we have a specific mathematical properties or equation related to a vector. So if you have a vector, it representes a vector, as an array, so in a computer point of view, we can present the vector, where it is a number in a space, in a Viva I mentioned, based on what is this dimension, and then we easily can convert it to an array, okay? And then we can perform some mathematical corporation from district. One of the mathematical operations, we can compute the magnitude, OK, using an all, which is a common technique, and effectful analysis, and we can perform an inner product, and this is a equation of the inner product of in vector space, which is very simple mathematical techniques. Okay? Right. What is the factorization, the factorization, is to convert each token in your corpus to effect. So any word should be converted to a vector license. Where it has a value, it's bent on your dimension. So, if a mention, egg, one, B, then for each, okay, and be represented as x, 1, x, 2, x, a point in that space, okay? How we can get this value, X, one, X, two. This is based on the technique that believes. Okay? And this is how to compare how improvement that any takes presentation into the juice. They have always the same penguin. But the difference is, what is the value of the spectrum, and how this value reflect the meaning of this fact. This is how to differentiate between the street, main it looks. All that it needs, produce values, but how much this value reflects the semantic meaning, and the actual meaning of this world. This is the main difference between the different people. Okay? And feature, vector, once we have a feature, the spectrum presentation. Now, we can have a table of kitchen space. So the document one, NP represented as a vector, document two, can be represented as a vector, and then we have ready to enter this table to any modelling technique. And he doesn't have a specific model in the name for the test. We can use any watering information learning, any output. Okay? Right. One of this technique, or every technique, it's one hot encoder, but actually, if you fold one hot encoder, we have another, which is the first trial to convert, the take into numbers. And it's very interesting. So, a set of, okay, group of linguistic experts come together, and they try to find a back to presentation for each word. The first trial, it was a list of English words. All the list of English words, I can say here, oh, as an example, and man, uh, play... So, a list of me, which call a list of book tablet. Unique words in English, okay? And then they have a list of questions. Okay? So, this questions, maybe, if it is made, If it is letting the same, Or if it is, can talk. And we want. And this is a list of all questions. And then what 0 and one based on if this question is true or force based on this. So, And this is very, very early tribe. So it comes with a book, okay? If it is man, yes, maybe, living, smoke living, and talk, no, and we buy, yes, for a man, it is a man, living, yes, and two, and we buy zero. We can adhere something. Okay, it's me, I think it's no, I don't know. Yes, and top zero, can we find one, and so on. How much questions? Maybe a huge number more than you expect. So we can represent is a property of each work in this event. And this is the 1st step. The linguistic people come together, and propose some question, and using this question, they think that we can identify or can create a vector for each word. Okay? It's one of the tribes, but, again, it's based on that number of question, maybe, 300 questions, 3,00 question, it's based on this, and it is language dependent, okay? So we cannot use it for any other language. It's language thing. But it was a first try to have a vector space or back representation for each one. And then we come to one hot in coach. So, one hot encoder. It's a very simply, we can create a binary detection, which is a computer lock, like, 0 under one. Based on, if this heck exists, you're talking, it exists, a good one, and if this token is not exists in a sentence, we look zero, okay? And this is the idea of one hot ink. So if you have a sentence like this, this is an example. The first step is to identify or split your work in a simple tone, you need to, okay? Or what? And then we, go to the next step, is to create an encoding for each on this one. So this. This is a vector presentation of this. Notice that I mentioned the build on. How many vocabulary in this sentence, so the dimension of the back to the build on, that number will happen. So, this has one here. So, for me, to represent in text. And for a document, if you have a set of document, we can have a list of document, document, document, document. Again, we have to find the set of the vocabulary, and each vocabulary has a unique ID, and the base of this, we can represent that one encoding for each documentation. So, for document one, I, like, one, love, zero, love, is not exist, and LAP, one, AI, zero, fund, zero, and so. And this is how we create a big process. features. We can enter this to any machinery, yes? Okay? And this is an early decade. What is the problem with this technique? In your opinion. The dimensionality is gonna be ridiculous. Dimensions will be very hard. Depend on the list of the vocabulars. You could use an inverted index... We can use, sorry? An inverted index. To have the words point to the document. We have, we can invert it. So, you know, inverse engineering. If you have a vector, it's easy to know what is a word inside the vector. And this is one of the advantages. Okay, other advantage. Let us divide it. What is advantage. This is one of the advantages. Reversal, okay? What is other advantage? We can do the possession of each word, because one word, encoding, arranges a word, and take that, give an IG for each word, so it have somehow preserves the possession of the word inside the vectory. The other advantages, easy to interpret, okay, for interpretability point of view. You can easily interpret what is this value come from? The other advantage is, the computer market. Okay? Computation point of view, it is okay. But this advantage is, this factor may be huge, based on the number of vocato. So, consider that, in English, we have more than 3,000 different... Okay? So it's cost a lot of time, and... And then one other disadvantage, if you look at... the document. The more, the more dark we have, the more dimension, and more zeus. There is a lot of seals. Okay. And if imagine if a dimension is huge, and you have a sentence like this, we have a lot of news, okay? So this is what we call it disparity. presentation. And then, okay. it's a good point. Good start, we can now run many MLT application, classification, clustering, using this presentation, but the standard result is not very good, okay? Then the next step. They started to think about a technique called back before. We not consider any order. We just threw all the vocabulary in a bed. And then, for each world, we just account the frequency of this world inside the book. How much time in this water? And this is all what you need, rely on. Okay? So, an order collection, no order, no syntax, no positioning, but just rely on the frequency. All right, so this is an example, if you have two sentences. This is the first sentence, and this is the second sentence. We list all the list of the moccazari in our document. So, the first step is extracting all the need, vocabulary, and then the number here in the vector's face represents the frequency of each one inside this. Okay? It's a good presentation. It's more a step ahead, improves the performance, but at the same times, and this is how we can... Python, we have counter vectorizer. It's a part of psychic learn, where we can use a counter vectorizer to create... a table, okay, with the document, considered as an instant, and each column represents avocado on this docket. So we are able now to create a table and defeat this table to any modern elbows. Machimodel algores. Okay? Now, what is the problem in this modern look at this exam? While the meaning is completely different. Okay? but the bad. That is a signal, right? So, another example here, we have a two document, document one. The child makes the dog happy, the dog makes the child happy, but they have the same feature percentage. using a bag of water. And this is one organization. Another important meditation is, which is common in all the text and the frequency. Okay? The dimensionality almost depends on them, okay? And at the time, meantime, we have a first presentation of your document. The other important here, drawback or disadvantage in all text frequency tests, there is no semantic meaning. There is no meaning of the word here. I just represent the world, it's a frequency. There is no context here. Okay? No semantic meaning, introduce, invested meaning. And this is a comment, this advantage in frequencies, the mess. So each one of the best frequency method, we try to improve it, the technique somehow, but it's not ideal yet. Okay? Okay? So, we try here to get right, we call a lot of zeros, depend on the frequency, but still the dimensionality is hard. There is no semantic meaning, and at the same time, it is, we face a problem, what we call it, out of Pocata. It trained a model to create a feature space for all your vocabulary. Now, in the testing case, if you introduce some wood, that is not inside your vocabulary. This is the main problem, what we want it out of vocabulary. So, I think here, I can show you something... some time to open it. I will continue... Okay, so this is... Is there, we have the CM, uh, problem? I make it ready, but I have to restart my computer to make this machines work. So, okay, still we have some advantages and some of disadvantages, advantages, simplicity, and interpretability. It's easy to compete, just account, and frequencies how many, what, when, the status classification, where we don't need a mean. So, I think it's classification, we classify the biggest, so based on, we choose some words from this status, and if this word repeated, more frequencies this status, so it would work with a thickest classification cost, and also information agreeable. Okay? Computation compression, very simple in application, language, and language, and most techniques, it can be applied to any natural language. Just in the bank or not, French, so we can apply the technique in any language, English, French, or any other language. But the disadvantages, in this case, let's ignore context. We just rely on the frequency, there is no meaning, semantic meaning, but no context. Time, the dimension, picture space, based on the vocabulary size, spar steam, may have many zeros, and lack of the sphenic information, and we also have out of vocabulary. I hope it In your ink class code, I give you some examples, let it have to implement one of encoding, how to implement a pack of wood. And I, as usual, I will publish in... after the lecture. So, that you're street, that's good. So, one hot encoding in psychic learn. So, when you hear some, some builds, how, when holding, something that you have to, uh, not here, why the number of vocabulary is, I think, seven, but I have more than seven, like, because one of, one hot encoding clique, represents, it's not work, work, on a word, it's work in the talking. So for every talking, even, if this talking, I represented before, they have to create a feature detector for system. So it's work on the token level, not on the water level. So you may find a lot of redundancy, and this need. So that may be represented more that at one time, as it appeared in the text. So, while I have seven vocabulary, but the feature space is more than set. And you can find the repeated cabinets here, I think. So, okay, so now, because it's located in the latest, so I think it is the second one. I think it's also... Okay, so a lot of redundancy in the presentation using one hobbingo. So for the back of words, this is how we create a back of words, so movie 2, it means it's not repeated 2 times, it just gave a unique idea for this one. So it's the idea of this world, the index, maybe, the index is too, God index, one, love that, one, and watch 3, and then can end, uh, document a feature space. Where the column here, present this document. So 0 is a document one or sentence one, sentence two, sentence three. And this is the picture space for this sentence. All right, so I, all right. go back to Right, so one of the men, this advantage, as you see, then... lack of the semantic information. So, in the research area, or in a development where we divulge a mess with the people, study well, what is resting the message, and then they start to work on the limitation to solve the some limitation. And this is how the research improve. If you find a search topic, if you work in a research, always research for, because of all the documentation related to this topic, and we try to study it well, to find how we can come up with a new idea, to overcome this limitation, or to solve it, one of the limitations. Then it comes with a bag of enigra. So what is a bag of enigra is to divide you, take this into a chunk of any consecutive words. At the end, it's your choice, okay? So, if you have this sentence, and the end is one, we divide the sentence, the chunk of one word, which is a bag of word. Now, if N is two, we divide our sentence into two consecutive words. So, for here, this is a sentence, so we can divide it, because this is, is a sentence. And this is what we mean by our enagram, okay? If N is 3, then we divide it to chunk of 3, consecutive quarts or topics. Right, so what is the idea of using any graph? They try to introduce some semantic meaning inside our presentation. So, if these two were to come together many times, how many times is, toward, repeated, or this three word repeated, it gets some of local... Introduce some context. So they try to improve the idea of producing some semantic meat. All right, so, monogram, by gram, tri gram, so we can, and the same example here, based on your end. This is a unigram, and this is a bi gram, where an equal three. Again, we have an example here before we talk about advantages and disadvantages. This is an example of anagram, right? So, it is cool. Stefan, the biggest. So I have a text here, and I choose arrange two to two means here, I want to only extract the two crab. Sometimes you may write here, one, two, two, and this case, it will create a picture space for all one gram, and then for all two gram, okay? So 2 to 2 means I need only the 2 grand. So, based on this, this is the resulting, and the grand division. As you see, I love LLP, and so on. And this is a vector space related to this sentence. And then frequency, but instead of the frequency of the unique one, I'm searching for the frequency of anyone, based on your end. Okay? How many times is this? And then, it's a little bit improved the presentation, and as a consequence, improved the result of MLP task, but it's not still the ideal way. It suffered from some disadvantages, okay? So, if I introduce a new text, So, the response is no representation for this work, because this is a common, disadvantage in older death, frequency based technique, out of vocabulary. So, if the modern, from this vocabulary before, then it produced a presentation of, see, what consistent. Right, so for a pipe of anagram, we have some adventures, capture some contests, a little bit, same thing, efficient to represent is a data, we can interpret it, and it's a very simple, but at the same time, it's a computation that is very expensive, okay? So spicy, again, it's a high dimension, maybe with a lot of zeros, competitionally expensive, ignores the overall structure, we just context of researching for a local context of two or three words, with more all the structure of the context of the sentence, and the choice of N is a challenge, actually. So, if you increase M, to capture more semantic, data, it gives you a huge fact of presentation, okay? So, it is choosing which perfect area. For your application is a little bit challenge, and it comes with a trial and error, and the common, this advantage is out of vocabulary. If the model didn't see the vocabulary yet, So for any new vocabulary, it gives you as you. And more improvement, they think about it, and all the technique, these techniques, it's invented before, and LEP, come from the statistical background, they used to use this technique to rank, the search, okay? So how much, how much them, the search result, match the user quit. So it's an only technique to evaluate the search engine optimization. Then they think, Why would anything use this message to represent this text? Okay? So what is the idea of TF IDF? In a frequency? We just compute the frequency of each word in a document, okay? But we didn't consider the frequency of this word in a whole documentation. The idea is, if the world repeated too much, in a specific document, what we can conclude. If a specific word, if I have a document, a certain document, and a specific word repeated too much. It is, it's important, if it's not a stop word, it means this word is very important for this document. Yes? But what happened if the same world repeated too much in the whole documentation? What you conclude in this place? Is this what is significant? To this specific document, or it is a common world between all the documentation. It's a common world. So, we try losing this technique to add what we call it, a weight. And the way it comes from 2, 10. How much is this world repeated in a document? Plus, how much time this world repeated in the whole corpse? So, if the world repeated in a specific document, and it is rare, in the other document, it means it's significant, and the weight of this world should be high. But if the world repeated in a specific document, many times, but at the same time, it's also repeated in the other document, many times, it means this world, it is not the weight of this world should be reduced. So TF, we compute the frequency of each one in a document, but adversity F, we may add more weight for this one, if it is rare in the other document, or we can reduce the importance of this word if it is not repeated in a different document. This I this here? Okay? So waiting, waiting to keep. Okay, based on, if this word is red, or this word is not red, then the other pocket. And this is the basic idea of the, tell me frequency, and verse, document frequency, which we call a TF idea. Okay? So have a computer, F, I, D, F, it's a score of multiplication of term frequency, multiplied by inverse document, uh, frequency. Okay? So this is a equation. How we can compute it... Okay, so tell me frequents. So this is a terrorist system. For each word inside the document, we compute, it's turn the frequency. And what is the equation for the term frequency is to tell the frequency how many times this water repeated, and provided by the total number of document in this total number of world in this document. All right. So, for example, if I have a document consisted of maybe 100, and I'm searching for the term frequency, maybe, for a specific word, Trump. Trump repeated insist about this document about five times. So what is a GF in this case? What's a GF? All right, so, how many times? Five divided by? 100, so it equal. So this is a GF. Okay? Now, for each word, we compute the TF for each word inside the locket. The second is that... to compute an adverse documented frequency, right? So, this idea, as I mentioned, red water should get additional weight. And the weight come from this equation. IGF for any 10, It's equal to lockdown. I will tell you why you will use a lot, but it's a total number of documents, divided by number of documents, resistant. And this is the waiting equation, which may increase the TF or may decrease the value of TF. So this world, not repeated too much, and the other document that GFA value will be increased. But in this world, repeated too much. In the other document, the GF will be at least, okay? So, for example, here, for the same example, I have maybe a thousand document. This is a number of... Okay? And the water tram, like being it's... maybe 50? in this pocket. Now, what is I T F I G F for the water trap? So IDF? Have a complete IDF? What is the number of document? 1,000? 1,000? So you have to think, okay. Divided by how many times? Trump repeated in this thousand document? 50? Okay, so can you help me? Lock one service and it's locked 20, yes? One, two. What is it of 20? It's one plus a long ten of two, so it's 1.3. One.3? One.3 for sure. Okay. So one. is three. Now the score of IDF, a GF ITF. So, TF, I, TF, is on application of TF, which is 0.05, on divide by 1.3. What is the result? 0.061. 0. sorry? 0.065. 65. Okay. Now, I make a little better change. Okay, so remember this equation, all right? Now, instead of this one, repeated 50 times, I may have this one, repeated in the other document, five times. Now, I want to see another colour. What would be changing? IGF, yes? So IDF, in this case, will be... 1,000, the amount of document will not be change it, but we divide it by... five. What is the reason? Sorry? Okay? Now, compute the new IGF, IF, ITF. Now, the new IDF, uh, TF, I, GF, it is the same. 0.05, 2.3. What is the reason? One, 0.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, Please, please. So, what is it? And second, the case... The water tramp repeated five times. So it's considered as... an air world. That's why the total score of TF IGF comes to P 0.15. And more weight. What is this word? But in the first time, it's repeated 50 times, so it's not considered as rare, or it's repeated more, okay? Then the GF IDF, for this word... with you. Did you got the idea? Yeah. Okay? Now we have a weight. Not event, on the frequency, we add more information, how much this word is important. They need to do the whole document. Okay? So, this is the idea of IDF, ITF. Then, the score will multiply TF by IDF, and this is an example. If you have this document, we have four documented here, one, two, three, four, and this is the set of vocabulary. The first step is to compute the fragrance of each wall inside the vocabulary. So, glued in the first, okay, one time, and all the other, sealed sky is one time after removing the stockhold. Okay? And the same for the second, and the same for the second, and this is a frequency. Very simple frequency. Then we started to compute TF. So, TF is, the number of coqueras, divided by the total number of war, or tokens in this sentence. So, here, it's one, but the total number of tokens is two. That's why we do take one over two, and the same for each one. So we compute a TF. This is the first system. And then, we compute, based on the GF, we started to compute IDF for each step. Okay? So, IDF comes from, if we have, the number of document is four, okay? So for the blue, I compute log of 10, 4 divided by, how many times the blue appeared in the whole document. So, if you go back to this, it just repeated one time, the total appearance of, no, the whole document, just one time. Okay? Then... So, this is the not, and so on, for... Once we created IDF for each world, then we can multiply IGF here, one by H, TF, multiplied by IGF, and then we come up with... the pitch of a space T, which is awaited feature space. The implementation is very easy. We have in a psychic land, a specific function to create IDF. So, IDF, for example, I have a sentence, we have TFIDF, from the psychic learn, and then once you apply TFIDF, you can build, and this is an example if we try to get the document. So this is TFITF for each day. So there an index, I didn't create the whole document just for one sentence, so this is IDF, TFIDF, for that. This is TFITF for quick, and so on. Okay? And we can balance. Our feature space based on this. I gave you an example here of a different note, a little bit huge document, and then we try to use... So, this is an example of a set of documents. So, we have, this is a sample of document, and we have a category here, and then he can represent this in any techniques. So, I go here with cleaning, stop board, organization, punctuation, and then clean that test, and the second step is to create counter factorization. And this is a way, for countercaterization, and then we can use a feature is based here. So, it's a normal, non high operation to have to prepare your document values. Where in the column, you have to get the feature space, which is a talking, and in the room, we have the talking. And we have now a feature space for each door, we sell it. It's a normal table, as any machinery task, we can feed this model, and we can perform classification, okay? And this is based on the frequency, and we can create the same table, on the same corpus, but using that enigraph. So here, I use two, two, and this is another feature space for the same corpus, but we're using two grand here. Again, we have a feature space, and then with TGFIF, I have the same presentation. So now, if you have a Texas data. We can now understand how we can represent this data in numerical format, and if it's this model, it's this data, as a picture, space, to any... machine learning outputs. This is the standard way, for input, for any machinery embrace, except, of course, be your enemy, okay? I can apply regression, I can apply decision to be, uh, forest, yes. Okay. So, this nature, it's very sparse. This is a common. Yeah, this is a common disadvantage in all frequency days. Even I FEGF. IFGF improve the performance as we not measure the frequency of the world, we also introduce some weight for this world, how the world is important than other talking. But still, it's a sparse, the dimension based on the vocabulary size. And this is a common disadvantage. Would you be... Previously, what we did was that we would use a inverted index, we would use a dictionary to have the document ID, and from there, you get the word, the document frequencies, and the term frequencies. And that's why you don't keep the ones that are zero. Reduction technique, it means a reduction technique. Yeah, no, it's just you use a dictionary instead of a... Okay, so to reduce a dimensionality, okay. Maybe, but still, you know some information here. If you remove the seals from presentation, you use some information. Because they're not there, so there is zero. No, because here, okay, so here, here, what do you mean by here? So here is this word, okay? What you mean by zero in this documentation is not exist? Yeah. So, like, when you refer to document zero, you don't need to even refer to beans, because it would be zero. We try to generate a factors that represent some properties. The attributes on this document. So, we have a document, and then we want to take a vector space that present is attributes. So, if this world exists or not exist, it's one of the attributes. It's an important attributes in our case. Okay? In our case, it's an important argument. Okay? Right, so... remind me if it's come before 3:30, please? Because I love it. All right, so... This is an example of TFI, any question for calculation? TF IDF? We'll have to practice with this. You may find some, um, Question in your midterm related food, TF idea. And so you can work on it as a practice, you can practice with your colleagues to have, to practice, how to create IFTF IDF for this information, and this come from, in class code, to show you that we have a message in psychic lab that automatically, you can create a computing F ID. All right, so, and this is a count factorization output, a counteractorization, we have a whole number, but now, we have a balance, okay, real numbers, which will reduce a weight. So the weight is now is different. For the same document. If we just use the counterfactorization, which is count how many times this world repeated, and now we have a different presentation, based on the weight of this world. Why we use log, actually, we use log to smooth the difference between the values, okay? Because without reduce the difference between the value. Some what may be occurred, uh, like 100, so it may be 100, so its frequency access is one, but using the log, we just reduce the computation difference, which is normally, we need, uh, will cover this. So, limitation, still, we have an medication. So one of this imitation, there is more relation between a word, we deal with each word as an independent yacht. Whenever think about it, the context. Just independent one, which say it's not correct when we talk about the language. The world has a meaning when it's come in a different context, okay? But there is nothing like this. Spicy, a lot of zeal, still we have a zeal, based against exist or not exist. Lack of semantic understanding, we just compute the world as a frequency, there is no meaning at all. What represents the representation, or tobacco presentation for each word. It's the name on the frequency, no semantic meaning of this work. So, the one effect, when it's come as a financial constitution, no difference, because for pain, when it's come as a side of the right. Okay? And this is another drawer. Not well suited for a small course. So if the number of documents is small. Then IDF, the inverse IDF will mislead. You appow the importance of this world, because the total number of documents is not too big. So it may give the world a high weight, why it's not importance, okay? So don't use it when you have a small curse. And, again, out of vocabulary presentation. So, if the model didn't learn, This vocabulary, we are not able to represent. So here, if I feed the modern ways, a new document, the sky and JGG today. So, they are able to improve or update the sky, and today, but JGG, it's not in the Boca. So we cannot have a presentation for any words that it's not verified that come. Okay? So, this is a main techniques for presentation of your text based on the frequency based, which come from statistical techniques, right? It's to consider, at that time, a huge step, to perform, a lot of, and it beat us, but still, it is not, it's not good presentation for the world, it suffered from many, many disadvantages, which, in the second step, in a world embedding, they try to overcome all this disadvantage. So, it is an open research topic. Even with the use of the transformer, a large language movement, will not come with the idea presentation of We still work on this. With a transformer and a word in body, we improved, we introduce some semantic techniques, and we consider the context of the work, but still, it is not perfect, to still work on it. So it's still a hot topic of research, okay? Now, I will jump between the similarity. What is a ticket dissimilarity? Take a similarity, actually, it's considered as a pinch marked test. To measure the performance of any text presentation. And they tell you, tell you, tell now. If you have a mesod two, take this presentation, and you have another basis for text presentation. How we compare them? We compare them using a text similarity technique, okay? And I will show you how we use. So what is it, take a similarity, it's just a method or computational technique, to... Measure that a degree of similarity between any work or even any documentation, okay? So, the application, many, many applications, and speech recognition, machine transformation. So, this is to improve such a formance of any of this technique, using pathic similarity. It's a pain marketist, right? Right. So what is the measurements? How we have many techniques to measure the similarity between human sentence. We have jacord similarity, providing similarity, gladiant distance, lendency, distance, humming distance. Do you remember these techniques? You can understand. Last three, we measures the similarity, yes? So, we, we may have, did you go through this, and you guys, what is the designs? I think, yes? Because I seem that to use the a machine in natural language processing. So, but I will give you some information about this notable, but I will give you some information. Let me see, let me see, it's only the key information which have been actually between 2 words based on counting the number of this specific operations. How many delations, how many insertions, how many mutations, you should, you apply, to convert, is this work, or this work? Okay, so this is what we, this is the living scene measurement. So, this number, which will present how many dilations, how many insertions, or how many change, it's a measure of the similarities between the two worlds. Okay? So, for example, if you start with a kitten, and I want to find the similarity between kitten and the setting. So we just account how many? This is evocation. So starting with a kettle, I can change K to S. So this is one operation. And then take the setting, I can, a setting, so what are you changing here? I change the eye, to E to PI, one battery change. And then starting from this, I can add J at the end, this is an end session. So the distance, living scene, lesson, between this two, what is three? And it's used before as one of the similarity measurement techniques. And then the gradient distance, and I think, okay, if you have a two point in, vector space, you can measure the distance between them with this specific equation, collision, this square root of x1, so some measure of x1 minus 1. Okay? And this is used as also one of the measuring similarity between two vector space. Okay? So if a world X presented as a vector space, and worldwide presented as a vector space, we can easily apply a granging distance to get the difference of similar, how much similar, these two words. Okay? And up to the technique, which is used now, and MFT, is a cozign similarity. So the difference between a Canadian distance, and the design similarity, a Canadian distance, doesn't take in its account the direction of the vector. We just measure these on, the magnitude, okay? No direction here, in the equation, right? But in a cozain similarity, which is used enough, So, the magnitude, plus third direction. So, the equation is, A, B, A, that product, got in an inner product, and then, divided by north of eight. You know, the equation of the north, and a little north. So, the idea comes from Husain is, if the world is two words are too similar. So, the angle between the world will be small. So, what is the Husain 0? So, one, it means two words are identical. And if the two worlds are far from together, there is an angle of mighty, between these two worlds, what is the cause of 90, zero. So it's a value of cosayen. One that the ID value is one means that two words are identical, and it's come to less if the two words are not identical, and sometimes come to minus one, okay? And this is the behaviour of the coupon. Okay, the cosine, the cosine function. Right, so, if how you compute it, the first step can make a bit, you work on a vector space. For each sentence, you can get a vector for this sentence. For each document, we can get a vector for this document. And then once we can get a vector, space, then we can apply our function. So, A will be, and a product of A will be, and the angle, we accumulate the cosine, and the other side, so it's A will be divided by normal A, and then by by normal. So based on this equation. So the value will be 0.667, which is considered the height, as the similarity between this document. It's not ideal, but there is a similarity. Okay? How much about the similarity? It's based on how much you got the fact of presentation. And that's why we use the Posaiian similarity, to measure the performance of the presentation mode. When we have a text presentation technique, for the world, and we get that text presentation technique of the other world, and then we can measure the similarity.. If the two words, we know they are identical or very near, and we get high measure of the similarity means this method is a good mess of for presenting the world. If we get a less measure, it means it's not a good position, okay? So this is how why we use it as a benchmark, to measure the performance of each techest presentation technique. Right? Okay, so the computation is very easy. Using any technique, by the way, you can use back of phone, county frequency, one hot encoding, OTF, ITF. And then, we can check, compute the posyte simulate. So, as an example, this is another example here, just for your computation, here, I just use a point, uh, factor, pointization, counter, how many times this work repeated, then we can measure that obey similarity between any document. Uh, give you an example, poster, like, something to practice with, such calculation, and this is another example to practice. If you have any questions you ask them asking me, next selection. So, try to practice, this is again source of the question in your intern. Right. As I mentioned, we use, uh, take similarity to measure that, performance between different presentation. And during this lecture, we have different presentation, we have hot encoding, and we have bag of water, bag and 04, and also TFIGF, okay? And I gave you, in this example, how the different presentation affected the similarity between the world. And this example, it's a part of your ink gospel. So I implemented this example in what things must. So consider we have this documents, and then we need to measure the similarity, which, which is similarity between appear wise sentence in this document, okay? So we use account factorization at the beginning, based on current factorization. You see, the highest similarity is between the ways that is home under the sun, one hot in code. This has got the higher similarity family. So, this two sentences are similar, and then the next step, one hot encoding, which this is a hot cell today, maybe because it's hot appear in the two sentences, that's why the similarity is hot. This is using counteractorization technique to represent each sentence, okay? Now, if I change it to TFIDF. I represent each, send this, we see F, I, D, F, and I measured there, design similarity, pay, wide designs, and it, because I, it, it. Okay? So, the highest, one is, I make my hot chocolate with men, and I will have child appear with men, which one is mean for me. The counteracterization, which gets these two sentences, are very similar, or this which TF IDF, which get these two sentences are set. So which better presentation in your life? Which you get you a better presentation, so the similarity is different. This is a logic, yes? It's logic, because they are in the same, at least the context, okay? So, here, the hot chocolate with China, there, many. So, it is a leather and pet, more logic. So, the presentation here is, this is how we can use acosine similarity, as a pension mark for any different reason, and we will continue with the cosine similarity next lecture. When we talk about the embedding technique, we will use now the benefit of the neural network. So, instead of frequency test, when we rely on statistical technique, to have a text presentation in world embedding, which is a huge improvement, and I think its presentation, we will use a. Regenerate, effective, based on learning the costs, okay, between awards, okay? This is why I will explain an expiration. Right? Any question, then? Now we have seven minutes before we start our activity for this week. What is, what are the ethical concerns related to the frequency? This, this, this. Now, we try to compile our ethical course, ways and... Do you consider there is any ethical concern? I mean, we use frequency based presentation. any property? Yes. Potential failure to not deal with context and meaning may create situations. Like, for example, with text comparison, you may have two texts that may feel, that may be represented similarly, but have complete communings. But this is is our concern. It could potentially create situations where you have, say, for example, somebody's cheating, if you have a conception where you try to compare the similarity. You have a similar document, a foundation, you have conception. We have an issue where there looks similar by the conception of this is a model, but it foundationally has different or... Yeah, this, yeah, this may be. Okay. Yeah, I't know. Since frequency base checks would prioritize higher frequent words, right? So that would create a term bias about words or topics or even, um, It could also come into social dimensions of, like, what is being talked about, gets more priority and that could have downstream effects of. Perfect. Yeah. This is, yeah, this is one of the ethical consent by swords, for the topic that is common. And what about the minority dogs? Okay? So the frequency, it's not good presentation, it reduced some vise for the minority topics, and less important topics. So many people now, you know, the important topics may be considered as a minority, using a social media, that people talk about specific things, but ignore maybe scientific or deep knowledge topics. So, based on the frequency, we consider these topics as... So, it's not a good presentation for this, it will reduce some poise. Another option, another idea of bias here. What about languages? If it's kind of internet, how many document written in English? How many document written in a different language? So frequency based, introduce abides in this case, related to the types of the languages, okay? So, the topics, the language, the representation, concern, and some ethical concept, to represent the minority public or minority languages. Okay? Any question for the Texas presentation? Now we have an idea how we can feed that takes us to your machine learning algorithms, okay? We can prepare a table with a decision, and then we can make an input for your, prepare our feature space, feed it to your model, and then we can perform any machine learning task. Classification, clustering, or anything, because this is a starting point. How we present this a text, okay? Till now, it is not good representation, it suffers from many limitations, but still, we are able to do this. And still, it's a good way to represent a specific task, for a specific task. So, for example, while embedding, it's a good presentation for your tech, but use a neural network. Why the neural network introduce some computational challenge. But for a simple classification task, TFIDF may work very well. For other NLP task. Information retriever. TFIDF work well. Okay? So now it's based on your task. If the semantic meaning and the context is important, then this is not a good way to represent your data. Why, if the context and the frequency, It's more important, in this case, takes the frequency based, the method, it's a good choice, okay? Not because it has many limitations, it means that it will not work with any NLP task, no, still, they use it, but think about your task, which method is good for your task, and the trade off between accuracy and the computation cost, okay? Right? So, I think it's open now.