Today we talk about word embedding, which is a very, very important concept in natural language processing. It is a building block for any application in natural language processing, and even all the improvements introduced using the Transformer are based on the idea of word embedding.

In this lecture, we talk about specific techniques in word embedding: Skip-gram, Continuous Bag of Words, Word2Vec, FastText, and the most recent technique, which is GloVe.

Last lecture, we started with feature engineering. We have some statistical techniques to represent the data as a vector in a space, and then we feed this vector as a feature engineering vector and apply any model. Today, we have a different approach.

This is just an overview about the techniques we have. As we classified before, we have three categories: statistical-based, which we covered last time, including one-hot encoding, TF, and TF-IDF. And we have a prediction-based representation, which we call word embedding. This will be the topic of our lecture today.

Now look at this illustration of the three types of unstructured data. For the image, we have a specific mathematical model to represent this image, which is a matrix, where each entry in the matrix has a value, and this value represents some feature related to the image. A simple feature is the intensity of the image pixels. And then even with the audio and speech, we can represent it as a Fourier transform, wavelet transform, or any transformation technique where we have an amplitude that represents each signal value. So each point has a value. And then, based on the previous lecture, we have now a representation for the word as a vector in space.

The main difference here is that we consider it as a dense vector. What we mean by dense vector is that it is the opposite of a sparse vector. In a sparse vector, there are a lot of zeros. But here, in an image, we do not have zeros; each point has a specific intensity and a value. And the same here for the speech: each signal has a specific value. And then, in the previous representation of the word as a vector, it is a sparse vector. So we need another way to convert or to represent this word as a dense vector, where each point or each value in this vector has a value, no zeros. This is what we mean by a dense vector: no zeros in this vector. And this is the aim of our lecture today, to come up with a technique to convert or to represent the word vector as a dense vector.

Now, just go back and check what were the limitations of the text representation, the count-based text representation. One of the limitations is high dimensionality. The vector dimension equals the vocabulary size. If you have a corpus with one million words in the vocabulary, then the vector for each word has a dimension of one million. And the other one is a sparse vector, where we have a lot of zeros. And we are not able to deal with out-of-vocabulary words. So if you train your vectorization for a specific word or a specific vocabulary, now during testing you may have a new word that you have never seen. How do the frequency-based techniques deal with out-of-vocabulary? They just ignore it, as we saw in the in-class code in the previous lecture.

And the main point here, which is very, very important, and this is the point that led people to think about a different representation of the word: lack of semantic meaning. What do we mean by this? A word is not just a string; it has a meaning. So each word in any language has a specific meaning. If you remember, in the previous technique, we did not consider the meaning, we just worked on the frequency, even in TF and TF-IDF. We just look at the frequency of the word; we have a lack of any meaning. Why in text representation should we introduce the meaning of the word? This completely changes the representation. The word itself has a meaning, and the word has a meaning when it comes in a specific context.

So in this lecture, we try to capture these two features and then create our embedding vector. The embedding vector captures two features: the word's semantics, the meaning of the word, and the relation of the word in the context when it comes in a specific context. And this is our aim, and we will try to see how the different techniques try to get rid of all these limitations.

Of course, if we have these limitations and we use this technique in any NLP task, the result will be poor performance in most NLP tasks. While it still has good performance in other tasks, in tasks where we do not rely on semantics, the count-based technique is preferred because it is easy to implement. But with some computational considerations, the majority of NLP tasks suffer with such a representation of the word, which is based on frequency.

As I mentioned, a word is not just a string; it has a meaning. And if you think about it, if I told you the word "king," what will come to your mind? Regent? Royal, princess, queen. So each word has a relation to different words in the same field. Imagine the word "restaurant." This word has a meaning when it comes in a specific field or in a specific context. While the meaning of each word is completely different, when these words come in this specific domain, they are related to each other. So this is important: we try to find a relation between the word and its surrounding words in a specific domain.

One way of getting the value of word similarity is to ask humans for similar words. You remember, at the beginning, when I drew a table, when we get some experts, and these experts describe each word as a set of features, zeros and ones. The idea here is similar.

And this is a first trial to introduce a meaning for each word: WordNet, which is already implemented in NLTK and most of the text processing packages like spaCy and all these tools. It is an API. And when you just write the word, it gives you all the information about this word: what type, definition, and more. So the idea of WordNet is that it is a network of words. As you see here, it is a network. They group each word in a set of synsets of this word. Synset means a set of synonyms of this word. And also included in WordNet, they define the relation between different words in different formats. So this word is a part of this word, this word is a synonym for this word, this word is an opposite of this word. They give you all the information related to this specific word, plus something we call a gloss, which is a definition of this word, and an example of where this word can be used. So you can refer to this API, which was invented by Princeton University.

And this is them, I will not ask you to remember all these things, but this is the different semantic information that WordNet can produce for any word. This is something related to the semantic information.

So how can we use WordNet to have a dense vector, for example, or in a text representation, while it is a lexical database? It is a network of words. It is not related to numbers. I do not have a vector representation in WordNet, just a network of words and the relations between the words. This is what is included in WordNet. But still, they try to use WordNet to introduce some semantic information to the vector.

One of these trials, as you can see here, they can get all the examples from WordNet because WordNet also has some examples for each word. So they try to get all the examples related to this word, all these words that come in a different context, and then make some text processing, identify synonyms, and so on. And then they compute an embedding. It may not be an embedding technique per se; maybe they use any technique and average all these words that may represent the actual semantics of the word, because these words come in the same context. And this is always the first try. And then when you average all the words which come in the same context, you have a vector that somehow introduces some semantic meaning to the word. And this is the first attempt.

Now, there are some limitations in this technique. The biggest limitation is that it is not computational at all. It is not a mathematical model that can produce a vector with some semantic meaning. This is one of the main disadvantages or limitations. The other one is limited coverage and it is static. It is not updated dynamically, and not every language is available. Every day we have new words, new language, and this is part of language. So it is not up to date with some language features, and maybe for specific terminology related to scientific domains, you may not find it in WordNet. Language limitation: it will not be applied broadly. WordNet is just for the English language; there is no equivalent WordNet for other languages. While there are some trials, they are not at the same level as WordNet.

And then they built it by experts in the language. So this is how WordNet was created. They brought a group of experts in the language, and then they defined the words, examples of the words, and relations of each word. This is done by humans. And what is the risk of this? It may introduce bias in the language. And this is the main disadvantage. I insisted on putting this to give you an idea. We are still able to use WordNet now. In recent applications, we are not using it as a standalone. We use it as an integration to another NLP system to improve its performance.

Look at this example of how to use WordNet. You search for a specific term in a question answering system or even a document retrieval system. You write a query, and then this is a new user query. You take this query and search in the database to extract the documents that relate to this query. The question answering system: the user writes a question, and then using a specific key and a specific database, it searches for the most relevant documents to this question.

How do we introduce WordNet to improve this? One of the ways is query expansion. When we write a query, we can identify the keywords of this query. And then we can go to WordNet and get all the synonyms related to these words. What is the benefit of this? So you search for "car." They bring all the related words: motor, mobile, automobile. So what is the benefit? You search only for "car," that is your search query. But using WordNet, you are able to get results that are very close and related, but not exactly "car." Yes, but at the same time, you expand the query. You expand your range. So if you search for "car," it will not give you only the exact documents related to "car," but "car" is related to other words, so it is a synonym. It gets all the documents related to the synonyms. So it broadens the range of your extraction. And also, it is applied not just for query expansion but also in question answering systems. When you write a question, the system extracts the most relevant documents to this question. So if you expand the range of the user's question, in this case you may access documents related to your question. So just to tell you, we can still use WordNet to improve some specific NLP systems. But again, we do not use it as a standalone; we use it as an integration to another NLP system, and it just improves a bit.

WordNet is not a good way because it is not a computational model and it has a lot of limitations. So the researchers think about how we can build the system based on these two specific phenomena: distributional similarity and the distributional hypothesis. So what is distributional similarity? The meaning of a word can be understood from its context. Is that true? Maybe I can find and understand the meaning of the word, but the context plays a very important role in understanding the meaning of the word. The other thing is the distributional hypothesis: words that appear in similar contexts are similar. There is some similarity between these words. Like this example: all the words related to the restaurant, all the words related to the house, they have some similarity. And these are the two hypotheses that became the starting point to have what we now call word embedding.

So what is word embedding, or vector semantics? Semantics means it has some semantic meaning. It is a computational model for representing a word based on these two phenomena: distributional similarity and the distributional hypothesis.

So this is the idea. The general idea is that we have a set of words and we have huge corpus resources. We try to build models that learn and predict, not count. So the idea here is prediction. We can create the vector, or the embedding vector, or representation vector of the word, based on learning. The word embedding comes from learning from huge corpora of data. Gather all these huge corpora of data, and then using a neural network, we can have an embedding vector for each word that captures all the information: the meaning of the word and also the context of this word. And then we have this vector, which will be the feature vector for any NLP task. So we have now a new technique to create a feature vector. And the most important property of this vector is that it is a dense vector.

So we can choose the dimension for the representation vector. Any word in my corpus can have a vector of 100 dimensions. I can specify the dimension D. So this produces a dense vector representation because I specify the dimension. It is not related to the vocabulary size. As you see, it is a function: this is the vector space and this is your vocabulary. And I will map each word in this vocabulary to a vector of dimension D. So D maybe 100, 300, or 200.

Is it just trial and error? Yes, and it is a challenge. But in most of the pre-trained models that use this technique, like Word2Vec or FastText, they provide us with different values of D. So D may be 50, sometimes they have 100, 200, or 300. Based on your computation and desired performance, you can choose your dimension. But more dimensions are supposed to have more features. So 300 is not like 100: it captures more features from the word. So doing this, and if we are able to represent any word as an embedding vector, it will enable us to do what we call word analogy: find the similarities between the words.

For example, if I represent the word in a specific vector and then I plot this point in a two, three, or four dimension space, or the dimension of the vector, now if I represent "king" as a point in the space, as a vector, and "queen" as a point in the space, if your embedding representation actually reflects the semantic meaning of the word, then you can expect that the distance between "king" and "queen" should be equal to the distance between "man" and "woman." So we can predict the values of a new word. The common technique here is: the difference between "king" and "man" should be equal to the difference between "queen" and "woman," and this captures the semantic meaning. So if we have this, we can conclude: "king" minus "man" plus "woman" equals "queen." So we can predict any words.

The same for other examples. If you know the distance between "walk" and "walking," you can expect and predict that this is the same distance between "swim" and "swimming." If you know the relation between a country and its capital, then the distance between "Spain" and "Madrid" should be equal to the distance between "Italy" and "Rome," as they are in the same semantic space. And using the embedding representation, you are able to make some mathematical operations and predict a new word vector.

For the analogy and the similarity between the words, it is one of the techniques that measures the performance of the embedding. So if someone comes up with a new embedding technique, how can we measure its quality? Based on how much your representation captures the similarities between words that come in the same context. And this is a very important use of word embedding evaluation. We can measure the performance of any word embedding technique based on how well the embedding captures the semantic meaning of the words.

So the key insight is that we will not use a statistical model, we will use a neural network. We will learn rather than count, or predict rather than count. The main idea here is self-supervision. What do we mean by self-supervision? In machine learning tasks we have two main tasks: classification and regression. Now, for classification, it is supervised. What do you mean by supervised? We should have a label. You have data and then we have a label. But what do you mean by self-supervised? In a supervised setting, someone should annotate your labels. When you gather the data, whether it is text or even images, you need a human to annotate the labels. In self-supervision, you will not need anyone to annotate your data. How? The data itself provides the annotation. The annotation comes from the text itself. So this is the main feature of self-supervision.

Self-supervision and predicting rather than counting are the main features of Word2Vec. The code is available, you will not write the code; it is available everywhere, and it is very fast to train. This is one of the most popular embedding techniques.

Which technique will we apply, and how do we apply self-supervision? This is Word2Vec. The embedding vector in Word2Vec is a combination of two common techniques. One of these techniques is CBOW, which is Continuous Bag of Words, and the other is Skip-gram. Continuous Bag of Words and Skip-gram use the same idea: self-supervision and prediction, meaning we will use a neural network.

Now, let us talk about the first one: Continuous Bag of Words. Say you have a text. This is a corpus. I will use a sliding window starting from the beginning of your text. Then, in Continuous Bag of Words, we identify the window size; maybe I take the window size of three. So I will take the first three words in my text. The surrounding words will be the context, and the middle word will be the target. So I try to predict the middle word based on the surroundings. And this is the idea of self-supervision: no one will annotate; the annotation comes from the text itself.

And this is Continuous Bag of Words. Specify the window size. The size should be 2K plus 1; it should be odd. Why? Because we have to predict the middle word. So specify the size of your window, and then slide this window starting from the first word. This will be repeated for all your corpus. And this is the neural network: just one hidden layer. So after I finish this learning process, what is the embedding vector? The embedding vector is the hidden layer matrix. The hidden layer matrix is the embedding output from this process. It is a matrix, and the dimension of the matrix is V by D, where V is the size of the vocabulary and D is the dimension that I specified, maybe 100, maybe 200, maybe 50, whatever; it is my choice. So this hidden layer weight matrix will be considered as the embedding vector for each word. Each word has its own dense representation as a row in this matrix.

And this is an example. For example, I can use a specific word, like "lamb," and then the output of the vector is, as you see here, a dense vector based on the dimensionality that I specified, where this represents the semantic meaning of that word. Why? Because I always predict a word related to the words around it, which are its context. And maybe this word is repeated many times in your corpus. So the vector for each iteration would be updated. The hidden layer vector will be updated every time when the word comes in a specific context.

I will try this, and this lecture is not focused on mathematical details, but what I want is for you to understand the idea. If you want to go to the mathematical details, I will give you a reference for more mathematical material on how they calculate it exactly. I will let you understand the idea behind each technique, and then there is code. There are functions in our library that implement all these things.

So this is the idea of Continuous Bag of Words: predict the middle word based on its surrounding words. Here is an example showing your training samples. For each sliding window, this is the training sample. The context words surround the target, and you try to predict the target. So this is your training sample for each window. Then we slide the window one step ahead, and then we repeat this process for all your corpus.

And this is the structure. As you see, it is a very simple neural network, just one hidden layer. So the input gives a vector that represents each word, and then we have the context, and we try to predict the target. Now, how do we represent the input in this architecture? What is the form of the input? It is one-hot encoding for each word, where there is a one in one position and zeros in all other positions. Why? Because if we multiply this input layer with the hidden layer, which is a matrix, it produces just one row.

For the hidden layer, this matrix, which is the weight matrix, at the beginning, what is the value? At the beginning, it is random. Initially, the weight matrix has random values. And every time, we update these values. So we initialize this matrix with random values. Each word has a vector, and at the beginning, it is random. And then in backpropagation, what happens? We update the weights. Based on computing the difference, you have an activation function which produces a probability: what is the probability that the predicted word is the target word. Based on computing the difference between the predicted and the actual probability, backpropagation goes back and updates the weights. This is how the neural network works.

So we input all the context words as input, each word represented as a one-hot encoding, where there is only one "1." And based on the matrix multiplication, we can compute the result. Let me explain: if you have three context words, for example, and this is the one-hot encoding for each word, and this is the hidden layer weight, which is a matrix. What is the dimension of this matrix? It is the vocabulary size by D, our chosen dimension. So what is D in our case? Say I choose four dimensions. So if you multiply the first context word with this matrix, what will be the result? The row where there is a one in this matrix. So what happens with the second one? The result is the corresponding row.

And then there is a nonlinearity here where they average, so they compute the average of all these weight vectors, go to a softmax layer to convert it to probabilities, to be able to compute the difference between the probability output from the softmax and the actual value, and then we try to minimize this difference. After finishing all this process, this hidden layer matrix will be the embedding vector for our corpus. And it has semantic meaning: it captures the relation between the word and its surrounding words, based on how much text and what is the size of your corpus. The more, the better. The more corpus you give your model, the more opportunity to learn from the text.

And then we have Skip-gram. Skip-gram is the same idea. It uses self-supervision, but with a little bit of a twist. What is the difference? Instead of inputting the context words and trying to predict the target, we will input the target word and try to predict the context. The objective here is to minimize the difference between the sum of the errors of the context words and the actual values. So this is the idea: predict the context words given the middle word.

Again, we have a window with a specific window size, and we slide this window through your text. Under each window, we will try to predict the context. If our target here is a specific word, then I build my training samples. So this is the whole text, and this is my window. I want to predict the context, the surrounding words. So I consider the middle word, and then the words before and after it.

And maybe you may have this experience in machine learning: how do we deal with the situation where the window is at the boundary and the size is not full? For any neural network, the input size should be the same every time. So how do we deal with this situation where the input size is not the same? We use padding, or a zero vector, and then we consider that as context with an empty previous or next word. So for the boundary case, we use the central word to predict the context. We will use the middle word to predict the surrounding words on each side.

So this is the idea of Skip-gram. It is the opposite of Continuous Bag of Words. Again, we use a neural network, and this is prediction, and the main objective here is to minimize the sum of the prediction error across all the context words. So again, the input layer is just the one word as one-hot encoding, and we have the weight matrix initialized randomly. And during the learning, we come up with this weight matrix as an embedding vector for each word in your vocabulary.

And I found this resource, just yesterday when I was updating my lectures. I like it because it has a numerical example, so you can imagine what happens in Skip-gram, and you can refer to this resource because it actually computes step by step, mathematically. So if you are strong in math and you want to learn more about how this neural network works, I recommend going to this reference; it gives you information on how Skip-gram works.

And this is an example, an animation, of how Skip-gram works. We start with the first word. So as you see here, the first word has no previous context. What then? We have a dummy vector or padding vector. This is the start. And then as we go, step by step, we slide the window to the next word, then we choose the next window, and so on. Every time, you create your data sample for each window.

So now we have two techniques for Word2Vec: Skip-gram and Continuous Bag of Words. So which one do we use? You have to know what is the main difference between them. Continuous Bag of Words is considered to be faster than Skip-gram, and it is better when we have some words that have high frequency. Skip-gram is slower, but works well with smaller data sizes. Continuous Bag of Words is easier for classification than Skip-gram. So when you want to choose which one to use as an embedding technique, keep this information in mind. It is based on your task: how you can represent your task, and then based on this you can choose which one to use. This is based on the published research paper.

Then in some work, a group of researchers published a paper with another technique: we call it Skip-gram with Negative Sampling, abbreviated SGNS. So it is the same as Skip-gram. Using the middle word, we try to predict the context, but with another difference. Why would we not let the model learn more by introducing positive and negative samples? So we let the model see the context words, which are positive samples around the middle word. And also, we give the model negative samples, which are random words not related to the middle word. And we let the model learn from both: this is a positive sample, and this is a negative sample. So the model learns more information.

The target here is to maximize the similarity between the target word and its context (the positive samples) and minimize the similarity between the target word and the negative samples, which are not in its context. So for example, if you have this training sample, we use a specific word as the target. This is a list of positive samples: all the words that come in the context of the target word. And at the same time, we give the model negative samples. We have to indicate to the model: this is a positive sample and this is a negative sample. Maximize the similarity for the positive samples and minimize the similarity for the negative samples. This makes the model learn more about your words and adds more semantic information.

Google used this technique. They published the Word2Vec model in 2013 using Skip-gram with Negative Sampling.

So what is the benefit of this? We will not need to train our own model. Word2Vec from Google does the job for us. They gathered a lot of data, actually, I cannot remember how many, three million vocabulary words or more, coming from Google News and different resources. And they provide us with ready, pre-trained model embeddings for all these words. One of the dimensions is 50, and another is 300. So you will not go through all the training process. Read the corpus and do all the computational process to have the weight matrix. The weight matrix comes to you already with the pre-trained model. One of them is Word2Vec from Google. You have the list of words coming from a huge corpus of data, and each word has its own embedding.

And all we need is, if you want to use this in your NLP task, you can have a feature vector for each word coming from Word2Vec, and this is your feature space. If you do classification, sentiment analysis, or whatever, then you have a table where you have text and your label (positive, negative, etc.). How do you represent this data? You tokenize your input, and you grab the embedding representation from Word2Vec. If you have a sentence, you can average all the vectors, and then you have a feature embedding representation of this text.

So here is what you get from Word2Vec. I will provide you with this. It is an API; you can use the Gensim API. As you can see, I can load it. With Gensim, I have to install it and then import it, and then you can load whichever model you choose. I choose the 300-dimension model, and then you can just print the embedding vector for any word. So the model name is word2vec-google-news-300. As you see, it is a vector of dimension 300, a dense vector of 300 dimensions that represents the semantic features of a word in this huge, three-million-word vocabulary.

And the same for any word, and with this we can find the similarities. For example, here I try to find the similarity between words, so I get the vector of a specific word and then I can compute the cosine similarity to see how much the word embedding is successful in reflecting the semantic meaning of these two words. So here, for example, I try to get all the similar words to "cup," and I specify the top three. So the top three are "love," "mom," and "loft." And this is the way how we can measure the performance of any embedding technique.

The common one is Word2Vec from Google, published in 2013, and it was the first pre-trained model that came to us. But there are two other models: FastText, which comes from Facebook, is a special variant of Word2Vec with some differences. And you also have GloVe, which is completely different. GloVe comes from Stanford University. Word2Vec and FastText were actually created by the same researcher.

So this is for Word2Vec. You just install the Gensim package, and then you can choose which dimension and which model you want to load, then you can represent any word based on this. You will not train from the beginning; you will use a pre-trained model. And as I mentioned, the dimension is a challenge. Based on your computational resources and the criticality of your application, you can choose which dimension. It is a challenge to choose the perfect dimension.

Now, you also have two choices here. If you want to use Word2Vec to train your data from scratch, that is one option. Or you use the pre-trained model and then fine-tune using your own data. So if you choose to train your model based on your own corpus, you can specify the model like this. This is your data or your corpus, and this is the minimum count parameter. This option relates to which words you want to include in the training. Sometimes, if the word is not repeated more than one time, we may delete it and not include it. So this is what we mean by minimum count. Minimum count of one means I include all the words. You can change it to two, so the word must appear at least two times to be included in your training.

And then here, we have the vector size: I can specify my own size. And then the window size, which is one of the parameters, is the context window. What is the size of your context window? You have to specify. It is one of the hyperparameters. Someone may say, "I can increase the window size so I can capture more information about the context." So it is one of the hyperparameters to choose your window size.

And then this parameter is very important: sg. So sg, as I mentioned, Word2Vec has two implementations, Continuous Bag of Words and Skip-gram. If sg equals zero, the model applied is Continuous Bag of Words. If sg equals one, then you will apply Skip-gram. So this parameter identifies which technique you use.

And then we have negative equals five. So if you use Skip-gram, then you may use Skip-gram with Negative Sampling. If this value is greater than zero, you can add this option to add negative samples in your training. But if you use this to be zero, this will not be used, because Continuous Bag of Words does not use negative sampling.

And you can use this very simple model to create your own embedding vector based on your own training data. In this case, you will not use the pre-trained model from Google.

In the in-class code, I give you some examples. I gave you an example of how to use WordNet. For example, I choose the word "car," and then I print all the information related to the word "car": synset name, part of speech, definition, which has some examples, and this is a list of all linguistic features that WordNet can print for "car." So as we see, this is the name, and this is the definition of the car, and they give you here some examples to understand its meaning, some synsets like "motor vehicle," and we have another list of other synsets with synonyms toward "car" and all these relations and information about it. And again, it can play an important role to improve many NLP applications by integrating WordNet into your application.

This is an example of universal word embedding where they use spaCy. Actually, it does not use a Transformer, but spaCy uses a CNN. Without going into detail, with spaCy we can have a vector for each word, a pre-trained vector. So as you see here, the vocabulary size is this number, and this is the vector dimension in spaCy for the large model. And we can create any vector and retrieve the values of any vector.

So how do we deal with out-of-vocabulary words using, for example, spaCy? I tried to make this experiment to see if I feed spaCy with out-of-vocabulary words. SpaCy can deal with out-of-vocabulary. As you can see here, I gave it "NLPprofessor" as one word, which is not an English word, and the output here is a list of zeros. So still, we suffer from out-of-vocabulary issues because the pre-trained model was trained on a specific corpus, three million words or maybe more, but still, if the model has not seen this word before, it does not have a representation for it.

Still, we improve the text representation by introducing some semantic information, and we improve it by choosing a dense vector with a specific dimensionality, but still out-of-vocabulary is still there. We have another way to deal with this, but I will try to see here how spaCy is able to deal with this case. And I can find the similarity. For example, here I try to find the similarity between "I love school" and "I hate school." As you see, the similarity is very high because they share similar structure.

And word embedding using Gensim: here I create my own corpus, a very simple corpus, and then I go through all the preprocessing, tokenization, removing punctuation, and all these things. Then I use the Word2Vec model with the same parameters I explained. And this is how I can train my Word2Vec model with my own data. And this is how we deal with it. And I have also another example here: I try visualization, how to represent each word in a two-dimensional space, compute word similarity, and then this is an example of using Word2Vec from Google as a pre-trained model.

So as you see, the implementation is there; you will not write anything from scratch. All you need is to understand what you are applying. And this is a very important factor. The code now is available in libraries that implement a lot of techniques; you do not have to implement anything from scratch, but you still should understand what is going on. You have to understand the idea and how things work behind each technique.

Now, the second one is GloVe, which stands for Global Vectors for Word Representation. It is an unsupervised learning model. A group of researchers from Stanford University invented GloVe.

So what is the idea? GloVe is different from Word2Vec. The researchers from Stanford thought about this: using Word2Vec, we have some semantic meaning related to any word and its context. But they observed very important information: the global co-occurrence of words inside your corpus. How many times does this word appear in your corpus? And how often do these words appear together? This is statistical information, but it may enhance the embedding vector. So GloVe will combine the previous technique, which is Word2Vec (learning-based), and add to this some statistical global co-occurrence information of each word inside the corpus, which may improve the performance of the word embedding. And this is the idea of GloVe.

So the first step is to construct the co-occurrence matrix. And then we process the data. We compute the co-occurrence matrix. What is a co-occurrence matrix? You have a list of words, you choose a window size, and how many times does each pair of words appear together? It is a matrix, as you see. How many times in our corpus do these words co-occur? For the whole corpus, this is the co-occurrence matrix.

The idea comes from this: if you have word X1 with a specific vector representation, and another word X2 with its own vector representation, what about the dot product between X1 and X2? What does this value represent? The dot product between two vectors. What does the dot product represent in a semantic point of view between two words? It represents how similar they are.

Now, they found that the dot product between two word vectors approximately equals the log of the co-occurrence value between X1 and X2. So if the dot product is high, it means there is high co-occurrence; X1 and X2 frequently appear together. So there is a kind of similarity. These two words have something important in common. And they try to minimize the difference between the dot product and the log of the co-occurrence value for each word pair. And this is the idea of GloVe.

It is based on the co-occurrence matrix, because they mentioned that the co-occurrence matrix holds more information about the global structure or global syntax in your corpus. Statistical or frequency information still has importance that we have to consider. How many times does this word come with this word? This has some semantic meaning, and we can introduce it with GloVe.

So these are the steps to implement GloVe: construct a co-occurrence matrix, then use the co-occurrence matrix to compute the word embedding using the GloVe objective, and they try to minimize the error between the actual dot product and the log of the co-occurrence value. And this is what GloVe does.

GloVe also comes with different sizes, and actually it is trained with different resources. Here, GloVe is trained with Wikipedia, a web crawler, and many other resources. We have 25D, 50D, 100D, 200D, and 300D. And you can choose. By the way, the GloVe embeddings come as a text file. They trained it with a huge dataset with different dimensions, and then you can choose which text file to use. You will not train the model from scratch, and all you have to do is to search for your word and extract the vector representation. Each word has its huge vector. And again, it is text. And you have to know this is a text file, because when you use GloVe in an NLP application, you need to know how to deal with this text file to extract your word and extract the vector related to this word.

And this is the idea of GloVe. It is a very fantastic idea that improves the performance. Based on all the experiments published in different papers, they improve the performance of embedding tasks because we add more features, which is the co-occurrence information.

Now, we still have a problem, which is dealing with out-of-vocabulary words. With a pre-trained model, they trained on specific data. If in your data there is a new word that is not included in the pre-trained set, what will happen? You will not have a representation for this word.

We have three techniques. As you see in spaCy, they use a default vector, which is a vector of zeros or any default values. So the first technique is to use a default vector: whenever you cannot find an equivalent vector for your word, use a default vector. This is one technique. The second is to fall back to a similar word: use WordNet, find a synonym for your word, and you may find one of the synonyms already included in the pre-trained model. Or the third technique is to train your own embedding model. You will have your corpus, and you will train your model with your own data. In this case, you make sure that every word has a vector, if you train your data from scratch.

Another, better solution is FastText. FastText is a model invented by Facebook in 2016 as an extension to Word2Vec. So it is based on Word2Vec, and you can also load it in the Gensim package.

What is the idea of FastText, and how does FastText deal with out-of-vocabulary words? They use character n-grams. So instead of representing the whole word as a single vector, I will divide the word into subword units of three to six characters, called character n-grams. And then I find a vector for each n-gram. The vector of the whole word is the sum of all the vectors of its character n-grams.

So what is the benefit of this? Imagine we have the word "cities." If I use trigrams, how can I divide it? Using trigrams, I start with the boundary marker, then C-I-T, and then I-T-I, then T-I-E, then I-E-S, and then the ending boundary. And I have a vector for each trigram.

Now, later, you have a new word that is similar, like "city." Now, using trigrams: C-I-T, I-T-Y, and the boundaries. Now, this is a new word. How am I going to represent this word? As you see, there are similar n-grams between "cities" and "city." So I am still able to produce a vector for this word approximately, because some of these n-grams are shared. For example, the trigram C-I-T appears in both words, and you may have other n-gram representations from different words as well.

So this is the idea of FastText. We are able to deal with out-of-vocabulary words with reasonable accuracy. It is not 100% accurate, but still, we may have a vector representation for this word, and this vector may be very close to the actual representation and carry some semantic meaning.

For each word, we divide it and find three-gram, four-gram, five-gram, and six-gram subword units. Based on this, we create an embedding for each n-gram, and then the word's vector is the sum of the vectors of its constituent n-grams. So we have both representations: the n-gram level and the whole word level.

And this is the advantage of FastText: it captures fine-grained details as we divide the word into its n-grams, more details about the morphology. It solves the out-of-vocabulary problem. It is open source, lightweight, and part of Gensim, and comes with an API.

So these are the famous techniques for word embedding up to 2016. Starting from 2017, the whole landscape changed. In 2017, they published the famous paper "Attention Is All You Need" about the Transformer. Starting from that, the word embedding techniques changed to be even more semantic. We will talk about it, but not now.

But these are the benefits of word embedding: dimensionality reduction, instead of a huge dimension of the vocabulary size, I now have a condensed, dense vector. Semantic meaning, not counting: each word now has a semantic meaning because we predict the word related to its context. So the context has meaning introduced to this word. And somehow, with FastText, we handle out-of-vocabulary. And we use the most important concept, which is transfer learning.

And this, of course, improves the performance of any NLP task. Now, still, we have some limitations. It does not solve all the problems. One of these limitations is limited context sensitivity. Still, you will not capture all the context of the word. It is based on your window size. Maybe the window size should be larger, but still, we do not cover all the context information. We need more improvement.

Bias: we train on data from the internet, and that may introduce bias in your representation. As you will see in a later lecture, word embeddings can introduce bias in word similarity. For example, "doctor" is always similar to "man," but the similarity between "doctor" and "woman" is not the same. This is a type of bias.

Limited semantic information: because we still need more information to completely represent the meaning of each word, including lexical, syntactic, and semantic information. All this information is not fully captured.

Dimensionality is a challenge: do you represent in 300, 100, 500, or 50? It is a challenge to choose your best dimension for your representation. And resources: you need a lot of resources to train this. Training your model requires a lot of huge data from different resources and cleaning.

And the out-of-vocabulary issue is not handled correctly 100%. In FastText, we still have an approximation, not exact.

So the solution is a contextualized text representation, which is based on the Transformer. We will cover BERT, and we will see how this is a huge step to create embeddings. BERT, based on the Transformer, creates an embedding vector in a much smarter way that reflects the semantics, the context, and overcomes the limitations of word embedding.

How to evaluate word embeddings? I found this paper very interesting. It is published here as a reference. It is a very interesting paper that shows how to evaluate word embeddings, and they classify the measurement techniques into two categories: intrinsic evaluation and extrinsic evaluation.

Intrinsic evaluation means assessing the quality based on a specific task internally. So you use word similarity to measure the performance of your word embedding. Check the distance between similar words. If they are close or not, check the similarity between similar words. How to evaluate? If you compute similarity using Word2Vec or using FastText, what is the difference?

Or extrinsic evaluation: assessing based on the performance of a downstream task. So you try one embedding technique and use it for a specific task, like classification, and check the accuracy. And to compare, you use a different representation. As you see in our assignment, we have three different representations and we try to find the difference. It may not be high with classification tasks, especially with some classification tasks, but with other tasks, you may find this difference very high.

So this is how we can evaluate how much the representation or embedding reflects the semantic meaning of the words. Any questions? This lecture is very, very important. It will introduce you to the core of NLP. Now I will stop on the embedding. In the next lecture, I will not talk more about embedding, but I will start to talk about modelling, taking it step by step toward the idea of the Transformer. So we will start talking about the deep neural network and the recurrent model, and then move forward.
