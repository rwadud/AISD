I found this announcement, so enrol in a group. So, from now, so I've been seeing fuel functional in the room. And because at the end, before the submission, I will not accept any emails related to the problem between the 2 partners. So because I cannot resolve it, okay? So, as long as you choose your partner as a crew, by yourself, so, I'm not responsible for any dispute between you and your party, okay? Right. So... today, we talk about world embody, which is very, very important concept in natural language processes. So, it's a building plot for any application, in natural language of processing, and even all the improvement, introduced using transformer. It's based on the idea of the world in bed, okay? Right, so... In this lecture, we talk about specific techniques in our embedding, scapegram, continuous backwards, worded, okay, positive text, and the most recent technique, which is a lot. Okay. Right, right, again, to the same... And I feel life swiping, and you still keep on that feature engine. Okay? Last election, we started with feature engineering. We have some statistical techniques to represent the data as a vector in a space, and then we feed this vector as a feature, engineering vector, apply any... Okay? Today, we have a different... So, this is, um, just... an overview about the techniques we have, as we classified before we have three category, statistical waste, which we covered last time, uh, one home, including that for TFITF. And we have a protection, this, this presentation, which we call it, warding it. And this will be the task of our lecture today. Okay, now... look at this, I mean, this... I got an illustrated that the three type of uninstruction data, okay? So, for the image, we have a specific mathematical modem to represent this image, which is tomatoes, okay? Where each entry in the matrix has a value, and this value represents some feature related to the image. So a simple feature is the intensity of the image and physics, okay? And then even with the audio, the speech, we can represent it as free transform, web transform, any transformation technique where we have an amplitude that represent each signal value. So, when each point have a value. And then, based on the previous lecture, we have now a representation for the vote, okay, as a vector in space. The main difference here is, this is, we consider it as a dense vector. What we mean by dense vector, it's an opposite of sparse vector. Sparse vector, there is a lot of... But here, an image, we don't have a series, each point has a specific intensity, and a value. And the same here for the speech. H signal has a specific value. And then, in the previous presentation of the world as a vector, it's a sparse vector. So we need another way to convert, or to represent this award as a dense factory. When each point, or each value, in this vector has a value, no use, right? Okay? So this is what we mean by a dense vector. No serious in this vector. And this is the aim of our lecture today. is to come up with, they use the technique to convert or to represent the war detector as a tensive act, okay? Now, just go back and check... what was the limitation of the tech's presentation? The account be as the text presentation. One of the limitation is... by the dimensionality. It is on the vector, the vocabulary size. If you have a purpose waste, one million, and vocabulary, then the vector, for each word, has a dimension, one minute, okay? And the other one is a sparse deck, where we have a lot of zeros, okay? And we are able to deal with out of vocabulary. So if you trillion of your vectorization, for a specific word, or a specific vocabulary. Now, during the testing, you may have a new vote, that you are never seen. How the frequency based techniques deal with out of vocabulary, just about you, as we saw in the ink glass called previous lecture, okay? And the main party here, which is very, very important. And this is there, 80 point to let the people think about a different presentation of the world, lack of the semantic meaning. What do you mean by this? It's not just a string, it has a meaning. Okay? So each word in any language has a specific meaning. If you remember, in the previous technique, we will not consider the meaning, just work on the frequency, even in an IF and TF. We just look at the frequency of the world, we have lack of any meaning. Why, in Texas, in the thickest presentation, we should introduce the meaning of the world. This is completely make a change in your presentation. The word itself, the meaning of the word itself, and the meaning of the word when it has come in a specific context. Okay? So in this lecture, we try to capture this two feature, and then create our embedding event. So the embedding effect, capture two pictures. The war is semantic, the meaning of the war, and the relation of the world in the context, when it comes in a specific context, okay? And this is our aim, and we will try to see how the different techniques try to get rid of all these limitations, okay? Okay, so, of course, if we have this limitation, and we use this technique in any NNP task, the result, it will be poor performance in most of NNP. Why, it still have a good performance in other tasks, okay? So in tasks that we don't rely on the semantic, then this technique is preferred, it is easy to implement, but with some computation, consideration, but the majority of an empty task filled in such a representation of the world, okay? The world, which is based on that frequency. Right. So, as I mentioned, that word, it's not a string, just a string, it has a meaning, and what if you think about it, if I told you the word, the king, what will come to your mind? We do? Sorry? Regent. Regent? Another word? Royal, princess, queen, so each word has a relation to a different world in the same field, okay? So, like, in... Imagine the restaurant. This word has a meaning, when it comes in a specific field, or on a specific context. Why, the meaning of each word is completely different, okay? But when this world comes in this specific domain, they are related to each other's feet. Okay, so this is an important. We try to find a relation between the world and its surrounding world in a specific domain. All right, so one way of getting the value of the world to similarity is to ask the human and ask for the similar vote. You remember, at the beginning, when I draw a table, when we get some experts, and these experts that have described each world as a set of questions, okay? Zero and one. The idea here, it is seven. And this is a first trial, to introduce a meaning for each word, the word in it, which is already implemented in NMTK, and the most of the, um, text, um, uh, text, processing packages, species, and all these things, okay? So it's an ABI. And when you just ride the world here, it gives you all the information about this world. What is type of this information, something like that? So, the idea of wanting it, it's a network of faults, okay? As you see here, it's a network. When they group, each world, in a set of sunset of this world, sunset means set of synonym of this world. And also, including the word inet, there is, they define the relation between different world, in a different format. So this world is a part of this world, this world is a synonym for this world, this world is an opposite of this world, say they gave you all the information related to this specific world, plus something we call it plus, which is a definition of this world. An example. Where is this word that can be used? Okay? So, here, I think, okay, so you can refer to this API, which is invented by Princeton University. And this is the idea of the... coordinate. Now, and this is them, okay, I will not ask you to remember all these things, but this is a different semantic information that coordinate can, okay, can produce for any word, okay? This is something narrative to the semantic information. Right, so how we can use a word in it to have a dense vector, for example, or in a text presentation, while it is a lexical database. It's a network of work. It's not related to number. I didn't have a vector presentation inordinate, just network of artists and the relation between the world and each other. This is what included in the ordinate. But still, they try to use the word they need to introduce some semantic information to the vector. One of these trials, as you can see here, this is one of the world from the internet, so they can get all the examples from the coordinate, why? Because coordinate also have some example from each world, so they try to get all the examples related to this world, so all these water to come in a different context, and then make some text processing, identify, a sink, and Okay? And then they... He was an embedding. And embedding technique, it may be, not embedding technique, we call, maybe, use any of a technique. And the average of all this world that may be represents the actual semantic of the world. Because this world comes on the same context. Okay? And this is always the first try. And then here, when you average, always a world, which is come in the same country, we have, a veteran that, somehow, introduce some semantic meaning to the world top, okay? And this is more the first time. And other techniques, so, I mean, okay. Uh, okay. I, I, I forget to, uh, explain this. Now, there's some limitation in this technique, the most, the, the, the biggest, the, uh, limitation is, it's not the computation at all. Okay? There is, it's not a mathematical idea, it's a model that can eat, a vector that has some semantic meaning. This is one of the babies' disadvantages over limitations. The other one, it's limited coverage, and the static limited coverage, and the static. It is not updated dynamically, and every language is available. Every day, we have a new, new worlds, new land, and this is a part of the land. So, it is not up to date with some language features, and maybe specific, so, if you have a specific criminology related to scientific mutation, you may not find it in that this coordinate. Language limitation, it will not be applied. Okay, well, the net is just for English language, there is no equivalent word in it for the other languages, while there is some trial, but it's not in the same level life coordinate, okay? And then... they build it by an expert in the language. So, this is how the world is created. They playing group of experts in the language, and then they define the world, example of the world, relation of this world. This is done by the human. And what is the risk of this? Why? Introduce bias. So, it may introduce bias in that creative language. And this is the main disadvantage. I insisted to put this, to give you an idea. We still are able till now. In the recent... application, we are not using as a standable. We use it as an infigurated to another end of people, to improve its importance. Okay? So look at this example. How I use the water. And you search for a specific, and question, answer, system, or an even... a documentary theme. You write a quake, okay? And then, this is a new user queen. You take this way and search in the database to extract the document that relates to this point. Okay? The question answers this. The user write a question. And then, using a specific key, with a specific database, this search for the most selling documents. This question. How do we introduce what they need to improve this... The project. One of the ways is... architecture. When they use a right, a certain way, we can identify the key word of this session. And then we can go to the world and then to get all the ceremony related to this world. What is the benefit of this? So, I search for a cup. So they bring all the related wall also related to motor, mobile, or automobile. So what is the benefit of this? I search only for the cup. This is my search. But, yes. It's a water net. It's actually costing water, you're able to get results that are very close to related, but not exactly cold. Yes, but okay, at the same time, you import them the range of your expand the quail. You expand your plan. So, if you ride the car, it will not give you all the exact document related to the car, which is car in, but a car is related to another wall. So it also, it's a cinema. So it got all the document related to the scene. Okay? So it's brought that range of your extracting point. And also, it's applied, not for query expansion, it also applied in a question answer system. So when you write a question, then we will have a lecture in the question, Hunter system, the user write a question, and then we have a lot of document, then we need to extract the move that we have in document to this question. Okay? So, if you put the range of the user, question, in this case, you may access a document related to your question, okay? So, just to tell you, tell them, we can stay with the world. And to improve some specific ending. But, again, when not use it as a standalone, we use it as an integrative to another LND system, just it improves a bit. Okay? All right, so... Wordle net is not a good way. Because it's not a computational model, and it's have a lot of limitations. So, the research you think about it, how to, we can build the system based on these two specific phenomena, which is distribution similarity, and distribution hypotheses. So what is the distribution similarity? The meaning of the world can be understood from its context. Is that true? Maybe I can find, understand the meaning of the word, but the context play a very important rule, to understand the meaning of the word. The other thing is that distribution hypothesis, what is that, okay, in a similar context are similar. There is some similarity between this world. Okay? Like this example. Okay, they come from the semantic. So all the world related to the restaurant, all the world related to the house. They have some similarity, okay? And this is the two hypothesis that they starting point to have, what we call it now, a wording type. So, what is that, word embedding, or vector semantic, semantic means, it has some semantic meaning, okay? It's a computational model, so representing a word based on this tool, phenomenon, distribution, similarity, and the suffusion hypothesis, okay? All right, so, this is the idea. The idea is general. is we have a set of work. And we have a huge force, resources. So, we try to build the models that learn, predict, not to count. So the idea here is to protection. We can create the vector, or the embedding vector, or presentation vector, of the world, based on, and learning. So the world embedding come from the learning from huge harpers of edit. So, gather all these huge copies of the data, and then using the neural mutual, so, then, we can have an embedding vector for each world, 'cause that all the information, the meaning of the world, and also the context of this world, okay? And then we have this world, which will be the vector, or feature vector, for any NMP elements. So we have now a new technique to create a feature lecture, okay? And the most important property of this vector, it is a dense vector. So, we can, it shows our D that I mentioned. I wanted to choose the dimension, or presented a vector, any word in my corpus, and a vector of 100 dimensions. I can specify the T, which, if I mention, I want to... So, this is will need to... a dense vector presentation, because I specify the dimension, yes. Okay, so it's not related to the box. So, as you see, it's a function of... This is a vector, space, and list of your book. And I will map each word in this vocabulary to a dimension of vector D. So, D, maybe 100, 300, 200. So, so it's just a few times trial and error? Yes. And... Or is there, like, a... Okay, so, yeah, using DT. It's a critical. It's a challenge, but in most of the pre train, the more they will use this technique, like, go get, grab, or pasta technique, they provide us with a different G. So, D, maybe, at 150. Sometimes they have, as you will see, 100, 200, 300. So based on your computation, and this region performance, you can choose your performance. But more dimensions supposed to have more features. Okay? So 300, it's not like 500. How much feature you capture from this world, okay? So, doing this, and if we are able to represent the vector, any worm in this, as an embedding vector, will enable us to do what we call technology, word analogy. Find the similarities between the words. So, for example, if I represented the world in a specific vector, and then I drove this point as a two, three dimension, or four dimension base, or the dimension of the vector. So, now, if I present the cake as a point in the space, as values, vector, and the queen at a point in the space, okay? If you're embedded presentation, actually reflect the semantic meaning of the world, so you can't expect it that, the distance between king and the queen, it should be equal the distance between man and the woman. Okay? So we can predict the values of the new world. So, the common technique here, the world... to be equal, the difference between men... and men... captures the selectic meaning of the... So, if we have, we can conclude this... is equal, man, minus, woman, and plus... So we can predict any words. Okay? So, the same for the all depends. If you know that this is between walk and walking, you may know it. You expect, you protect, this is the same distance between swim and the swimming. Okay? If you know that distance, the relation between the country and its capital. So in the distance between Spain and the Madrid, it should be equal the distance between Italy and Rome as, they are in the same semantic. Okay? And using the embedding presentation, you are able to make some mathematically, operation in this, and they protect a new world vector, okay? For the analogy, and the similarity between the words, it's one of the techniques that measures the performance of the embedding. So if someone now come up with a new embedding technique, how we can measure its components, based on this, how much your presentation captures the similarities between the world would come in the same context, okay? And this is a very important use of the word elevation. We can measure at the performance of any world embedded technique based on how this world captures the semantic meaning of date words, okay? Right. So, the magic, we will not use a statistical model, we will use a neural network, okay? So, we will learn, rather than count, or predict, rather than count. The main idea here is self supervision. What do we mean by self supervision? And machine learning tasks. We have two main tasks. Classification, and... Okay. So, now, for classification, it is supervised or unsupervised. Supervised? Supervised? Supervised. What do you mean by supervised? We should have a link. Yeah, we have the level. You have a data and then we have a link okay? But what do you mean, myself, super dish? In a supervised form, someone should annotate your labour. When you gather the data, whether the data waste that, and even, okay? You need a human to activate the innocent. In a self supervision, you will not need anyone, too, and teach your date. How we have a data, the data itself. So, this is the idea of that self supervision. We will not need anyone to update the data, the annotation come from that, that takes itself. So, this is the main feature. Self the silver vision, and then predict, rather than sound. And it's one of the main features that the code is available, will not write the code, it's available everywhere, and it's a very fast food thread, and this is one of the most popular embedding things, okay? Now, which degree will apply it, and we have we apply self supervision. This is the... world compact presentation. So the embedding vector, awarded, it's a combination of two common techniques. It's common two techniques, okay? One of these techniques is CFA, which is a continuous pack of wood, and the other is a scapegrap. So, continuous amount of words, and the scapegram use the same idea. Selfie supervision, that is it, rather than help. So, protect the means we will use a numeral needs work, okay? Now, we'll talk about the first one, now, continuous bag, of course. Tell me if you have a text. This is a common force. I will use the window, sliding when, okay? Starting from the beginning of your text. Then, a continuous type of words, we identify the way to size, maybe I take the window size three. So, I will take three, the first is three, words in my text. This will be the context, text, and this is with the context. And this will be. So I try to predict the maiden word based on the surroundings. And this is the idea of the self supervision. No one will annotate. The annotation comes from the type of situation. And this is the continuous back of the world. Specify the window, the size is available, you can make a side, but you have a specific equation for the size of the window, you should be 2K plus one. It should be a problem. You know what? Because we have to predict the merit of this, okay? So specify the weights of your window, and then slide this way to starting from the first word, and this will be repeated for all your cups. And this is the urine with what? Okay? Just one hidden leg, okay? So after I finish this learning process, What is the inventing factory? The bedding vector is the heavenly. Oh. The head in layer matrix is an embedding vector for output from this process, where each wall, any embedding in the layer, exembly, it's a matrix, and the dimension of the matrix is... Well, V is, the list of all the vocabulary, and the dimensions that I specified, maybe 100, maybe 200, maybe 50, whatever, it is my option. So, this embedding the matrix, or this head and layer matrix, will be considered as the embedding vector for each work. So each world, the first world has this dead representation. The second one has sister, presentation, and so it's an epic. Two dimensions, okay? And this is an example, I will show you, like, some results, doing details for now. Okay, so, using any model... Mm... Okay, this is, um... For example, I can use a specific word, the lamb, and then the output of the vector, it's, as you see, here, as Dense vector, based on the dimensionality that I specified, where this represents a semantic meaning of that vector. Why? Because I always predicted award, related to axis around me. Well, it's a context. So, and maybe this world, by the way, maybe this world is repeated many times in your house. So the back for each, iteration would be updated. The head and back for be updated every time. When the war they come in a specific context. Okay? You understand the idea? I will try this, and this lecture is not cool to mathematical details, but what I want to know to... I try to make you understand what is idea, okay? So, if you want to go to the mathematical idea, I'll give you a reference to, more mathematical media, how they calculate it exactly. let you understand what is the idea behind each technique, and then there is a code. At least function in our library, that implement your all these things, okay? So this is the idea of continuous bag of work. Predict the middle world based on that, it's a surrounding world. Okay, so this is, here, an answer, example, where it is your, this is your training samples. Okay, for each sliding duendo, this is the train in the sanding of the sliding wind. So, the airport is square, while I try to predict that, the airport is power, while I try to predict that. So, this is your training salad, for each window. Okay? Then we slide the window, one step ahead, and then we repeat this process for all your curves. Okay? And this is the structure of your... So this is the structure. As you see, it's a very simple neural report. One, just one head in state, one head a day. So the endput gives a vector that represents each word, and then we try the context, then we try to protect the fact. Now, how we present the importance is. What is the what is the form of the import in this architecture? What is this? One not encoding for each word. Well, there is one in one location and disease in the other location. Why? Because if we multiply this input layer with a headed layer, which is a metrics, it's produced just one metric. Okay? For any head and layer, this mavericks will be, at the beginning, the weight metrics, at the beginning, what is, if you cover your neural, yes. So the matter is the weight matrix at the beginning. What is the value? Uh... At the beginning, it's random, yes. In Shalai, the way the metrics has a random value at the beginning. And every time, we update these values. Okay? So we shall write this matrix as a random value. So each world has a vector, and at the beginning, it's random. Okay? And then in a bad propagation, what will happen? Update the way. Based on that, compute the difference. You have an activation function, which produce a probability, what is a probability that the reduced world is a target world, based on computing the difference between the protected and the computed probability, back propagation, go back and update over these weeks. This is how the US report works, okay? So please go back and facial information about the neural network. But this is the architecture. Okay? So, we input all that context, what, as an input, each word represented as a one hot including, where there is one only, on this one, and based on this modification, we can have access, and maybe have, show this is, and some, some, some, I try to, um, just explain the idea. So if you have a 3 complex words, for example. And this is one hot encoding for each world, okay? And this is that, the head and layer weight, which is a matrix, which is, what's the dimension of this matrix? The vocabulary, and it's our dimension. So what is the dimension now, what is a D in our case? What's our G in this case? Oh. Okay, I choose to... in a four dimension, okay? So if you multiply the first context toward, with this method. What will be the result? The rule, where there is a one, in this matrix, on this presentation. Second. So what is one year in the second one? So the result of this is the second one, five, six, seven, eight. Did you want the idea here? And then we, they have a linearity here, they would average so, uh, compute the, the average of all this weights, go to soft max layer, to have, and to convert it to one hot encoding again, to, to be able to compute the difference between the probability, output from the soft max and the actual value, and then we try to minimize this difference. Okay? So after finishing all this process, this will be a embedding factor for our course. Okay? And this is, uh, then letting that we will search for, still, it has a semantic meaning, capture the relation between the world, which is surrounding, based on how much works, or what is the size of your course. So the more is better. more you add a purpose, you give your model, an opportunity to learn more and more from the text, okay? And then we'll have a scapegrand. Escape gram, it's the same idea. You're gonna need more, self is more vision, but with a little bit effort. What is the difference here? And instead of end with the context world, and trying to protect the target, we will enter the context, and we try to predict the contest. And the objective here is to minimize the difference between the sum of the error of the context world and the actual events. Okay? So this is the idea here. Predict the context word given the metaphor. Again, we have a window with a specific window size, and we slide this window through our your text. And under each window, we will try to protect what is the context of our... If our source here is, eh, which is, uh, context work. Then I build my, uh, the training design. So this is a whole... This is a whole Texas, and this is my window. I want to protect this contentist, please, or, uh, work together in the weather world. So, I consider the better word, the air that. What before that? I would just drink. Okay, so, it's like, you know, and maybe you may have this experience in machine learning, how we can deal with this situation, where the window size or emphasize is still the same. What you have to do. The animal should be the same every time, yes? For any neural network. Yes? So the input size should be the same. So what have we deal with this situation where the import site is not the same? You hear about bedding? Not bad. You're covered... a teen near a network or not yet? Nice. Not yet. Okay, so... with certain as a padding or an MVT vector, and then we consider that this two has a context and a previous two. So for here, let's take this one. So this is that, central, and this is the context world. So, we will use power to protect the fox, we will use ground to protect the sharks, we will use ground to predict the quick, you use ground to predict that. So this is the idea of the scapegrow. So, it's an opposite of continuous world, continuous type of world. Again, we use a neural network. And this is protection, and the main objective here is to minimize the sum of the prediction across all that context world. So, again, the airport layer is just the one word, one word encoding, and we have the weight vector, initialized air, And during the learning, we come up with this way. as an embedding event, for each word in your book. Okay? And this is, I found this is actually, just yesterday, when I try to update my lectures, I like it because it has a new American example, so you can imagine what happened in the scapegrap, and you can return to this resource, because he actually computer system by step. mathematically. So if you are a god in math, in math, and you want to learn more how this neural network work, I recommend it to go to this reference, but it gives you an information here, how the scapegram works, okay? Right. So, and this is an example, an Asian example, how the scapegrum works, so we start with that. So, as you see here, that is not, that, it's a first one. So, there is no two radius. What is then, we have a dummy vector or bagging vector, this is a start, okay? And then, as we go, step by step, we slide the window to the next word, then you choose your window slide, and so on, create every time you create your data. Here, data sample for each window, okay? Okay. So, now we have a two technique for working back. Two common tickets, scare gram, and continuous type of food. So which one we use, you have to know what is the difference or the main difference between them? So, a mediospect of words, considered to be faster. Then, scapegrand, and it's better where we have some words are, has high frequency, okay? So in this case, and this is based on the paper, okay? They publish the research published people. We're not investigating this by ourselves, but this is based on their published paper relating how to differentiate between skin gram and a continuous type of food. Escape gram is slower, but work well with a smaller data size, skin, potatoes, bag of water, it's easier for in classification than a sketch. So, when you want to choose which one, when you use as an embedding technique, keep this information in your mind. It's based on your task, how you can represent your task, then based on this, you can choose which one you can choose. Okay. Then, in some ways, one actually cooking. Uh, about a researcher, they published a paper... with another techniques, we call it a negative sample escape. Escape gram with negative something, okay? And then we fed it to SGNS. Skate gram with negative sample. So it is the same right, like a scapegrel. We, using the maiden world, we try to predict is a context, but with another difference. So, why would we, why would not let that, the model, learn more by introducing the positive samples? So, we leave the modern ways that context, which is a positive sample around the world. And also, we need the modern ways, a negative sample, which is a random word, not related to the middle world. And we let the model live. From, this is a positive sample, and this is a negative sample. So the model has left you more information about its own, okay? And this is their idea. Okay, so... That's my week was that, that, that, um, the target here is to maximize the similarity between the target world and its context, the positive sandbags, and minimize the similarity between that target world and the negative sound, which is not in its context. Okay? So, for example, here, if you have this train in December, and then, we have, we use an apricot at the end. So, this is a list of positive sun. All the world come in the context, or in the context of the apricot. And we hear the model at the same time, weighs up, negatives up. And we have to and get this for that moment. This is a positive something, and this is a negative sign. Maximize that. Difference between here, the similarity, between what the something, and to minimize the difference here in a negative sample, make the model, learn more about your work, add more semantic information for you. And this is the idea of that scapegram with a negative side. Hogel, use this technique. We will, who can publish a word to fake model in 2013, using skate gram with negative sum. So, what is the benefit of this? Will not be in our date? The water, the water, water, to make, from cooking, does the job for us. They, they, they gather a lot of information, actually, I cannot remember how many, three measures. come from Archidadia and different resources. And they provide us with as ready, free trained moment. Heaven or embedding effect for all this world. One of the animation, 50, and the one of the dimensions, 300. So you will not go through all the train into process. Read the congress and delete all the clinical process to have the way it matters. The way this matter is come for you already, with the serve the body. One of them is not worth it to fake from. You have the list of work, come from huge corpus of data, and each word has its own embedding. And all what we need is, if you want to input this in your NLB task. So you can have a feature vector for each word, come from one to web, and this is your feature space. What is the word in your, if you make a classification? fast. Then, in the classification, you have a deal, or sentimental analysis, or whatever. Then you have a table where you have a text, this is your text, and this is your decision, right? Negative, first it isn't a classification, okay? Have I represented this Davis? So, botanized, your input? Grab this talking presentation from... If you have a sentence, you can average all the vectors, and then you have a feature, embedding presentation of this text. Okay? So you got the idea? So here, what did you back from? I will provide you with this... It's an API, you can... Jensen API, as you can see here, I can upload it. Okay. So, I know, just, Jensen, here, I have to install it, Jensen, and then import it, and then you can load which one. I choose one to choose the 300 model, and then you can just print any embedding the factor for anyone. So the modern name, which is that will be, that will be okay. As you see, it is an a vector of dimension 300. Sensive vector of dimensions 300 represents a semantic feature of wood in this huge, three manual world. Okay? And the same for the computer, and by this weekend, find the similarities of any word, so I got the factory space of a specific word, and then I can compute the cozain similarity, to see how much this world embedding, successful, to reflect the semantic meaning of these two words. Okay? So here, for example, I try to get all the similar work is caught up, and I specify the top three, so the top three is love, mom, and the loft. So, and this is the way how we can measure the performance of any inviting things. Okay? The common one is, uh, one for back, uh, come from Bogen in 2013, and it was the first retrained model, come to us. But there is another models. Oh, sorry. There is another two models, past text, which also comes from booking, it's a special, special current, of worldly to book, with some, um, difference, and you have also love, which is completely different. Bluff come from the standard, but Stanford University, and wanting to pay and fast it is actually created by a woman recession. Right. So, this is for worth it, okay? You just installed in some baggage, and then you can choose which dimension, which model, you want to upload, then you can represent any world based on this. So, you will not train from the beginning, you will use a pre print, okay? And as I mentioned here, the lens is bad. So it's based on your communication, resources based on the criticality of your application, you can use which dimension. It's a challenge to switch perfect by mentioning news. Okay? Okay. Okay, perfect. So this is how to be stolen? No. It is in library. No. And this is, um... Okay, you have to, actually, we have two choice here. If you want to use Vortetovac to train your data from scratch. Okay? So this is true. What do you mean, the pre train, the model? Okay? And then you'll find tuning. Using your own it. So, if you choose to train your model based on your own carvass, you can specify the model like this. This is your status or your purpose, and this is minimum pound equal. Um, this option relate to which work you want, you can order it in the train. So, sometimes, the people may have, if the world is not repeated, more than one time, we may delete this one, nothing. So, this is what we mean by many men. So, minimum count one, I hear, I introduce all the ones. You can change it to two, so that the world is included in your training, it should be okay, two times, at minimum, two times, in your times. And then here, we have an exercise, I can identify my own size. Okay? And then the window size, which is one of the parameter, so the context window, what is the size of your context window, you have to specify. So, and it's one of the high parameters. You know, someone may say, I can increase the window size, so I can capture more information about the context. Okay? So, it is one of the hyper parameters to choose your window size, okay? And then this parameter is very important, SCG, one. So, SCG, as I mentioned, one, do they come as to implementation, continuous bag of warmth, and the scape gram. So, if SCGH was zero, the modern applied is continuous type of fault. If SCG 811, then you will apply the scapegrap. Okay? So this is parameter, identify which technique you use. And then we have a negative 815. So, if you use a step grand, then you may use a scaped gram with negative sum. So, if this value is one, you can add this option to add more negative sandwich in your tree. But if you use this to be zero, this will not be here, because continuous pack of one doesn't use a negative sum. Okay? And this, you can use this very simple model, tool, create your own embedding vector based on your own data, your training data. In this case, you will not use the pre-train demodive, come from factory, from Google, okay? So this is if you want to train your data from scratch. Okay, are you showing one here? Now, in the in class coat. I adhere some information, at the beginning, let's go through here. Uh, give you some records. I gave you an example here of how to use a word in it. And, for example, I choose support the card, and then I print all the information related to the water card. Sunset name, part of a speech, what's the part of a speech, definition, which is have some examples, some example of a sentence, come of this, and this is a list of all linguistic feature, that's what the print can print from God. So, as we see here, this is the name. And this is the definition of the car, and they give you here some example, to understand his meaning, some sunset, mutor, vehicle, and we have another, a list of other sunset, with a cinema, toward the car, and all these trains are all information about it. And again, it may play an importantly, in, to improve many of NLC application, to integrate the coordinate in your application. This is an example of universal word embedding, where they use a transport. Actually, it's not a transport, but the space you use a CNN. Without going in detail, but still space you have, we can have a vector of each word, pre trained vector. So, as you see here, the vocabulary is, this is the number. number of vocabulary in a specie, and this is a dimension in a specie for the large language, okay? And we can create any vector, the value of any vector. Right, so, how we deal with out of vocabulary, using, for example, spacing. So, I try to make this experiment to see if I feed the spicy ways out of vocabulary wall. Now, spicy, we could deal with out of vocabulary. As you can see here, I gave it NLP professor as a one word, which is not an English word, and the output here, it's a list of Zeus. Okay, so, still, we have, we suffer, from, I would hope to work out, because we, from cooking. We train it in a specific carpet, three million for maybe more, but still, if the model, not seeing this world before, what will be done. Still, Okay, we improve, that takes presentation by introducing some semantic information, and we improve it by chose a density vector, with a specific dimensionary but still out of the world, vocabulary, still there. We have another way to deal with this, but I will try to see here how many also, how if a stacy are able to deal with this case. And I can find the seminarity. For example, here, I try to find the similarity between, I love school, and I hate school, as you see here, the similarity is very hot, because they are related, suffering. Okay? And world embedding using cooking, so here, I create my own purpose, a very simple purpose here, and then I go through all the processing of re processing, organization, and remove punctuation, and all these things. I come with that. More than, then I will try to move forward. Use this modern basis, the same parameters, I explain. And this is how I can trail, use worthy to wake, but in my own data, okay? And this is how we send dealers. And I have also another example here. So, use, and I try to visualization, how to represent each world in a two-dimension space, compute word similarity, and then this is an example of use, a word to beg, from, as a patriot, okay? So, as you see, the implementation is there, you will not write anything from scratch, or what you need to understand what you are applied. And this is a very important factor. So, as the code now, it's here, and library implement a lot of techniques, you don't have to implement anything from stashes, but you still should be understands what's going on here. So you have to understand what is the idea, how the thing is going on in this idea. Okay? Now, the second one, breathe in the morning, it's a block. And it now, it's an unsupervised learning model.. And a group of research from Stanford University who invented love. I cannot remember the date, but... So what is the ideal thing? Either graph is different. From the worded effect from Google. So, the research, from the side, what, think about, okay, using what it is. have some semantic meaning, relating to any word, this, what is context. But when they best have very important information here, that the global appearance of the world inside your course. How many times this world repeat in your parts? End of this week. How this world come together? This is the system that information, but it may be enhanced the embedding people. So it will combine. The previous technique, which is a word to make, learn, and add to this, some statistical global appellants of each world inside the purpose, it may improve their performance of the world impact. And this is the idea of that love. I try to see it, as you see here, it goes through specific... So, the fairy step is connected the better. And then, we crosses the data, and the heat, we compute the co appearance metrics. What is appearance, not with something like that? You have a nest of words, we choose the word sign, and how many times in this world, it's magic magics, as you see. But how many times in our family is programmed, for the whole corps? This is ascent, and this is what they mean by co appearance matches. The idea comes from... I'm trying to... Okay, so if you have... I want X one. And this old X one have a specific presentation, okay? And another X two, another one, with a specific background presentation, okay? What about... X one, go through that, X... What this value present? What is this value present? That don't probe that between two vectors. I will perform me they don't prove them? Is the summation, huh? X one, X one, X two, X three, one. So, multiply this, this, this, this, this is a top book. What is the product represent in a semantic point of view, between two words? Sorry? The distance? The distance? In other world? Really? How they are similar. Okay? So, in, in, in, that now, they try to, they found out that... They don't grow that between, towards, it approximately, approximately, equal, the sea of one, two, the poor appearance and value, between X, one, and X. It's one... So, this is their opinion. Because, actually, one kid, I don't know that is hot. What do you mean by it? That's not problem that it's hot. It means here, there is museums here, there is all the, the X one is high, and all the P one is home, okay? So there is a kind of similarities that we are the importance and something important in common between these four. And they try to minimize the difference between the co appearance and the blood product, for each word. And this is the idea of that love. It's based on the appearance metrics, because they mentioned that coke and symmetrics hold the more information, maybe to that global structure or global syntax in your corps. Statistical model, statistical, or the frequency, still have an importance that we have to consider. How many times, this world, come with this world, some semantic information, it has some semantic meaning, we can introduce it by love, okay? So, this is that, steps to create, to implement the love, construct a cocaine's magic, and then use a cocaine's magic, to compute the body embedding, using the love and breast, and they try to minimize the error between the ward, the actual, not row that and the co-appearance, uh, college, but it's not, appearance, howard, it's a log of how, okay, it's a coming. Okay? Actually, the computer difference between the top product and the focus. Okay? And then, see if the present. And this is what that love has. Okay? So, I think I have glove also come with different sizes. based on, actually, it's trained with different resources here, glove, with equator, a crowler. Many, actually, but here, we have... 25D, 30, 200 feet, and you can... By the way, the love, it's a text vector. I now load it here. I hope to, I hope to... Okay, so this is enough for 300. It's a text file. I don't take time to download it, but... Train with a huge data set with a different dimension, and then you can choose which takes safari. That will not, and all what you have to do is to search for your word, in your purpose, and you can't extract that, the victory, represent it. You will not train the model from scratch, okay? And this is, uh... 6 to 5, as you see, for example, that. And this is the vector of that. So for each world, you have this huge vector. And, again, it's a fix. And you have to know this is a vector, because when you use this... glove, to... to use it, in an LAP, how to deal with this text, vector to extract your word, and extract the vector later to this word. Okay? And this is the idea of the glove. It's a very fantastic idea, improve the performance. These on the old experiment they publish in a different paper, they improve the performance of embedding task, because we add more feature, which is the whole kills factory. All right, so, this is the gloves, and this is, if you want to more about this, you can go and read more about that project. Now, we still have a problem. which is dealing with... out of vocabulary mode. With a free trend model, they trained on a specific date. If in your data, there is new work that is not included in this testify. What will happen? You will not have... The present is this your word. We have three techniques. Okay? Like, as you see in the specie, they use a default vector, which is a vector of zeus, okay, or any values, okay? So the first technique is use a default detector. Whenever you are not fired, equivalent, vector for your world, use the present your world in the port effect. This is one of the technique. Fall back to similar word, use, ordinate. Use a seminar, try to find a synonym for your world, and you may find one of your seminar world already included in the trend world. Or train your own embedding model. So, you will have your compass, and you will train your model with your old deal. In this case, you make sure that every word has a vector, if you train your data from scratch, okay? Another, a better solution is first takes more. So fast it takes the model invented by Facebook in 2016, as an extension to one to vac. So, it's based on the one to do web, and also, you can load it in Jensen package, okay? What is the idea of fast it takes, and how fast it takes, deal with, out of vocabulary, out in the event. So... They use an emigrant. So instead of presenting the world, the whole world, as a vector, I will divide the world from three to six, any brand, so they, they, they article. So any one, they can be divided into three to six immigrant. And then find a victim for each enigram. And the vector of the whole world is, is the adding of all the vector of immigrant. So what is the benefit of this? Imagine that we have, for example, the warm cities. So, cities? This is wonderful age, okay? If I use ligra. How I can divide it, so using three grand, I start, it's the boundary is part, C, I, and then... And so, you know, this one. Three grams, so one, two, three, then starting from high, I, T, I, and then start with O, T, T, I, E, and then, I, E, S, and then... Okay. And I have a vector for each grab of this one. Now, in your later, you have a word like something related, see, I eat, eat. Okay. Now, using three grams. Right, so, see, I, and then... I, T, I, and then... O, E, E, E, E, E, E... O, I, put it. No. Oh, I had this, so I... Okay, so happy. So, I would take this three, and then starting from C... Then I start to proceed, CIT, and then start... What? And then... Hi. Hi. Okay, okay, okay, okay. Now, this is a new word. Okay? How am I gonna represent in this world? As you see him, there is. Similar. So, I still are able to see or represent a vector for this approximate, but it may be, for example, this is the only difference, for example, from the cities, but you may have another epigram or vector presentation for the same IEE, from different world. So this is the hydro fast test. We are able to deal with our own vocabulary, with most... as much as we can, accuracy, it's not a 100% accuse. But still, we may have a vector of presentation for this world, and this vector may be very near to the actual presentation. Have some semantic means, okay? And this is the idea of the fastest, okay? For each world, we divided that, we find, for each world, three to six, so three gram, four gram, five gram, six gram, for each one. And then based on this, we create an embedding for each, any gram, and then the vector will be some of the vector of the collector, any gram for this word. Okay? So we have post presentation, the emigram, and the vector of the whole course. And this is advantage of fast text. So, capture fine details as we divide the world in its enigraph. So, more details about them, what solve... This is incorrect. It's all only, not we all. So that is a mistake. So we solve out of vocabulary problem, okay? Open source, lightware, light, white library, part of Jensen, come with API, where you can, in any, any, any. All right, so, this is the... famous technique for the world in panic. 10, 2016. Starting from 2017, The whole images changed. 2017, they published a paper of the famous people. A pension is all you need. The transform. Starting from that, the world embedded technique will be changed. Another item, more semantic item. We'll talk about it, but on time, not now, okay? But this is a benefit of the world embedding, dimitionality reduction, instead of huge dimension of the world, vocabulary size, I have now condensed, dense, vector, semantic meaning, not count. I just... I have H word now, have a semantic name, because we're protecting the word related to its context. So the context has meaning, introduced to this world. And somehow, in a fast it takes, we handle out of vocabulary, and we use the most important concept, which is... Transfer learning. Okay? Okay? And this is, of course, well improve the performance of any NLP task, okay? Now, still, we have a limitation. It's not... it's not solved all the problem. Still, we have some limitation. One of these limitations is context and some estimates. So still, you will not capture all the context of the world. It's based on your window size. So, maybe that the window side should be more than this. But still, we not cover all the context information. We need more improvement, vice. We rented the data, okay? With from the internet, that may introduce vice in your presentation, as you will see in lap three, how this world effect to introduce some lives, okay? In a world dissimilarity. So, doctor always similar to man, but the similarity between doctor and woman, it's not the same. Okay? Which is type of place, okay? Limited semantic adaption? Okay, so, because we still need more information to completely understand them or completely represent the meaning of each word, lexical syntax information, plus semantic information. All this information is not introduced in semantic. And by missionality, it's a trekking. Do you present in 300, 100, 500, or 50? It's a challenge to choose your best dimension for your presentation. And resource is, uh, you need a lot of resources to train this one. You're standing a lot of resources, to train your model, you need a lot of huge information, from a different resource of cleaning. And this is some of that religion. I want to board the vocabulary, it's not... IG, or correctly, 100%... overriding India, in the aposthetics, we still have an approximation, not exact. Right, so the solution is a universal fix presentation, which is based on the Transformer, we will cover a bird, and we will see how this is a huge step. to create an embedded. So part based on the transformer, create an embedding vector, but in more cleverty clique, okay? Reflected the semantics, the context, and all, the limitation of the wording. How to evaluate toward the embedding, I found this paper very interesting, I publish it here as a hybrid word. Very interesting people that show how to evaluate the world embedding, and they classify the measurement technique into two category. Interesting evaluation, actually. So, so, okay, it means based on internal measurement or extended measurement, intended measurement means access is the quality, depending on a specific task. So, okay, use the world is the similarity, to measure your performance of world impact, okay? Check the distance between similar words. If they are the same or not, change the similarity, they need the similar word, how to evaluate, so I, if you compute a similarity using world effect, or using fast taste, what is the difference? Okay? And this is will become in your... Or accessing based on the performance of the task. So, try one there. Okay? So, I use this measure, this embedding technique, and it's for the same data for a specific task, classification, and the checks are before, security. And to compare it, if you use a different presentation. As you see, in assignment one. So, we have three different presentation, and we try to find them. What is the difference here? It may be not high with the classification, especially with the classification task, but with the other task, you may find this difference very high. Okay? So, this is how we can evaluate. That's... how much this presentation or embedding presentation reflects the semanticantic meaning of the world, okay? Right. Any question? Yes. Okay, I finished, by the way. Any question? This lecture is very, very important. It will introduce you, Rosa, the rare, MLP what? Okay? Now, I will stop on the embambing, to tell, to this lecture, I will not talk more about the embody, but I will start to talk about the modelling, to taking step by step, have the idea of the transform. Come to the heart. So I was starting to talk about the deep neural network to render model, and then...