Today we will discuss what is considered a starting point for large language models. Starting from this lecture, we will begin to develop our knowledge of how the idea of large language models came about. We will start with the early techniques that use a language model, and then step by step we will see how the idea improved to arrive at the final step, which is large language models, which everyone talks about now.

I will talk a little about data collection, which is not a part of our course, but I think it is good to know how people collect data. And then I will talk about language modeling, go through some different techniques, and we will discuss the advantages and disadvantages of each technique.

So, back again to our development life cycle. Now I will talk a little bit about data collection. Especially for NLP, there are many sources of data, and the main source of data is actually social media. This is a huge source of data where many NLP applications rely on this data. As you see, there are many platforms, and you can see there is a huge amount of data that we can extract from social media. Each of these platforms has its developer portal that allows people to access this data.

I will talk about the most famous one, which is X. It was called Twitter before, and it is a very important source of data. People collected the tweets, and based on these tweets, many applications can be built on top of this. One of these is sentiment analysis. Usually during elections, we can find how people think about a specific candidate.

I had an experience collecting data using Twitter. At that time, they gave us a free account with a limited number of tweets per day. We could collect about 11,500 tweets per day. There was a limit, and we had to apply for a long term account, which required stating the purpose of collection, and then we would be approved to access the data. But now, with the X platform, I think they do not have a free account. You still have to go to the developer portal, fill out the application, state the purpose of your research, provide your organization, answer a lot of questions, and then you can be allowed to access the data and collect some information. This is one of the main sources for a lot of applications in NLP.

One of the applications I worked on before, in my previous company, involved finding the ethical concerns related to one of the research papers published by a research group in the Royal Academy. It was something related to mental health. The researcher at that time, along with his group, collected about three million different tweets over three years, and the objective was to predict people who are at risk for suicide at any time. They provided very interesting results, and he presented his application at many events. Three million different tweets are even hard to upload on our devices, and we tried to find the ethical concerns about this. But this is how you can build an application. Usually it depends on a huge amount of data to get some useful insights.

The second source of data is usually through web scraping. We have a lot of websites, and we can scrape the data and convert it to something that may be useful for us in any application. We have libraries such as BeautifulSoup and html5lib, which is based on the latest version of HTML, HTML5. Using BeautifulSoup, it contains a parser that can parse your HTML and extract whatever data you want from the HTML.

I included a very simple example. I request to scrape the algonquincollege.com website. This is the URL that I try to scrape the data from. Using BeautifulSoup and the HTML parser, I try to extract some data. If you have some knowledge about HTML, we have many tags, and you can find all the tags using the HTML parser. Here I try to find all headers: h1, h2, h3. Then it returns to me all the headers related to this website. Then I try to count them: it is 20. Then I try to extract all the paragraphs. The idea of BeautifulSoup is that you can extract this data, and then it is converted to a data structure that you can use later on. As you can see here, I extracted the data, then performed tokenization, and we can save this data to any file. Then you can use this data in your application.

So, these are two main sources of data collection in NLP: social media and other websites. Another important source is PDF files, papers or books, and it is easy to extract from them. We have libraries in Python that can read any PDF file and convert it to a data structure that you can use.

Before we start with the language model, I want to give you a brief reminder about probability theory. It is a very simple overview, not going into details, but just to make sure that we understand the definitions.

How do we compute the probability of picking some shape from a bag of different shapes? If you want to compute the probability of having a blue square, we simply compute how many blue squares divided by the total number of elements. We can also make a joint probability, so we can compute the probability of a group. The probability of having a sequence is the multiplication of the probability of each one. It is the product of the probability of each element.

Then we have conditional probability. We compute the probability of a specific element based on another element. The probability of X given Y is simply the joint probability of X and Y divided by the probability of Y. This is how we compute conditional probability.

In probability, we have the chain rule. In the chain rule, we express the joint probability as a product of conditional probabilities. If you have a sequence of events X1 to Xn, the probability of this sequence is the product: P(X1) multiplied by P(X2 given X1), multiplied by P(X3 given X1 and X2), and so on. This is what we mean by the chain rule in probability theory.

Given this information, we will now start to talk about the language model. What is a language model? Consider this statement: "The students opened their..." What do you expect the next word to be? Books. Bags. Exams. Different words. What you are trying to do now is what a language model does. You try to predict which word comes next. Language models take your input and build on it. In a high level sense, the technique is advanced, but very simply, we try to predict what the next word will be. It might be books, minds, exams, or laptops.

This is the mathematical formula for the language model. Given a sequence of words, we try to predict what the next word is given the whole previous sequence. Based on your training data, based on your corpus, we try to find which word comes next by computing the probability of appearance of each word. The word with the highest probability is the predicted word. We try to build the model, and our model is a probability model. We compute or try to predict what the next word is based on the probability distribution over the words in our vocabulary. The predicted word, x at t plus one, must be from our vocabulary. The model should have seen this word during training. This is what we call a language model.

We use language models in our daily life. When you write any text message in WhatsApp, text completion uses the same idea as a language model. Not exactly the same language model, but it is a type of language model usage. When you search in Google, text completion works the same way. Even in your email, when you start to write, it automatically gives you some suggestions.

The goal of a language model is to learn from the patterns in a huge amount of text how to predict the next word, given the previous words inside the context.

The first language model was built on statistical techniques. Statistical technique means frequency, based on counting. The first language model is the n-gram model. In a huge number of texts, you define n, and using a sliding window, you go through all your text and divide it into specific n-grams. It is based on your decision: trigram, four-gram, it is up to you. Then for each n-gram, you use the previous n minus one tokens to predict what the next word is.

For example, if you have x1, x2, x3, x4, this is your four-gram. We find all the four-grams in our corpus, and then we use n minus one tokens to predict the last token. This is based on counting how many times these words appear together. They convert the probability for our model as a ratio between two counts: the count of the n-gram divided by the count of the n minus one gram.

If you have a sequence of tokens x1 to xT, then the probability of seeing the next token using the chain rule is: P(x1 to xT) equals P(x1) multiplied by P(x2 given x1), and so on. This product is our probability model. Our model is a probability model, and we try to find its parameters.

Every model has some assumptions, and our assumption is that the nth word depends on the n minus one previous words. Predicting the last word, which is k, is dependent on all the previous words x, y, and z. This is our assumption. We want to be able to use conditional probability. It is logical. When you read a sentence, the words form a context, and based on this context, we try to find what the next word is.

They convert it to a computational technique that depends on counting. Very simply, it is just a counting process. To compute the probability for a specific n-gram, the probability of each word is the count of the n-gram divided by the count of the n minus one gram. So the probability of word k given words x1 to x(k-1) is the count of this entire pattern divided by the count of the previous words occurring together. How many times do you see all of this pattern? Divided by how many times do you see the shorter pattern in your text.

These probabilities come from the text collected from the web. You gather your data, decide your n, and then compute all the probabilities.

Here in n-gram, I created a text. I used one of the Shakespeare stories. Then I computed the trigram frequency and the trigram probability. "Bags for logs" comes with this specific probability, "this series" comes with this specific probability, and so on. This is what the model does during training. During training, a sliding window goes through all your text, divides it into a fixed window based on your n-gram size, and then computes the probability of each n-gram.

For example, if we have this sentence: "As the problem starts, the students opened their..." We try to predict what the next word is. Based on our n-gram model, we have to specify what fixed window we use to predict the next word. If we use n equals 4, then we discard all the previous words and just focus on these three words to predict what is coming next. This is based on the fixed window.

How to compute it? We count how many times in our whole corpus "students opened their [any word]" occurs. We search for how many times this sequence appears with any word after it, divided by how many times "students opened their" occurs in our corpus. This is the probability. They compute this probability for each possible word in your vocabulary. How many times does "books" come after this? How many times does "exams" come? How many times does "minds" come? Using this ratio, they assign a probability for each word. Based on this probability, they determine which word wins.

For example, in your corpus, "students opened their" occurred 1,000 times. "Students opened their books" occurred 400 times. Using this calculation, the probability of "books" is 0.4. Another example: suppose that "students opened their exams" occurred 100 times in your corpus. Based on the ratio, the probability is 0.1. So which word is winning? "Books," based on your corpus. This is how the n-gram model works.

The order is very, very important. An n-gram model chops your text into specific n-grams. You slide from the beginning to the end, going from left to right. So the order is preserved.

What are the problems with this model? First, choosing n is a challenge. If you increase n to capture more context, what will happen? The computation becomes very, very hard. N-gram computation complexity is very high. Second, think about if a particular sequence never appeared in your corpus. "The students opened their..." never occurred in your corpus. What will happen? The probability will be zero. And what if the denominator also never appeared in your corpus? "Students opened their" not appearing at all means the probability is undefined or zero. This will produce sparsity in your data, a lot of zeros. They use some techniques to fall back. If they did not find a four-gram, they reduce to a trigram. If they did not find a trigram, they go back to a bigram. This is one way to overcome this problem, but still, we have sparsity issues.

Another challenge is computation complexity. N-gram models are heavy in computation. If you try it with a small data set and compute all the n-grams, you can see how much computation is required. The last and most important challenge is context limitation. To compute the next word after "the students opened their," we discard the previous part of the sentence. But the earlier part of the sentence could indicate what the next word should be. This is one of the main limitations of using the n-gram language model.

We can still use n-grams to generate a sequence of tokens. Given a seed, the model can generate text based on the probability distribution. But the generated text may not be coherent. For example, after creating the n-gram model, I feed the model with a seed, maybe random or a specific seed. Starting from this seed, the model computes over all the words which one should come next with the highest probability, then adds this word and repeats the process again. This is how it generates text using a language model. This is one of the earliest techniques, which relies on frequency based counting, and these are the main limitations of using n-grams as a language model.

Now they started to think about a new technique. When we talked about word embedding or text representation, the earlier models were based on statistical techniques: frequency, bag of words, TF-IDF. Then we started to work with neural networks. This is a logical transition. Neural networks started to appear around 1940. They tried to think about how to build a language model using a neural network. Instead of counting, we let the model learn what word should come next using a neural network.

A quick overview of neural networks. You have an input layer, a hidden layer, and an output layer. During learning, the weights in the hidden layer are updated to minimize the difference between the predicted output and the true output, which is the loss function. We try to minimize the loss function.

At a closer view, we have a sample, which is a set of attributes based on your data. We always have a bias. The bias is a value added to the total weighted sum of the attributes. Before the result goes to the activation function, we always add the bias. Then we compute using the activation function and measure the difference between the predicted output and the actual output. During backpropagation, we update the weights to minimize this difference. This is how the neural network works.

If we use a neural network to create a language model to predict the next word given the context, we still need to use a fixed window. Why? Are we able to use a neural network without a fixed window? Usually, in machine learning, you have a table, and this table has a fixed number of attributes. Each sample has a fixed size, and the input size should be the same. So we need a fixed window, even if we use a neural network. We discard the previous part based on the window size and input size, and then we use the previous words to predict the next word.

Here is an example of a neural network language model. We feed our text. For each token, we get the embedding vector for this token using any embedding technique. Then we concatenate or average the embeddings and try to predict which word has the highest probability of being the next word. The probability is a probability distribution over all the words. The word with the highest probability will be the predicted word. This happens during training. A sliding window goes through your text, and each time it feeds the data, creates an embedding layer, and tries to minimize the difference. Based on these learned parameters, in the test case, you feed the model and expect it will predict the next word.

The word is represented as a one-hot encoding, and then we create an embedding vector for each word. We concatenate or average all the embeddings to have one vector representing the whole context, and then we use the softmax function to predict which word will come next.

We still have limitations with the neural network approach. One of the main limitations is that we still use a fixed window, because this is the nature of neural networks. We need a fixed number of inputs for each sample, so we still need a fixed window. We discard all previous words that may be important for the context.

Another limitation is shown by this example. "The food was good, not bad at all." "The food was bad, not good at all." What is the difference between these two? Imagine that you divide each sentence into tokens and get the embedding of each token, then represent the whole sentence by getting the average of all the token embeddings. Is there any difference between the two sentences in calculation as a whole embedding? No. But in meaning, there is a huge difference. This is one of the limitations of using a neural network that averages all the word embeddings in a sentence. Averaging does not reflect the meaning of the whole context.

Now look at another example. I have two sentiment reviews related to a movie. The first one is short: "Just watched the movie, loved it." The second one is longer and gives more information. How do we deal with variable length sentences? This is a challenge when using neural networks. We also should not treat each word as independent; that is not how natural language behaves. Based on all of this, we need a model that handles variable length input.

We need to treat input not as independent tokens but as a sequence of words. We need to represent natural language as a sequence where the order is very important. We also need to share information or share weights during training. In a standard neural network, each input has its own weight, and they do not share weights. When we share the weights from the first word to the second word, this builds our knowledge about the sentence. We need to deal with language as a sequence, not as individual independent tokens. We need to share the learning and treat it as a sequence.

Now, universal approximation theory. A neural network, when it was introduced around 1940, was proven to satisfy the universal approximation theory. What does this mean? In classification, if you have data, you can sample it as points in a space. During classification, we try to put instances with similar features in the same group. We try to find the boundary in the data that classifies the instances. Using a neural network, you can find even very complex boundaries. Deep neural networks can do that even more efficiently.

That is why the idea of deep neural networks came in 2006 by Geoffrey Hinton from the University of Toronto. He started to introduce the idea of deep neural networks. Actually, from around 1980 to the early 2000s, not many people used neural networks. At the beginning everyone used them, but during that period, other techniques like random forest and support vector machines came along and had better performance than neural networks. Because of complexity and training time, many people stopped using neural networks. But after 2006, when they started developing deep neural networks, it became the basis for many architectures as you will see.

The advantage of deep neural networks over standard neural networks is that they can utilize big data. In your machine learning tasks, without deep neural networks, you have a limitation on the amount of data. After a certain point, the model will not improve if you increase the data. This is the nature of standard machine learning models; they are not able to deal with big data. Deep neural networks can handle a huge amount of data and utilize it. They still benefit from increasing amounts of data. You can try this: in your machine learning course, take a specific amount of data and then try to increase it. At some point, your model will not benefit from additional data and the performance will not increase. But using deep neural networks, which came in the era of big data, they can handle huge amounts of data. This is the advantage of deep neural networks.

In deep neural networks, you have many types. Convolutional neural networks, CNN, are related to computer vision. Recurrent neural networks, RNN, are used for time series data. Time series data has a specific characteristic: each instance depends on the previous one. What about our sentences? They have the same characteristic. That is why they decided to use recurrent neural networks to build language models, because RNNs are designed for dealing with sequences of data, and they overcome the problems related to standard neural networks.

As you can see, we always call recurrent neural networks a stateful computation, because each time we compute a new step, it is a combination of the input at the current time step t and the previous hidden state. At each time step, we predict the output based on the current input and the previous hidden state. So we now have information propagated from previous steps. At each hidden state ht, the computation is a combination of the weight matrix of the input and the weight matrix of the previous hidden state. This is how the recurrent neural network works.

How do we train a model using a recurrent neural network? Starting from the left, we feed our tokens one by one. At each step, we use one-hot encoding and the embedding layer. Then we compute a hidden state for each step and compute the loss. When we reach the end of the sequence, the total loss is the sum of all individual losses at each step. This is how we train an RNN.

For each sentence, we do the same: we feed the sentence step by step, compute the hidden state, and compute the loss for each step. Then we compute the total loss at the end. The backpropagation comes back to update the weights. This happens during training. It is self-supervised: when we feed a word, we know the next word is the target. We compute the loss based on this, and the model learns from it.

During testing, when using the RNN as a language model, we feed our text. Based on the learned weights, we can compute the probability distribution and determine which word should come next. The model utilizes its learned parameters to predict the next word.

This is the difference between a standard neural network and an RNN. In a standard neural network, you feed all the words at once and have one hidden layer. But in an RNN, at each step, we have a hidden layer. We carry our information from step 1 to step T. We always propagate all the information because the output of each step depends on the hidden state of the previous step.

There is an interesting website you can check. They trained a neural network model with a huge number of Shakespeare speeches, and then gave the model a prompt, and the model generated a speech in the style of Shakespeare. The result was not very accurate. It generated text, but it was not coherent. Still, it was a good result. You can find that the model sometimes predicts unrealistic words, which is what we call hallucination. When the model cannot find the correct data, it produces hallucination.

During forward propagation, as we read the words, the hidden state is updated at each step. After the forward pass, we compute the total loss, and then the backpropagation goes back from the last step to the first step to update the weights. The backpropagation in an RNN is called backpropagation through time, because it is time dependent. For each time step, the gradient at that step depends on the gradient at the next step. What is a gradient in a neural network? It is the derivative of the loss function with respect to the weights. The weights are the parameters of your model. The gradient represents the rate of change of the loss function with respect to the parameters.

How do we update the weights? The new weight equals the old weight minus the learning rate multiplied by the gradient. This is how we update the weights at each step.

Now, consider a recurrent neural network with four steps, a sequence of four words. At the end, we compute the loss function, and then we propagate this loss through backpropagation to all the previous tokens. If we compute the rate of change of the loss function with respect to the hidden state at step 1, h1, we compute it based on the chain rule. The rate of change of h2 with respect to h1 is multiplied by the rate of change of the loss function with respect to h2. We can further expand using the chain rule: replacing with the derivative of h3 with respect to h2 multiplied by the loss with respect to h3. We continue: h4 with respect to h3, and the loss with respect to h4. This is how we compute the rate of change of the loss function at step 1.

Now, imagine if these derivative values are small. The gradient will become smaller and smaller. In any learning process, when the gradient becomes very small, the model will not learn, so we stop the process. If these values become very small, it means there is no dependency between the first step and the last step according to the gradient. There is no updating of the weights. It means there is no dependency between these two. This is one of the drawbacks of RNN backpropagation, called vanishing gradient. Every time we compute the gradient of a step, it is a product of all the derivatives from the subsequent steps. The tokens near the end of the sequence learn more than the tokens at the beginning, because the gradient starts to diminish. This is the nature of RNN and one of its drawbacks as a language model. We cannot utilize the far away information to predict the next word when the sentence is long.

As you can see, the gradient will decrease as we go from the last step back to the first step. Look at this example: "When she tried to print her ticket, she found that the printer was out of toner. She went to a stationery store to buy more toner. It was very overpriced. After returning the toner to the printer, she finally printed her..." What do you expect? Ticket. If the model is able to remember "ticket" from earlier in the passage, then it can predict "ticket" here. But due to the vanishing gradient problem, the model will lose the information from far back. This is one of the drawbacks of using RNN as a language model. The dependency between "ticket" in the second step and the target "ticket" at the end cannot be found by the model.

This is a problem in RNN. The model cannot remember the information from the early steps or be corrected by the later steps. Then they started to develop another architecture that overcomes this problem, and this architecture is LSTM, Long Short Term Memory.

RNN and LSTM both work with sequential data, time series data, where the order is very important. Stock market data, for example, is one type of time series data. I remember working on a research project with one of my colleagues on sun spots. Sun spots follow a time series pattern. Every year, the number of sun spots changes, and based on this number, everything related to the weather and environment is affected. Forecasting the number of sun spots year by year is very important.

Long Short Term Memory tries to overcome the problem with RNN where we are not able to remember the dependencies from distant steps. They did this by introducing three gates. LSTM was invented in 1997, and it introduces a forget gate, an input gate, and an output gate. Using the forget gate, the model can decide which data it wants to forget from the previous states because it is not important for predicting what is going to happen. Then we have the input gate to update the memory state at every time step.

LSTM introduces a new component, which is the cell state, also called the memory state. In a standard recurrent neural network, at any step we have the input and the previous hidden state. LSTM introduces another component, the cell state. The forget gate decides what to forget, the input gate decides what new information to add, and the output gate produces the output. The input for each step is the current input, the previous hidden state, and the current cell state. The output is the output plus the updated hidden state and the updated cell state. At each step, the cell state is updated based on the data.

A gate means on or off. On means we allow the data to pass through. Off means we do not allow the data to pass through. This is the basic concept of LSTM.

I will try to simplify the architecture by dividing it into specific steps. At time step t, the inputs are: the current input xt, the previous hidden state, and the previous cell state (which is the memory state). Using the previous hidden state and the current input, we compute the forget gate using a sigmoid function. The sigmoid function outputs values between 0 and 1. A value near 1 means this information is important, and a value near 0 means this information is not important.

The forget gate determines how much data we need to forget from the previous state and how much we need to keep. The forget gate output is multiplied by the previous cell state. If you multiply two vectors element-wise, you combine the information. Multiplying the forget gate output by the previous cell state decides which information from the previous state to keep and which to discard. Higher values near 1 are kept, and lower values are discarded. This is why it is called a gate.

Next, we compute the input gate to update the memory state. Using the previous hidden state and the current input, we compute the input gate as a sigmoid function of the weights of the current input and the weights of the previous hidden state. The input gate decides what new data needs to be added to the current cell state and what data does not need to be added, based on the current input and the previous hidden state.

We also compute the candidate cell state. The candidate cell state, denoted c hat, is the tanh of the weights of the previous hidden state and the current input. The tanh function outputs values from minus 1 to 1. We create a candidate new state based on the current input and the previous hidden state.

Then we update the cell state: ct equals ft (the forget gate) multiplied by ct minus 1 (the previous cell state), plus it (the input gate) multiplied by c hat (the candidate cell state). Now we have a new memory state. In the first part, we use the forget gate to decide which information from the previous state is not important and should be forgotten, and which is important and should be kept. In the second part, based on the input gate and the candidate state, we add new information to the cell state.

Once we compute the new cell state, we compute the output. The output gate decides which information we want to include in the new hidden state. Using the previous hidden state, the current input, and the current cell state, we update the new hidden state.

This is the internal structure of an LSTM neuron. For each time step, we have a forget gate that decides how much information to delete, an input gate that decides how to update the new cell state based on the current situation, and an output gate that decides which information will be sent as output to the next step.

To summarize: the forget gate is computed using a sigmoid function. The input gate is computed similarly. The candidate cell content is computed using tanh. The previous cell state is updated with the new content. The output gate is computed, and the hidden state is determined based on the output gate and the new cell state.

This is a very good resource for LSTM. I recommend that you go to these resources and try to read more about LSTM. But this is the idea of LSTM. It is not a perfect solution. In the next lecture, we will talk about the shortcomings or limitations of LSTM in creating a language model.

Actually, LSTM has a more advanced version called the Gated Recurrent Unit (GRU). But there are still limitations when using LSTM, and that is why they started to find another architecture. The idea evolved step by step, as you will see in the next lecture, to arrive at the transformer. The transformer architecture, as a language model, tries to overcome all the problems related to using deep neural networks as a language model.

In Keras, you can use LSTM simply. Your model is sequential, and then you add an LSTM layer or a GRU layer. You specify the parameters, add the second layer, then add a dense layer and the activation function. Again, the code is there; just use the function, but you have to understand how it works.

I included some code examples relating to how to create a language model using LSTM. We can use it for classification. I created a small corpus, tokenized it, and then tried to train the model using LSTM. I created a sequential model with a softmax function. I gave it the seed "deep" and asked the model to generate 10 words after this seed. The output was "deep learning," and as you see, it is not coherent text, maybe because the corpus is very small.

I will publish these examples on the practice space so you can see how to use LSTM with GloVe, how to extract the embeddings. I tried using GloVe embeddings as the embedding layer before the sequential model. The performance is different because the pre-trained embedding model carries more semantic meaning. The generated text felt a little more coherent than the one without pre-trained embeddings.

This is not a perfect language model. The modern large language models do not use RNN or LSTM. But the idea of the transformer, as you will see in the next lecture, is based on these models. They took RNN and LSTM and tried to build another architecture on top of them. This is how the idea evolved. No one started from scratch. It always started by understanding what happened before and then building on top of that. The idea of large language models comes from this.
