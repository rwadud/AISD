This is the nature of a standard machine learning model. They are not able to deal with big data. Deep neural networks, on the other hand, can deal with a huge amount of data and still benefit from it. To try this, in your machine learning course, take a specific amount of data and then try to increase it. At some point, your model will not utilize this additional data, and the performance will not increase, because this is the nature of standard machine learning. But using deep neural networks, which came in the era of big data, they can handle and benefit from huge amounts of data. This is the advantage of deep neural networks.

In deep neural networks, we have many types. CNN, which is convolutional neural networks, is related to computer vision. RNN, which is recurrent neural networks, is used for time series data. Time series data has a specific characteristic: each instance depends on the previous one. What about our sentences? They have the same characteristic. That is why they decided to use recurrent neural networks to build the language model, because RNNs are good at dealing with sequences of data, and they overcome the problems related to standard neural networks.

As you can see, we always call recurrent neural networks a stateful computation, because each time we compute a new step, it is a combination of the input at the current time step t and the previous hidden state. At each time step, we predict the output based on the current input and the previous hidden state. So we have some information coming from previous steps. At each hidden state ht, it is a combination of the weight matrix of the input and the weight matrix of the previous hidden state. This is how the recurrent neural network works.

How do we train our model using the recurrent neural network? Starting from the left, we feed our tokens one by one. At each step, we use one-hot encoding and the embedding layer, and then we compute the hidden state for each word and compute the loss. When we reach the end of the sequence, we compute the total loss, which is the sum of all individual losses at each step. This is how we train an RNN. For each sentence, we feed it step by step, compute the hidden state, and compute the loss at each step. Then we compute the total loss at the end. The backpropagation then comes back to update the weights. This happens during training. It is self-supervised learning: when we feed a word, we already know what the next word should be, so we compute the loss based on this, and the model learns from it.

During testing, or when using the RNN as a language model, we do the same with our test data. Based on the learned weights, we can compute the probability distribution and determine which word should come next. The model utilizes its learned parameters to predict the next word. This is how the language model works.

This is the difference between a standard neural network and an RNN. In a standard neural network, you feed all the words at once and have one hidden layer. But in an RNN, each step has its own hidden state. We carry our information from step 1 to step T, the end of the sequence. We always propagate all the information because the input of each step is dependent on the hidden state of the previous step.

There is a website where someone trained a recurrent neural network model on a huge number of speeches by Obama. You give it a prompt, and then the model generates text. The result was not very accurate, but it is interesting to see. You can find that it generates interesting speech. The model can predict words, but sometimes, like what we call hallucination in LLMs, when the model cannot find the correct word, it just puts anything.

The backpropagation in an RNN is called backpropagation through time, because it is time dependent. For each time step, the gradient at that step depends on the gradient at the next step. What is the gradient in a neural network? It is the derivative of the loss function with respect to the weights. The parameters in a neural network are the weights. The gradient is the derivative, and the meaning of the derivative is the rate of change. We compute the rate of change of the loss function with respect to the parameters, which are the weights.

How do we update the weights? The new weight equals the old weight minus the learning rate multiplied by the gradient. This is how we update the weights at each step. The learning rate is something that we set before training begins.

Now, consider a recurrent neural network with four steps, a sequence of four tokens. At the end, we compute the loss function, and then we propagate this loss through backpropagation to all the previous steps. If we want to compute the rate of change of the loss function with respect to the hidden state at step 1, h1, we use the chain rule. The rate of change of h2 with respect to h1, multiplied by the rate of change of the loss function with respect to h2. We can expand further using the chain rule: the derivative of h3 with respect to h2, multiplied by the loss with respect to h3. And then h4 with respect to h3, multiplied by the loss with respect to h4. This is how we compute the rate of change of the loss function at step 1.

Now, imagine if these derivative values are small. What could happen? The gradient will become smaller and smaller. In any learning process, when the gradient becomes very small, the model will not learn, so we stop the process. If the gradient is very small, there is no updating of the weights, which means there is no dependency between the first step and the last step. This is one of the drawbacks of RNN backpropagation, called the vanishing gradient problem. Every time we compute the gradient at a step, it is a product of all the derivatives from the subsequent steps. So the tokens near the end of the sequence learn more, and the tokens at the beginning learn less, because the gradient diminishes as it propagates back.

This is one of the drawbacks of using RNN as a language model. We cannot utilize far away information to predict the next word. As you can see, the gradient decreases as we go from step four back to step one.

Look at this example: "When she tried to print her ticket, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After returning the toner to the printer, she finally printed her..." What word do you expect? Ticket. If the model is able to remember "ticket" from earlier in the passage, then it can predict the correct word here. But using RNN, due to the vanishing gradient problem, the model will lose the information from far back in the sequence. This is one of the drawbacks of using RNN as a language model. The dependency between "ticket" at an earlier step and the target word at the end cannot be captured by the model.

This is a problem in RNN. The model cannot remember information from the earlier steps. Then they started to think about another architecture that overcomes this problem, and this architecture is LSTM, Long Short Term Memory.

LSTM works very well with sequential data, time series data, where the order is very important. Stock market data is one example. I remember working on one of my research projects involving sun spots. Sun spots follow a time series pattern. Every year the number of sun spots changes, and based on this number, everything related to the weather and environment may be affected. Forecasting the number of sun spots year by year is very important.

Long Short Term Memory tries to overcome the problem that comes with RNN, where the model is not able to remember the dependencies from distant steps. They did this by introducing three gates. LSTM was invented in 1997, and it introduces a forget gate, an input gate, and an output gate. Some say four gates. Using the forget gate, the model can decide which data it wants to forget from the previous states because it is not important for predicting what comes next. Then we have the input gate to update the memory state at every step.

LSTM introduces a new component, which is the cell state, also called the memory state. In a standard recurrent neural network, at any step we have the input and the previous hidden state. LSTM introduces another component, the cell state. The forget gate decides what to forget, the input gate decides what new information to add, and the output gate produces the output. The input for each step is the current input, the previous hidden state, and the current cell state. The output is the output plus the updated hidden state and the updated cell state. At each step, the cell state is updated based on the data. A gate means on or off. On means we allow the data to pass through. Off means we do not allow the data to pass through. This is the basic concept of LSTM.

I will try to simplify the architecture by dividing it into specific steps. At time step t, the inputs are the current input xt, the previous hidden state, and the previous cell state, which is the memory state. Using the previous hidden state and the current input, we compute the forget gate using a sigmoid function. The sigmoid function outputs values between 0 and 1. A value near 1 means this information is important, and a value near 0 means this information is not important. This is why we use a sigmoid function to compute the forget gate: it determines how much data we need to forget from the previous state and how much data we need to keep.

The forget gate output is then multiplied element-wise by the previous cell state. If you have two vectors and you multiply them element-wise, you filter the information. So multiplying the forget gate output by the previous cell state decides which information from the previous state to keep and which to discard. Values near 1 mean we keep the information, and values near 0 mean we discard it. This is why it is called a gate.

Next, we compute the input gate to update the memory state. Using the previous hidden state and the current input, we compute the input gate as a sigmoid function of the weights for the current input and the weights for the previous hidden state. The input gate decides, based on the previous hidden state and the current input, what new data needs to be added to the current cell state and what data does not need to be added.

We also compute the candidate cell state. The candidate cell state, denoted c hat, is the tanh of the weights of the previous hidden state and the current input. The tanh function outputs values from minus 1 to 1. We create a candidate new state based on the current input and the previous hidden state. Then we can use the input gate to decide how much of this candidate state to add.

The cell state update is: ct equals ft (the forget gate) multiplied by ct minus 1 (the previous cell state), plus it (the input gate) multiplied by c hat (the candidate cell state). So the new memory state has two parts. In the first part, we use the forget gate to decide which information from the previous state is not important and should be forgotten, and which information is important and should be kept. In the second part, based on the input gate and the candidate state, we add new information to the cell state. Now we have a new cell state ct. Each time step, we repeat this process.

Once we compute the new cell state, we compute the output. The output gate decides which information we want to include as the new hidden state. Using the previous hidden state, the current input, and the current cell state, we compute the output gate and update the new hidden state.

This is the internal structure of an LSTM cell. For each time step, the forget gate decides how much information to keep or discard, the input gate decides how to update the cell state based on the current situation, and the output gate decides which information to pass as output to the next step.

To summarize: the sigmoid function is used to compute the forget gate, which is combined with the previous cell state to decide what to forget. The input gate is also computed using a sigmoid function. The candidate cell content is computed using tanh. The previous cell state is then updated with the new content. Finally, the output gate is computed using a sigmoid function, and the new hidden state is determined based on the output gate and the updated cell state.

This is a very good resource for LSTM. I recommend that you go to these resources and try to read more about LSTM. But this is the idea of LSTM. It is not a perfect solution. In the next lecture, we will talk about the shortcomings and limitations of LSTM in creating a language model.

LSTM has a more advanced version. What is the next version? The Gated Recurrent Unit, GRU. But there are still limitations when using LSTM, and that is why they started to find another architecture. The idea evolved step by step, as you will see in the next lecture, to arrive at the transformer. The transformer architecture, as a language model, tries to overcome all the problems related to using deep neural networks as a language model.

In Keras, you can use LSTM simply. Your model is sequential, and then you add an LSTM layer. You can also add a GRU layer. Then you add a dense layer with an activation function. Using Keras, the code is straightforward: just use the functions, but you have to understand how they work internally.

I included some code examples demonstrating how to create a language model using LSTM. You can also use it for classification. I created a small corpus, tokenized it, and then tried to train the model using LSTM. I created a sequential model with a softmax function. I gave it the seed "deep" and asked the model to generate 10 words after this seed. The result was not very coherent, maybe because the corpus is very small. The number of epochs also plays an important role in improving the text generation.

I will publish these examples on the practice space so you can see how to use LSTM with GloVe, how to extract the embeddings. I used GloVe embeddings as the embedding layer before the sequential model. The performance is different because the pre-trained embedding model carries more semantic meaning, and the generated text felt a little more coherent.

This is not a perfect language model. The modern large language models do not use RNN or LSTM. But the idea of the transformer, as you will see in the next lecture, is based on these models. They took the concepts from RNN and LSTM and tried to build another architecture on top of them. This is how the idea evolved. No one started from scratch. It always started by understanding what happened before and then building on top of that.
