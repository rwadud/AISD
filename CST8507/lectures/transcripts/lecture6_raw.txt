Well, then we'll not learn if you enter easily. And this is an itch of a normal machine learning model. Okay? They are not able to deal with a big data. So, deeply, you already work, have deal with a huge number of data, and you can utilize it. So still can get benefit of this huge number of it. And to try this, a new machine, a learning course, try to have a specific number, even the head, then try to increase it in the sub limit, in some limit, that your mother will not utilize this date. The performance will not increase, because this is the effect of machinery. Okay? But using in Europe, it work, it comes with the era of the big reason. If you have it working, can't do this. And this is advantage of a neuronimo. Now, in a deep neuronimo. We have many types of deep, uh, DMN, which this one is what you, you learn a new machine, machine. So, yeah. And then our land is certain. And not in and that's it. I think to see and it's related to machine pleasure, of course. Yeah, see, and it's directed machine vision. And INNYP, how news are it? Uh, advance it, the machine down equals. It has a specific use. There was time series. Time series. At work, perfect, it was a 5 CMC. Why? Nine series later has a specific function. Each, any stand, it is all that... What about our sentence? As you say, the functional through, yeah, intelligently. See, that's one, they think it will use, uh, the current neural, each one, to build the language model, because it's good ways dealing with the sequence of data, and it has overcome what's a problem related to neural network. Okay? So, but highlight, you cover it in details. As you see here, we always call a neural network as a stateful computation, because each time we compute the new everything is a complication in between. There's high understanding, at this time, G, and the previous headed state. Okay? And this is the important this. So each time, at time, is time, G1, we compute, why, how, protect, why, based on the current estate, and that radius state. So, we have have some information come from our... And it worked very well at that time. Okay? So each time an HD, any HD, it's a combination between the weight matrix of the M one, and the weight matches of the prettiest hidden state. Okay? And this is how the recurrent neural network. Right? Okay, so how to train our model? Using the current unit network, starting from that. Okay? See, our tockets, one, by one, and when at each step, again, one hot encoding, embedding layer, and then we have, we can compute ahead in layer for each uh, for each wall, and then compute the difference there, not, and then when we start at the end, we go to the end, see, at the end of time, we compute. Then, no, the rose function. And the total loss is the submission of all noses. Okay? This is how we can train RNA, right? For each sentence, you can lose a C. With these a sentence, step by step, we can find heaven speed, and compute the loss for each step, and then we can compute the loss and same, right? And then the back provocation. Come back to, to update, the head estate. This is happened during the train, okay? When we have a super self supervised taste. So, we know when we feed them, we know the next word is student, so computes a loss, based on this. And I'm only trained from this, okay? Okay. And then we use, during the testing, or using the language mode, RNN as a language model, we can do the same, within our tests, and the based on the learning, the weights, we can compete with the probability, what will happen at first? Because this is the modern film during the language. Right? We had the lies, it's parameter, to predict what is an institute. Okay? Right. And this is the difference between NN and RNM. So, and, and, you feel this is all the words, and then you have one hidden layer, and then you can find out what this one, one hidden name. But in RNN, each step will have a heavy weight. In each step, so we put our information from the one to step T, the end of your sentence. So we always propagate all the information because the input of each step is dependent on them, the headed state of the previous step. Okay? Now, I like this website, you can find, you can uh, check it in your, um, In your style. At that time, say it, uh, the neural need one model with, um, a huge number of speech, of obama, and then you say, meet the woman, you work, and then the mother generates as much. to them. Again, the result was not very accurate, but it's not a coherent. But it's, it's, it's a problem. I'm paying with this, okay? You can find that it's a great, interesting speech. Well, the model. Can predict... I'm a rifle, world, world. Him, what we call it in LLM, where the model cannot find the correct data, then you can put anything. Okay? Right. So, this has happened during the back normalation. Okay? So... Judy, reading H word... The head of the state will not update. When the head in space is updated, after that end of the sentence, and the computer told them, uh, probability, and then, We, the back room, I shouldn't get back from step 4, or stick T minus one, T minus 2, and then step one, which is one to one. This is how the back location go. In a sequence, like we read, in a sequence, that that provocation also computed in a pen port. Okay? Right. So, you're a bit work, or in current, you need work, you use more information. Because it's a high division. So, for each bystand, they computer provocation, if we have a graduate of this staff, fent on the gradient of this system. What is the graduate? By the way, in Europe. What do you mean by energy? the error function. Related to? Related to the inputs? Related to the weight. The function role, then the parameters, in neural network, is a weight. Okay? This is a parameter of your model, okay? So, all right, so, as your colleague said that, that... The gradient is at revative, so derivative by the meaning is height of a change, okay? So when compute, the leaf of a chip. This is that the semantic meaning of decorating, the friendship, okay? So this is a late topic. What is the rate of change of the loss function related to the parameter, which is faint, and interesting. Okay? And at the end, how we can compute, how the modern, how we have is their weight, so new wits. It was, huh? It's minus... Wait, it's perfect. Minus? What do we call it, learning rate? Yeah. Okay? Learning great, multiplied by? The river... This is a correct, and this is how we update the weight each button. Okay? And the living rate, it's something that the neural network left during that training. Now, consider that we have this recurrent renewal network. We have one, three, four steps. Sentence, I just for, for, sequence, okay? Now, at the end, we compute that. There was some, okay? And then we try to move again, with us, in a bad conversation, to the rest of the talking team. Okay. So, and this is as an agency for agent, so, for sure. Now, if we're trying to compute, what is the rate of a change of loss function relating to the headaches? So the computer, then, of the most functional, is related to... Okay, now we can help you. We computed this one, Chen Ru. The rape of a change of HMO related to H1, multiplied by the rape of a change of HGO, that was functioned, lead to next book. Is that correct? So, please, all that. Chain mode in the thrift. Okay? Now, have a plan? This is okay. Now, how about 10 computers? Rolling chain again, okay? So I replaced this with relative of HSV, HSV, relatively change of HSV, related to H2, multiplied by the loss, the regulated to the industry. Now, we want to compute. Yes. Using the seed, G. So it's 4, 3, 4, 5, 5. And this is how we can compute the rate of a change of the loss function in a step, one. Okay? This is okay. Right. Now, imagine if these numbers are smaller. What could happen? Usually, in any level, we can process, it means that... The mother were not left, so we stopped. The process. So, this is very, very small. They'll get me into that. There is so much... Yes. If the grade is smooth, there is no updating weight, okay? So it means there's no dependency between these two. And this is one of the rule types of the RNN, I have probation. What we calling benching gradient. Fancy immigration, because every time to compute that region from this step, it's a combination of water. We use all the derivatives of the previous system. So, the tokens near to the end, learn to move, and the toy has a, because the region is not to push. idea? Okay, this is an engine called... And this is one of the lowbacks of using RNN as a language. We cannot. Fine, the previous of utilize so far information, predicts things. In the sentences, oh. Okay? As you can see here, the rainy grates when, and decrees as we go from step four to the step one. Right. So, look at this example, and we have... try to print a ticket... Which... I always tried to paint her ticket, she found that the printer was out of dinner. She went to stationery store to buy Mortonette. It was very overpriced, and at the time, the tumour into the centre, she found a... What accident it was? Okay. So if the model are able to remember the ticket here, then it can protect the system here. But using RNN, due to the benching problem, the moment will lose, the information comes so far. And this is one of the drawbacks of using R&N in as an attitude. Okay? Any question? Okay, so the dependency here between tickets, which is another seven, the step, and the time you take it, cannot find these dependants, or the model, cannot find system events. All right, so... This is a problem in... RNN, so we cannot remember... the further step... that another man for more than update, for making me that, he insisted, then, before he's a step, then they started to think, okay, another architecture, and overcome this. Okay? And this architecture is NST, and long short, deter, then, right? So, do you have a visit? All right, so what is the difference? In RNN and MST. And a, and a, and a food for the food, so... Save the state of the song, or be it... Okay, so... Did you cover anything with machine building? Yeah. Okay, from which way? So understand the architecture, what is going on, what just use it. When do you go on? How are you? Did you do this? No? So, nice to be, so you just finished. What about our course, you forget everything? From week to week, or you still remember. Honestly. In me, don't... when we practice anymore. The lab makes it us to go through the game. And, okay, another thing is that that's why I make the the weekly, the weekly quiz that to keep you up to date. So please. Don't rely on ChatGPT to solve this question. So if just if you're self-time to review the lectures. And I was, okay, so now we understand, and this is one of the, what was that discussion between me and one of my students in the various semester, that he told me during the lecture, I understand everything. Okay? And I'm very happy with this. And after that, during that death, I forgot everything. So this is a natural, okay? If you didn't review, you will lost all the information you gained during the election. So you have to take notes, this is the first system, the action, because I had this experience before. When I am taking, you know, during the lecture, while I read my notes, I remember every single relationship. I never have the protestor mentioned that during the next year. So taking, you know, is very, very important. The 2nd thing is to have to be up to the west septure, okay? Right, so try to be up late. All right, so NSTM... I thought you were LSTN, best? yes. Okay, so he gave you the architecture or just... And he's explained the equations. I don't think the boy is coming, but, yeah, okay, it's a very complicated thing to understand it, even the equation, it is not easy, but at least you should be able to understand what is the background speaking, or the gender, if you're still. What is the improvement? And NSTM? For one point, with a sequence need, tie it seriously, okay? Where the cycle is very important. All of these things, the order, this is, this is, one, one of the 1960s. I remember that I worked in one of my research project. It was one of my, on the sun, school. You know, the sun is fun. It's like a time series there every year. The number of sunny spot changes. And based on this number, everything related to the weather and the environment may be changed. So forecasting, the number of satisfies year by year is very important and I want to ask you. Right, so long, short, the term, memory, trying to overcome the problem, come with RNN, to, they are not able to remember the dependency in the further steps. So, they do that by, they invited in 1997, and then, It was at by introducing 3 gates, or skates, right. They have, can you get a 4 gift kit? Using a 4 gate, they can decide. Which did I want to forget from the previous states, because it is not important to expecting or predicting what is what. And then we have, in this, to update this state for memory state every time. So, LSTTM come with a new extra, Talking or new extra new one, which is the state one. Okay? So, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, So, We have... So we have the airport, so at any step, we have the airport and we have the heading, the previous header state. Now, LTL introduced another MPT, which is the current distinct, or what is the current distinct? Okay. Then this is a forget, and we have a gamebook game to update the new information at every step, and then we have, they are okay to produce out. So the input for each state is the current input, the previous Hidden State, and the current memory skate. And the output is, the output plus what is that, the head instead, and the current. A memory state. So each time with each step, this memory state will be updated based on the data, okay? And it skipped. So what do you mean by it, even on, or off? So on games, allow the data to go off museums means we're not allowed the data to go. okay? And this is against the concept of the NST. And this is the architecture. Did you saw this architecture before? Right. So I will try, as much as I can, to simplify this architecture, so I divide it to a specific steps. Okay? So, This part will focus on this part and then again. So, a time tea, we compete at the beginning, that's, Forget kids. So the endput, at this time, is that, and put as XT, plus the previous head and state, plus they, previous state, which is a memory state. Okay? So this is the 3 ends. Then, using the previous, steady state, and the endput state, B, and U, the only function to calculate that for it to eat. So based on that, currently, and the previous head in the state, we can have a significant function, which the values won't function between, 0 and 1. So the higher value is written near to one means this is important, and the zeos means this is not important. Okay? That's why we use here as a white function. Okay? To compute that, forget how much data, we need to forget from the previous and how much data we need to keep it. And this is a rule holder, forget it. And this is will be what the white boy, the previous 70. So multiplication means we add the information. So if you have 2 vectors, and you multiply 2 vectors, that means, and the information in the topic. And this is what we have. So, multiplying by the previous estate, we can now decide which information from previous estate, we need to eat it, and which information we need to discuss. Because the output is between you and one. So high advantage here to one, we give it no advantage. That's why, you know, this is a gift. Okay? And this is with having in this how to complete the pocket. Now, the other part is, we try to compute for update. The state, the member state, okay? How, we using, again, the previous head and state, and the end of state, we can use or update the input rate, a signal function between the rates for the current state, and the way it's for the previous heavens. So, current state, each input has its weight, okay? And this is how we can compute that. Aren't able to get. based on the radio saving state and the data at this time, okay? So this is how we can compute them. Use a signoid to create or update. Yeah, get more. So every day we decide by this vector, what is the data that we need to add it to the current estate and what is the data that we not need to add? This is the one, the current, the elbow, plus the greatest head of state. And we can use this data to calculate or find the candidate to the state. Based on our current state, what is the candid state? The rotation, the rotation is... So, C head is attached to, H two, the weights, all fainted, make us head and state, and the current... input state. Okay? Candidate, hash, okay? And in Spanish, what is up? Usually... minus one, two... Ready for minus one to one. So that's why we use that? So we can create a candidate. New state, based on the current, and one and the various state. And then we can use how much you need and add it. to the previous state. So, the city is empty. Forget it, multiplied by CT, minus one, which if I'm from previous, has empty, multiplied by CT. And then now, we have a new memory seat. So the new memory state, and the first step we use a forget kit, to decide which information it's not important. We need to forget, and which information is important, we need to keep it. And then base of the airport, and the previous hit in the state, we carry it, huh? Candidate state. And then we add this information to that state. Now we have ST, new ST. related to the current, at T, at ST. So each time we repeat this, okay? Now, once we continue XD, now we want to do, write it out. Okay? What is output, the output, as social? It's a previous set. Plus, ahead of the previous thing. This is what we need to feed to the latest XT. Okay? This is bad using a forget it, so that, uh, sorry, the output kit. And I would get it decide which information we want to, as a new hidden estate. So, using, again, all the previous state and able to stay, and the current. CG, we can update the new head resting. output the new head distinct. Exactly. I hope so. It's very complicated, but this is the... Intend the structure of an NSTM new. And I think that that's the, okay, so this is related to one step. What happened in each type? Okay? So, the party, for the time, once, how much information, we need it, and then the endput? Uh, G, uh, N would sell, or N would car, or N would get, set the site, how we can update the city, the new state, these on current situation, and then I would get that the site, which information we live? Uh, because, and we, as an output to the next step. Okay? Right, so, again, to try to simplify it. So, this is what it is to compute the word that, okay, the public did, you think this is? I said, what function? X, always connection will forget some content when it's combined with a brain cell, and this is to compute the gatewood gate, and this is to compute the new self content, and this is to update the new, the previous state with the new content. And this is to compute out, okay? And this is true to sign, which, what is a fun pent of the heather state. Okay? This is a very good resources for NSTM. Uh, I recommended them to go to these resources and try to read more about everything. But again, this is the idea of NSTM. It is not a perfect way. In the latest section, we talk about the shortcoming limitation. And if you can in career, a language. So, but it is, actually, NSTM, and there is more advanced version of NSTM, there holds... You will give them what is the next version? Uh, 8 to... Even then, what is the next version? This... You get it... Eat it for guarded. I can't remember, but there is a lot of limitation using NSTM. That's why they started to find another architecture. And then, The idea evolved with step by step, as you will see any selection to come to the transform. That transformal architecture as a language model, try to overcome all the problem related to deep new and network in as a language model. Okay? So this is, uh, space. What is the name? I cannot believe that it's 8 or something like that. Now, And yes, you can use the NSTN. Simple and easy. What you have to do is, the stuff, I think it is the same, like, she learning, your mother is friendship, and it will be sequential. And then you add an NSTM. What is what? The Gated Record Unit? Okay, yes, maybe... Jerry in Poland Division. Okay, the second layer here. R N, N, we have to have a dense layer, and then with the activation option. Using a caress, a code is there, just use a function, but you have to understand how to endure. Okay? This is what we mean by creating a language is what they use in MST. I don't have Okay, so in any class code, And you know that some code, dating how to create a model, uh, large language in what they're using, NSTM. And even use it for a classification. So I can be in a small arms, and documenting both, with the steps, and then try to feed this for them, or tree, with someone that you can see, NST. I think there's so many here, I can need some attention. It's a sort of nice function, and... Okay. So, I gave it the seat as a G, and I asked the morning to carry it 10 words after this or the G seat. So, and this is what was... Keep learning that is, as you see. It's a lot quiet, thanks. Maybe because the number of the purpose is very sweet. While, uh, the heat pops play that important to me to improve, that is the generation, okay? I will publish this on the pride space to see all this example and be able to have to use NSTM with a glove, how to impact extract the embedding. Okay, so here's the embedding layer from health, and this is considered as an embedding layer before sequenction moment. So, he was more than I can eat at the moment. Okay. So, this quenching woman, where I thought that embedding delayer from that block. So, and then 3 in the morning, then I train the morning, of course. The performance, different, based on the embedding model, have more semantic meaning, but also the same, it generated. I feel it has been, and why I think it's just this explorer, but a little bit more coherent. Deserve the answers. Okay? No, this is not a perfect language, modern, the large language, more than now, didn't use the RLN or MLSTL, but the ID of transformer, as you will see, the next lesson. It's a end on this mod. So they take the argument and NSTM, and they try to build another architecture based on them. So this is how the idea is evolved. No one just struck from scratch. Start always by understanding what's happening before, and then then in a top office. And the idea of large language more than this, as we see, like, discussion. Any question? Is that clear? Okay. All right, so we can start our place, little barriers, so... Federal documents.