ity of what is filering, and even the can energy addiction for addictive just fromages, right? And how that. So you with these Today, we are going a little bit deeper into future detection, so this is one of the critical or crucial part of the machine system. So without detecting feature from the teenage feature not to any further processing, right? So we need to identify a new feature from the image. So today we will us how the beach is like the dation can be trib in different different. Today, we have a lot of topics, but we cover it, so the first one is the segmentation and binary image. Last week, also, we have been seen we learned about different stages, right? So quite of the stage was like, you know, psychization and I for you remember, I mentioned it is that ouroug first task like out of those nylon steps. So we will see how segmentation can be done by you see using a threshold. You already did thresholding for your josing pan. Some of you at least. sub cover by how it differs between basic and aty fresh shorting, so how we can apply this to the segation of image. Then we will see what code movors means. It's a bit of like concept that helps you do understand how to blow shapes on how to a shape from an object or shape from the image. Then we will see a feature detection introduction and then the basic of dolphin. And the main idea of all the feature detection is a kind of forage and we will see the equion and all the stuff for computing vari And then comes different types of feature detection take place that we use vaccineer and OB, which comes under advance to each detection. And then we talk about the feature thiscriptures, some of you have already good about, descriptor because you have seen down already, right? Then finally, towards the end, we will see featured matching. It's nothing what we are matching the features I find to be dangerous. Okay. So finally, the deb bit about rating and featression and future training featureation. So these are today. It's rapid segregation and binary image. So last week, we have seen about the threshing, right? The showing can help us do, you know, identify the problem feature from the image. So if you look at this image, you can see the input image is a coloured image, which has a lot of like, you know, shapes like circle or square connecter squares and bar, one is always shape, etc. But if you see at the outward, this out represes the segmented image or threshed image. So if you look at the out, you can see not if you look carefully, you can say not all the shapes are being transferred in the output issues, right, especially around this area, this blue colour space, this object is spacing here, right? And half of this red bag is missing here. So which means we don't need all the details from the input image for the processing. We need like maybe a portion of it or you know, some of the objects from it. So how do we design is like by defining it threshold for choosing what we need for the processing. So that's what is spent here. So what's admination is doing, it expands objects from image for further processing. So we are extracting, particular images from the sorry practical objects from the image. Okay, and if you can clearly see the artput is a binary image, right? It's just a black and white image. It just contains zeroeres and ones. So output of sementation is typically a binary image. Of course, it has like valus of zero and one and one indicate all the piece of the image that we want to keyboard that we want to use for further processing. And zeros represent all the part which we don't want to like, you know, we don't want them. It's not that necessary for yourself to the processing. So if we can neglect that point. So, by looking at this, you can get an idea, like the output image or the threshold Image Act as a mask to the source image, right? If you put this mask, you can see what is leaded from the image and thevided part will be masked, right? So it act like a mask to that source image. So, as I mentioned, segmentation is one of the critical tasks because in major processing, we need input like binary images. So most of the cases in our application be converted into grace K or, you know, like we convert it into binary in order to make the processing much easier and also that's why we say that segmentation is critical because it generate binary image. And we need this binary image for many of the image processing algorithms. So it actses the mask and then one of the typical way to get a binary image is thresholding. So you have seen when you apply a basic or simple thresholding, you get binary image, right? Anything about the threshold we assign white and anything below we assign flags. So we can just black and white image. So thresholding is a type of segmentation that looks at the value of the source in image. So it looks for each epixel in the source in image, and it is compared with its global value or a you know a central value to decide whether a single pixel or group of pixel should have a value of zero or 1.. So depending upon this comparison, we assign zero or one, two, a group of pixel or a single pixel. Right. So this is the basic concept of segmentation. So segmentation can be done by technique life thresholding. And the main is like you have to keep in mind that the output of the sementation is always in by the image. Yeah, so this is a little bit like we have disc last week also about how to do the shorting, right? So this is how we performed facolding and how we get the bary image. So the left side shows input metrics and right side shows the output metrics. So in the input metricacy, you can see the first column shows all COS, second one, all 64, the third one 190 and 255. So in the first exam, we take or we define our threshold as 128 a random number, and then we combare each pixel in our infrtrics with that 128, and then, depending upon the decision like, for example, the first pixel is zero, right? So when you come back with 128, it's less than 128. So we keep that pixel at zero in the output. Similarly for the next two pixel 64 and it is also less black 120 So we keep that pixel also zero in the output matrix. And when you go to the third, you know, third qu, it's 190 and it's of course, greater than 128, so we make it Similarly to 255 is also greater than 128th, so we keep it one. So after likewise, we process over the pixel in that input mentric and we get a final metrics like this. So from this metrics, did you get an idea of like what that would be? This is like a binary be, of course, and not really that it represents an edge because an edge is something like, you know, a part of the image which has a, you know, a drastic contrasting in the intensity value. So from zero to one, is say drastic change, plant to whiter. So it indicates there's an edge. Okay, so just an additional AA, but this is how, like, you know, the metrix willress in their corner and stuff like that. Yeah, so that's the first example. when you come to the second example, we have the same input metrics. But this time we use a binary threshold, which is 64, basically half of the plus one right, first time we use now4. And here also we do the same comparison, Hixel in the input metricis combined with 64 and if it is less than, 64, we keep zero, and if it is greater, we keep one. So when you look at the output metrics, you can see the difference, right? Now we have more details and less like unwanted stuff, right? So, depending upon the application, like for example, if you are doing an immediately ageing applications, we need more media at that time, we consider small thresholds because we need more detail from the image. But for any other, like, you know maybe if you are developing a filter, for your camera or something, some blurry, like portrayboard or something, you can be said, like, maybe you can go for a much more threshold. So it depending upon your application, you can decide like what kind of threshold you want to use, but the main idea is that depending on the threshold, the output will change. Okay. So this isn't Basic fish morn from an absol BR CB2 and we are Yeah, we are reading in. And I'm going with the excscape by passing, so I have told you in that I one way to work kind of space and he can specify from BDR to relay, you can do this with. gring your image, you can have zero, which will charge you do this. And then you are showing the image and now I am computing the high andth of my input image since this is a coloured image I use as my swe because we have a third by our, which is a number of channing how to explain last name. Then I define a binary array, which just see, which is basically the same height and ri of m I just define safe size, like my v image, but just with zero. And I define the threshold does 85, a random number, and I'm travelling across the raw hand through a formula and I'm looking for each, you know, each index, like zeros, you know, and each mixel technically, each mixel in the image, come here with the threshold. And if it is greater than the threshold, we assign the value to 55 for graphics means like the visions, they take 55 as wide, zero interior BC explain likeide wide. But for the ko, like the 55 as one right. So we use of here to just to say we have making it white. And this is this law b and because you take some time to you know, like tramers, the two follow loops and you know, So it's a slow bangery process. I' ultimate for this is you already right South that you love to you know, this is the fun, we just replacing this like handquered pro, you can simply use CB2 threshold and pass the foot image and the threshold. Here we have 85 and5 is the value to be replaced. If it it is passing the threshold, if the pixel is passing the threshold, you will aside And this is like threshold bries type of the like thresholding So if you run this program, you can see it takes a lot of time. So basically the small b NC, we better rate half the same out There's no difference, but three O C I learn to much faster, so recommend to do that I'm just showing you like I'm going to behind behind that direction. Yeah, so this is my watching image, which is quite into play sk and then this is the art for the I born and this is the word the overse and the basically the same one. Yeah. And now we have an acting facrity. So, you have seen, like, binary thresholding, it just, like, you know, it's not great for all the scenos. It's okay if you have like a good cing age, it can clearly sit by your age like, you knowales and your background. But if it is if you're having an image, which is not having a even lighting some sort of shadow or something that is presenting your image, then it's very hard for this varying to processage. It's more likely that you've been with a lot of details from that. So that's why we need another solution called adaptive fresh day, so adaptive thresholding, as I think I mentioned this last week as well. So instead off several thresholding values. We have like multiple thresholding, threshold value for the same age. So we divide our image into subr s, for each sub region we will have a different threshold variants. And how this threshold value is calculated is by using, you, like there are two functionctions, maybe, after you mean adaptive. So these are the two ways we can like perform the adaptive Krish day. So instead of taking a super global value, areitional comparison, the adapted thresholding will use its local neighbourhoods. So we specify how much will be the size of the neighbourhood, let's say, or like four or five, so it takes that much neighbouring fixes and then in perform a me. And it fixs ad that three me, itute the mean value that pixels and replace that and compare each pixel that region with that meat value as. And if it was a Gaussian mean, then what you do is like you do a Gaussian weighted you apply the Gaussian question and then Gaussian weighted, some of that local neighbourhood is used as the threshold for comparing each pixel in that bridge in. by doing this, you can't coun issues like, you know, unable like. As I mentioned, it can't place a threshold value for each subregion instead of for the whole. And as I told you, there are two mets and have to me and and we can see like grow what's the difference of like we can see like why it just use spread for engages that even. So I'm using, like, I'm showing you all the three showing like this. First is the one, the buy one we have this sort of 70 and we are up to special fin basic this data that 70, we are to zero and the calcul is the function that that function. This is the b helps us to generate the binary based on this. and we specify our great and one is to it from the main because otherwise it's more likely that more of the pizza be get wider. So just to prevent that, we always use that Hel, which ranges between what you tell me, we find it from the fish after that way, we apply the otherm to. So this is what an active push word for shorty and be used to saying nush but here instead of fishing, we used for computing the threshold and seeing the tary is used to form the writing the binary m based for theosency is the box size and what is the forest that by called the mixer. for an idea of So, uh, I input image had some issue with the lightning. So you can see if we have used the binary thresholding, the fing is out a binary would say, you know, like these or more darker because it has some shadow. In mind putting it. So it's not showing any important details here. and you could see the bottom and Ross you see here. Yeah, it's not that great. And when I go to main threshold, it gives a good result, but still you can see like the text the door, the door it's not that clear right? In this scenario, I think good, because I can read the pleasures, even the numbers and the other clear. So that's why we say, like we refer to you our media recommended to use an app sure if you haveput images with the uneven landing. Any doubts? Okay. Now let's talk about pondos. Kondo it's similar to it detection. So if you look at theput image, you can see if we have S word, and when you do the couno detection, you get an image like this, which plots the shape of that weapon, right. So you could, like easily get confused with agation would also have like detected this particular shape, right? But the difference between enginection and photo is that cho always form a close to path. Okay, it gives exact shape of the object, but in interation, you have seen that there is it's log guarantity that you will get it across to but sometimes some part of the image is like, you know, the age will be visible. But it's not always a cross above, right? But for four, it's always will be a cross to path. So this will be useful for shape analysis. cur that joins a set of points, which are glossing an area that have the same in. So if you look at this image, you can see same intensity for the spawn, the different portion, right? So if it covers all the, you know, all the points that encloses an object which have the same, in the city or same colour of the, of the object. So that's what it's mentioned in the second point, the area of uniform colour or intensity forms the object that we are trying to detect. So what we are trying to detect is the object which have like, you know, uniform intensity, right? So in this case it's the so which have like uniform indensity, only the handle will have like more tagger coloured wed. We have like, you know, the uniform colour for the handles. So we were still able to add identify the shape of the handle also. So it always encles a path, you, which collects all the points enossing that area. So it was similar to theation wide with the restriction that theges detective must format cross to path. And so it defines boundary, so we can say that it can be used to define the boundary of each object. So this is useful for shape, analysis and object detection and recomition. So these are the areas where we use corn. So I mentioned last time, right, like my sementation is useful is that the output of segmentation is binary image, and I mentioned again, I think that the this binary image is a key input to many of the image processing algorithm, right? So one of the ex photoss. K always need finput. So we need to have like segmentation done before, like, you know, performing the photo analysis. So the output of segmentation, which is a binary image, is used as a seput to condo detection for free processing. Yeah, that's and we have the functioning over foringers. So this is a function we do not find hers. This is the off C built-in function for finding corners in any age. Basically, this function retires up to output. The fast one is cornors. Cors, like the list of cornors, Cers know, corners are represented as like points, like X, Y points, and each location. So it represents a ventor of boundary verumates, like anything in space which have like a value and magnitude, right? So the vector of powerbering points may like. Something like this, X. Why X, but since the coordinates of this, you know, like for example, if we have a corner of like circle for each co, we will have X15 or X2Y2 that values, right? So it represents a vector. Yeah, so that's what corners me, like Hondor, will be having a list of corners and the image, which is basically vent. Okay. Yeah. So, I'm through Mana Herati. Her means, like, if in this image, you can see in the image of the phone, you have like a child, right? The s represents a child of that. So this is the parent and this is the child photo. Okay, so it is within this image, right? So if you like use this function, it can even return the hierarchy like, it can even give you the child po and also the parent po, etc. So that is why the hierarchy and convers. It's a list of convers in the image is what the first output of that the value that function written. And herearchy is the optional output vector. I'm information about to topology, basically the parent and child relationship. So in this image, you can see that the like when you perform the chordal analysis of the input image, you can see we have like chors around the shape of the phone and also the camera and the legical sensor for the flashlight. Then we have on the pencil and also around the side of the right. So it enclosses the path, it shows the shape of the project. Okay, so once you identified all the condo coins, then you can draw them, actually, like around the shape. You can draw them, right? So how you can draw them is this function draws cral out planes in the teenage, like you can you when you forward this draw codral function, you can pass a thickness parameter. So if you specify that's thickness is greater that or equal to zero, but it does is like it draws it outline around your equ image. Otherwise, if the the thickness is like a minus one or anything less than zero, it just feels the input image with whatever colour you must supply. So that's the true whether and how you can draw the th. So in this example, like you can say we have the input image and then we do convert a different grace tain and then we apply a b of like thresholding and you get a binary image, right? Like zero and And this should be the input to the condo, right? So when we do the condor, we see that like, you know, it clearly identify the shape and the highlighted around the tree and the roots. So it then crosses the path of the shape. So this is how the h So next, please introaction will meet a detection. So, it is the process of addifying and locating important features from important or significant structures of patterns within the image. So if you look at this two image, so can you say that this is the image of the same building? Do you think this is the image of the same building or different building? Same building, right? As a human being, how did you conclude that? Like, by what made you to think that it's the same? Yeah. So you looked at the car, maybe, and shaved, like, two, like topers and maybe the archers. You see, like an additional of like pri, which was not presently the first thing, but still you see you can say that this looks same, right? And also, do you think the size is the same from the image there is a difference, right? Because closer. Yeah, one image was like, you know, the camera was more closer, so we feel like the object is much bigger. And the other one, you know, like the camera viewoint is much, like, you know, farther, so you can feel that it's more taller, but it's more, right. So that's the main kind ofciple. Even if it appears in a different scale or different rotation or different lighting condition or different, like, you know, appears. As a human being, we are able to recognize it by looking at the, maybe the patches, right? So you looked at the shape, colour, and, you know, the edges or stuff like that. But the asemation, like, it's very hard for the machines to add identify this right. So how we train them is by using some of, you know, feature inventional wororithm that I could identify some important features, like, you know, the top portion you can see the corners and even the ar shape and, you know, there are some structures which are, you know, which are called highlighting in the image. So they identify those features and they're trying to see, like if the same feature is there as well in the second image. If it is exactly matching, the machine is there. Yeah, this image is matching. So for that, as I mentioned, as a human being maybe look at the colour of the shape or like that, right, how identifies by features. So we call this interesting points, features. Okay, so it's is the process of adifying and locating significant structures or patterns, which are core features, within the image. So these features are crucial for understanding and interpreting usual informationing tasks such as opt recognition, of course, in order to recognize an object we need to, identify the features from the image and it's also used to call motion track, for example for motion tracking used sport andalytics. For example, if you are playing ming for board, if there are like scenarios like, there are countries together in forces hand touched in the war. So they do like, you know, the tracking, right? With the board, they do the score motion and for making decision. So for this time, they are ching just the ball, right? So we need to make sure that the board is being ching every frame of that video, right? So for this, we identify the feature of that ball, maybe the shave or some analytic feature, and then we like, you know, track it. So that's one of the applications where we use feature detection, and then a feature, like I mentioned, this is the definition of feature is nothing, but it's an interesting part of the image. Example, edges, edges an interesting part. So if you can identify the same ging anotheraging, and like you say that, so this images are matching. So it's just one of the example feature. So as I mentioned earlier, anything like if there is a sharp change in the intensity. So from zero to 20 is a sharp change, right? So it clearly ceases if you draw a light here, it's an edge because it's the white background and then it becomes black here. So it represents an edge, right? So that's what edge means. Like a sudden change in the intensity and corners, corners of course, means the intersection of the images. So toingles, we get the corners and then bloss. Bloss is like a similar texture. Sometimes that patch or something who will appear in the image of the same intensity. We call them blosss and bridges is like, you know, sometimes in it looks like edge, but it's not connected to any other edges, it's just like some kind of like, you know, blinds of highindens, some lines. So these all are examples of features from the image which help us to, other processing., Historic California next. Fe Deteiction has more significantly of a 60 beginning of, because since the beginning ofation, we use pizza detection, but we were giving more focus on H detection. Okay, so A diction, like candy, you have seenerily predict just, right? So the focus was mainly on simple detection at that time, but now, as we know grow in you know, like technology, we have like several complex algorithms and they're also like, you know, some solutions from declaring, which help us to have more efficient, you know, which provide more efficient and acid m the traditional methods. So the application of speech detection can be from like, you know, different fields or homos vehicle, like healthcare seral for medical evaging, for diagnosing dases, and even for like end entertainment for reality for enhancing the real world in your men with the digital olies. So auton normal speakers, we use it for navig and obstacleetectionality and pestrian forcing, etc. So these are all like, you know, examples where many feature collection. So, the basic are behind all the features detection is called it is this one. The image gradient. So gradient means like it gives more importance to the direction in which the intensity of of the image changes. Graded measure the directional changes in the intensity, so for, example, like, you know, if this is the image and this is the sp position. And we say and get more by these that lead value at this particular point, so that will be the magnitude. The city value at this particular point X value. he can, you know, the ipity of the image can vary from left to right or from top to bottom, about the versus the centre or towards the outer words. depending about that, we will have a orientation. So we have a, you know, for each fixer there nation, right direction. So there is a mathemating equ for commuting this gladi. So each pixel will have, you know, in magnitude and their direction, or orientation. So that's the main concept of each detection. So if we are able to understand the orientation of like, you know, the image, then even if the image appear in different rotation or different scaling, we can easily identify if the system can easily identify the feature from any like any variation of the same image. So like in the previous image as a human being, we' able to identify, even if this scale was different, the camera viewpoint was different, the size appear to be like different, right. But we just laboured to identify. So this is how computer is identifying. The main idea is the trick for each child going to is the grient. So if the algorithm can't find the ingredient for each pixel, it can each like, you know, it can identify the appearance of the same feature in another, image. So this is how a histogram of orangearian would look like. So if you closely looking for each image, like around the around that person, like you clearly detected the shape and also, it has a like, you know, gradient which is drawn, which is like a tour different direction. So gradient is nothing in question, the directional directional changes in the intensity or colour of the image, are fundamental in identifying the features. So this is the basic answer, we find our feature djection. So this is the equation that we use for computing image redient. So as I mentioned if this measure of changing image function, so for this particular pixel we have an intensity and also a gradient towards which direction the colour is changing on the intensity is changing. So the change in this image, you can see towards the centre is more dark, right, black colour, and as it goes outwards, it gets more brighter. And you can see towards the centerary, we can see the arrow towards the centre, which means that the ncity is changing, decreasing, or like changing, towards the centre. So this is an example of like how the gradient would look like, you know? So the image density is converting towards the centre, like it is getting more darker, so we can our the orientation towards the centre. And is the right hand again, you can see, it's more darker towards the, left side and the right side is more bright and so we represent the direction from right to left. So the change in colour repres magnitude and the blue arrow represent the direction of this image. So this is the formula to find image gradiient. So grad, as you mentioned, it is the change in intensity of the colour, or like change in intensity of each fixer. So this is the question for a computing the you, the gradient this offers an angle, which is data and which is of that immersse of, you know, change in why direction why changing it. So this is why is written in terms of matatingation here. So Italian of Delhi by Del divided by. So F represents the change in image function towards a an change of image function in wirection and left by dlects represents change of image function in X direction. So this is how the angle is being calculated and the magnitude and the particle fixel is completed by using, by taking the square root of the change in its direction, square plus change in by direction. So this is the questions. So you don't have to worry about this equ at all. The algorithm is already taking care of it, but this is the kind of behind, you know, feature detection. So once it can like compute, the excuse me, the magnitude and direction for HPixel hand them, it will do a lot of like, you know, sorting and, you, some kind of like clean up and get the good key points. key points is a bird that we will use because key, key points means unique, unique points from image. So, that key points have led, you know, particular orientation, which which can be used for identifying the same image or the same object in different images. So this is an impression for computing in each see a lot of safely faces, maybe getting five minutes of break and then we get.
shows, right? So in the right side also the same, but it's a little bit related and you know, it's not the same size more and also a top of the Bader Adboo's place, so it's not clearly miserable. So this is called occlusion. So there is a hinderance, right? Or Oloaded. And our task for the commuter is to find a same book to find if there is the same book is present in the second image. And if there is an image, drop like, you know, blanks to show that there are this particular piece person. Emma, these boys. So that's for this, like, we can use different algs the H miss it. So this is because it' very scale or rotation. So this kind of scenario. So what it does is life, you know, it identifies and describes local features in imagesages, and it's invaried to scaling, rotation, and partially invariing to change in illumination. So it is not completely like, you know, efficient to deal with change in illumination, but it can deal just from extent. Ieated corners, circles, glos, etc. So we turn the features, like, which we need for the processing, right. And the next is key points. key points is a term, like which is very. Use? Because in your alarm, you will be using in, right? identify the keyp fromages. So the key points are special points in image that carry unique information. So key is like basically a feature, but it carry unique information. So each keypoint have, you know, like data called thiscriptor. So each descriptor for each keybo is humique. It is not adical. So from the image, we be calling unique feature for processing. So that is what key point is. These are special points in an image that carry unique information. And the sift, I learn them is powerful whether carbermiss for detecting and describing local features in images. And there are different steps. The first one is scale space, extrema detection. So don't get conf with these words. It's just giving you an idea of what is doing. There are several stuff. The main idea of any feature detector is like first step, what they will do is. It's just detects the key points. And second step, with the compute. terrace. How may be, maning. This is social, but. much features between. Okay, so these s three stages of many feature intetection algorithms. So it to also do the same. The first step is detect key points. So how do detect key points in first step is, Kale space extre injection. So what it does is if we have an input image, it may pretty care of this image in different, for example, and each of these replication o. So that is the term o. Optaves means, you know, down sample decisive or inputinates. So there can be like a number of there is four, and oail one may be half of the size of age and oct two can be 194 of the original size and octave 3 can be one by eighth of size 54 leng. make different scale of our input. Okay, so we make different scale of our inputage and we we apply equos flur on each of these objectaves. Okay, and blur. Why we need to understand like it' smollenss the image in each level, and even if the image appears and people are like blurring or you know, a different scale, it can still be at identify, right? So we first of what we do in scales we extremeation is we downsample our inputting image into different octaves. And each objective will be, you know, different in the scale of our input use. And in each of these objectives we apply for per and we take a difference of these Vian learning painters. So that is called a difference of OG. That's what Iression here, difference of. For example, difference of pin familyut like Sophie have like first second of them minus first. person this will be, what of DOG will be G3 minus2. like that. Okay. So we take a difference in the bing blood images like each Octius and we get a sequence of you know, difference of Goian. Then what we do is being fine. Our key boys from these different social images. Okay, so and we say that like we try to see what is the maximum point or minimum point from this kind of difference of proced image. and we consider those points a sub potential key points. So we don't say that they are key points, but we consider them as a candidate for a potential key points. And in the second step in the second step they identify the actual keypoint. So in the fastest part, they do is they divide into do tails and then they apply go and blur and then they take a difference of GC and between those images. And from that, difference ofians, they compute maxima and min of like 11 values and they categorize a maximum and minimum values into potential keybo and then comes the second step where we find the actual key points. So here we eliminated like we have a by end of the step one week a list of, you know, key points. and we don't want all of them. We can eliminate some of them by eliminating the law contrast points. So if there is not much difference, between Ad and B, then we can like, you know, we can just ignore them and removing points that lie along edges. So I mentioned like, you know, we need only unique poems, for edges what happens is like, they have same kind of indels city or like, you know, something is common between four points in the edge, right? Okay, so we just need maybe one point from one edge. We don't want to replicate the same information from the edge. So we can eliminate images or points that lie along the edges. Do they have like, you know, similar information? So we just want unique information. So all these things are taken into consideration and then they have a final list of key points. Okay, this imploses stability and accuracy, and finally, as I mentioned, for those points we compute, the gradient. So I showed the question for commuteing the gradients, right? So that algorithm will compute the gradient, like the orientation and magnitude of each poins. And then this depth is critical, because it like, you know, it ensures the attention invariance. So if you find the orientation of that, point, then even if it appears in different, you, orientation or, different scain or something is the alvert to par. And the false step is is basically this one. So the dicted key points take, the faster three stepping zip, and the fourth one is commuting the des scor part. Okay, so here, like, finally around each, keep going, we take a region and then we divide it into some blocks, let say 16 by 16 block and then we commute the histogram of each and age. Okay, each block. And finally, what we give us an output will be a one dimensional feature vector, so it can be let you know a lot of smile these two 55 to points for vector. So it will be a 128 dimensional. It's a huge 120 has like a lot of key points in that. And that is the fourth and next step is the feature matching, which is the like, you know, this one match the features. So it can use different techniques. One of the techniquees is Euclan distance, so itute the distance between poes inside the, you know, like it from the list. And which distance is minimum, we take those points, the points, which have smallest distance between them, we take them, because that is closer matching, right? If they have frater distance, then we know. Okay, so this is how feature matching is done. This is the old idea of this human. Weect the key point, become given the descripture and finally we match the feature. So this we are the basic idea behind it. And for Sith, what we do is we divide the image in objectives, we apply OC and blur, and then we compute the difference of CCN and we like select some key points and in the second step we eliminate, block contrast value, all points from there, so we get them, you know, finite a set of key poems and then we have a like for that keybo, we generate descriptors by using, you know, by considering some area around each keyway and drawing and protein the histogram of them. and then the final output will look like a one day dimensional vector. So from this, like, you know, the last of his motion, we wanted to do a matchy, it's we can do it. In this case, like in this se example, we don't do a matching, because we just have one image, we just lost hold the important features. Like, you know, hold the you know, the corners or top of the corner of the ear portion and eyes towards the mouth and the chin. Yeah, so they're around the like her and all the important features are. identified. The list to one is the surf surface of the faster alternative tube sieve, and they offer like robustness to change the six air rotation and illumination. But Sift gives, like, you know, partial efficient for, you know, dealing with illumination, but surface more much better in that. And this is more faster than also. Why it is faster is because it uses integate image, integrate image means an image which have fixes in it, which each pixel in that image even have will be a sum of the pixel versus the left hand top, I can show you to measure I understand because it's hard to explain words. It's computer.. This is gonna matter...' gonna have to Ar already there. But it's it's fine faster for you. Okay. Soeg image, like we have, like, in the city like this. So what integral image makes for each pixel, let's take this for each pixel, the pixel maybe some of the pixel its left or top for the faster pixel there is 17 towards its left and top, right? So we use one. But for the second pixel we have a pixel, one here, right? left to the left is one, right? So we are the 2 plus 1, 3 and on the top, no, so we have left 2 plus one. Three here. For the next two one, how we go the second, each does it have anything on the top? No, but it have two fixes for the left, right? So it will add them. Okay, six. Similarly, if you go down here, it doesn't have anything towards the left, but you have by itself, on the top. So we will add that, so we we have some. And if you come here, it has on top and left, so it will be five plus to 7 plus 11. And this one also could take. So one, two, yeah. So everything's really counted, so it will be Yeah. And for this one, it will be some of everything. Yeah. So the So, each of some, the fixers in its left and. So this is what illegal game, so that's how they image, like the image taken as input for the safe algorithm. So that advantages that because of this in because we are using the integal image for conolutions, it uses pure features, but paintings, you know, key and proacy, and it's more suitable for real time application. For example, it's like for certain features, like, it can even redect the, a portion from the like, you know, the flash litter from the T shirt of Sheldon. So it is clearly, you know, matched. That's an example just showing how some were. did lesser key points than s, but it will be very hard. So here also the main place, the busterp, feature detection, second steppase, descriptor,ace feature. The past two is, you know, key point detection, right? So for that, here is let me use metric space detector, which is nothing but like, you know, a secondary value of the holm. find key points. We don't want to go much dep do that, but this is how these are the technology that like, you know, the algorithm that is being used for identifying the keyphones faster than it due to use of integral imagesages and f filters and so this is how we find, you know. the keyboids. And again. like sits, we have like different scale, like as I mentioned earlier. Iferent size, like half of the size, one by 4 is one way, so we repres in different scale. And the third one is also similar to SiI. We have the oriition, aignment, but instead of like, you know, using the declation, we use hardware hard wavelet is a methor that help us to identify the changes in intensity. So this is the, like hard beverage is the method that we used for assigning the orientation. And finally, like after identifying the key points, we assign the dominant orient dish. So from the Harbant response, we get like, what are the dominant like orientation in that Keep going which we detected. So from that we take the dominant orientation, which again help us to identify, even if the image appears in different orientation in different images. And finally, the second step of the main detention of the computation of disp, right? So here, the fourth step does that descriptive generation. So there are also we have taken a smaller breing around the key point and we brought at the historia and finally we got 128iversional feature wwide. Here, instead of 16 by 16 block, we take 4x4 region and we do, you know, like 4 H up region, we apply the harbave response and XandY direction and finally, what we get, instead of there, we use a histogram, here, we use Harway met. And here, the output is 64 dimally instead of 128. So it's just half of the size of sft, so it's more, you know, like faster than sim. Yeah, it can fewer key points, but as... I tried to do this inside. feel slow, then use.. It's. No, it's advance to be detention technique. AllR is one of the advanced det technique. Where? Which is the LA is oriented fast and rotated. is. combination of different. So it uses an algorith called fast and brave so fast is used for you know identifying the key points. And braef is very, you know, you know, like a good algorithm for descriptors, like computing the descriptors. So I mentioned write three mainstep, first one is finding the key points second one is computing the descriptive. third one is feature Mi So the fast is very good keyp intector and brief is a good despter. So PRP uses the combination of what. Okay, so it uses a combination of both and the pastense for a features from axelriter segment, bring these binary robust independent elementary features. So these two are different algorithms, but we combine the best part of both algorithm in ORic. ORB takes advantage of fast, coroner protection, detending to locate keypoint. So in our like six and how we wereating the keyp by use computing the you know, the gradient, right? In one, like algorithm, we used to different of pients for computing the gradient and in the surf, we used, you know, like hard like, right? Yeah. So instead of using the gradient, this algorithm uses commononalchitecture, so it protects only the corn. It doesn'tart all the pixel and the orientation. Okay, so in this image, you can see detected very less features, but all prominent features, especially you see the corner towards the ears and, you know, the corner, which intersects the ears to the head. So all the like key points detected here are just the corner points, right? They don't detect a lot of points like surf or s, but still they're detected like good enough features from that. It focusses on intensity changes, making it raw and fast, it can to generate binary descriptor. So in previous example, we hand mentionional and 64 dimensional ve right. So here it's sort of that we have binary descripted. So which have like descriptors just zero, 1, zero stars, zero.. It doesn't have anything else, just bin values. So it is easy for like, you know, if you' doing a feature matching, it is easy for the algorithm to see if because the battery is more understandable, right, for the computer is so it's more easy for the system to, you know, do the amation. Okay. So is it is very efficient, when we see the matching? So for matching, it uses, like, ince as that we use the Euclan distress, for computing the smallest distance between the features and if the distance is small, we say that the features are matching. If the distance is high, we say that they are not matching, right? Similarly, we use different techniques for feature matching for h distance. That will show you in the next flight. So Opportunity have a driting function for you, the ORB, which is like, you know, this advanced to feature detection take for an ORB and we have a function called ORB create for creatity, ORB detector. So you have L 3 will be for you for using this opportity function. And we have featured des scriptures. So featured descriptor provide, you know, as I mentioned earlier, each descriptor is unique. It has unique information for each keyoid. It's not like totally information, but you have it. unique and robust representation of detected features. So this is a very important sort of features matching because it has all the unique information from one object that we have unique information from the other image. So it's easy to compare to make a decision. And one of the example for feature descriptor is histogram of four gradients. So this is the one which we use in SiIF. Si we use histograph, right? histogram of gradients. So this is the example. This image shows absolutely absolute value of gradient and absolute value of vigr. And if you take absolute value ofr, you get, you know, vertical edges or vertical features, you know, highlighted. And if you take the vr, you have all the horizodal features highlighted. And it's critical, descriptive representation for identifying human shape. So this is one of the techniques we use in autonomous vehicle to identify pedestroans. If pedestrates are pro, it can easily detect the human you know, shape. So this is one of the techniques, is a usefully not vehicle So that's why it's called it's particularly FD for human detection in computer vis. And it pots the image of pixel orientation and gradiates on a histoa simplifies the representation of image. So for example, is in this image. It works by analyzing radients and energy directions in localized proportion of an image, creating aique representation of human shape and m. So this hology is specifically used for flaunting the human shape. So that's why I mentioned it's used for, you know, identifying pedestrians because of the world. Yeah. If you can read through the slide, I don't want to let a brain mate, but the main idea is like each of the scriter is something which is unique and it have like unique information about the keyboards. Example this figure, HOG is one of the where we can represent our. Yeah, so feature, a descriptor as we say, the descriptor have unique information and if you look at this this is how we can calculate the grading manititude and angle from a pixel area. So if you look at that pixel from the top of the car, so it is highlighted here, so let's say that for that area, we have the pixel value, likeength this 171, 2050 We have no taking the corner value in Silver Valley. We just showing the change in the city various in direction and y direction. And the grading value takes direction, we can have you from like 120 minus 70. So we are taking the X2 minus X mile. So like the you know, the one towards the right minus the one from that left. So 120 minus 70, which is 50 and in wide direction be minus from the top one from the bottom one, right? Y to minus vibrant, which is again, 50 So putting it together, we will have a feature vector, like 50, 50s. So it shows the changing X direction and changing wide direction. So that's what I explained like earlier, when we compute the like the gradient, we have tann inverse of changing extern, yire direction divided by changing, X direction, right? So that's how this is being completed. The magnitude is computed, like, yeah, tanning verse of 50 by 50, which is 45 degree and also the grading magnitude is 17. So for that particular area, the grading magnitude, the value of the intensity of that particular area will be 70.1 and the angle will be 45 degrees. So we got an orientation for that particular area, right? Similarly, we calculate for each subregions and then we have, you know, histogram, which is short, the orientation of each pix says, and we can we can define what is orientation for each area and yeah, so that helps us too to, you know, the feature detection and even if it dumpears in different scale or different oriotation and So, the last step of featured detection is feature matching. This is optional, but most of the application we might use it, for example, like panoric made is teaching. So if you want to stitch two images, we need to find exactly matching object in what the image and we need to you know, stage stuff to design and show you an example of that. So these two images, which are not identical, but they have a common points, right? This particular stuff is brusted here as well, and when we like do a feature actually we can identify and finally we can switch it together like the sec secret image. So it was previously two different images and we teach it by identifying the similar points and you can combine them. So this is an example how teaching is that. So that's where we use feature. Sp matching it involves identifying similar features, like if just corners, blos, like we have the same textures, in different image, and this is the key for taskware the correspondence between features in multiple images, crotial examples in. We need a correspondence between two image, right? There should be a common, like po or common part of the image where we can st it together. for feature matching, we use different, like, you, distance computation, so one of them is you ple distance, one or another one is hamming distance. So hamming distance is used in OD because I mentioned, right, it has O binary. Okay, so if we have like Canada image next same, let's say let's say Canada, So this is a descriptive for image, and we can say descriptive, and this is the descriptive to. And if we want to say like, you know, if they are having like how much distance between Nima and we do, what we do is we look for each position in more than descriptive, and if it is smarching. If we can we not matching. We commute that this instance. For example, one body is matching. So you know, taking and it's distance and there will a difference under the mark. Then there is also a difference and aotional total, the distance for the harmming distance for this to this So how many pixels were not matching if the distance? Okay, so we got to spoil. So we have like two binary descriptures, how having distance is commended. we comeare each of the, you know, the the location at the next value. and compare it with the same index in the other descriptor. I if it is not matching, we say that, yeah, they' mismatched, so there's a distance of one. And for how many mismatches there that match is there in house in those two? So this is how having distance is computed. And this is used in ORD because we have either it is put in. Then another money is like R four and root for K and then flan. So these are different types of like, you know, a way to compute the distance between the distance for your lab, you can either use folds for a flag. don't need to let do both, but you can either choose between food force or plan. Okay, yeah. So, Brood Force Matcher is, like as its name indicates it's a brute force match, because it look for each epixel or each descriptor in value in one descriptor will look for each value here. So it will look for a match in all the picture here. For every pixel is taken and you will look for a match in all the other. So it is more, you know, like time consuming and it's not the best method if you have like a lab it doesn it. Yeah, so if you have a lab to doesn't, like plan is quite useful because it's much faster to compute the distance. Applicationsations involve anor is reaching portion tracking, same engineer sport andalytics is an ex example for motion tracking, and then opt recognition and creat model wings. So in all of these applications, we need feature matching. This is an optional step in feature reception plat for this application, we need feature matching. Okay. she learning feature detection of, as I mentioned, the feature detection is there ever since the beginning of capital and since the advancement in the field of you know, AI and B learning, we are also advancing even in the algorithms for future renection. So we have like a different techniques, like supervised learning and supervised learning and service supervised learning, right? There are like, you know, examples. But I know you guys already know about like supervised. What are the differences? I'm just like highlighting it here. It is still applicable for even machine machine and even the machine ling in feature detection. So here also we use these techniques, like we use label data for training, like emails, sparm detection. We give the labels to identify the spam, right? So we train them with the lababeled data. But for I superpervised still learning, we don't give them any, like, you know, labour circ, but they learned from the patterns. example, customer segmentation based on purchase and behaviour at UR like, you know, if you are a customer who buys only branded product or you are a customer which doesn't care about the brands and stuff like that. So they can like categorize you based on your history of purchasing, right? So based on the pattern, they can just don't pred your behaviour. And service supervised learning is combined both approach, so we trained them with labed and once it brings sudden maturity, then there is no need for riding label because. For example, Googlely, you can name them like, you know, have two identify the person's face. And after like point of time, you can easily identify the this is that person's photo from your gallery. So this are the examples of different techniques that is looking for each And real time teacher edition. So this is one of the challenging tasks in feature detection. It's very hard for achieving this because, you know, because of many race and calculating power and you know, the evfficient algorithm, all these needed to be there for achieving this, but this is really needed, because for a family, nonomous vehicle, we cannot rely on a system, which is very small, right? It has to make a decision very quickly. There is the pedestant, it should apply, bring, like all of a sudden you cannot like wait. So the real time feature detection or real time processing is very potially in machine machine. So it still faces significant challenge, particularly in balanceancing computational design with the need for agroce. So if we need like the current scenario is if you want to have a highly accate system, the computational load will be very high. So we are looking for something lightweight, but which week, but it should have like more approcy and efficiency. In application, like Villance and not normal driving, where decision must be madely and accurately, these challenges are amplified. because it affects human life, right? For the outcome is. So not risk life while relaying on some slow system, it has to be fast and relable. And common remedces algorithmivation we have to like, you know, if you end up in a position of like, you know, if you psych area to be scientist or machine vision scientist you, you have to think about like optimizing about the right thing you algorithm that you optimize you know this complexity. and another remedy is use of law-level programming. Surely pyon is example. If you maint need to use like our ev systems, usually you CC++ or like, you know, a level languages. So that should be the one because otherwise the overhead of like interpretting and for machine to understand it, it takes time, right? So use of law program language is an anotherdy, which is hard for human beings to like, you know, right, but still that this one not the remedy and utilizes hardware acceleration, like GPU, GPO, more computing powers. So these are all the reries for loss for the challenges and we have for real time future protection. and for the future trend, of course, the deep learning,ural network, so these are the nicknames in AI, and it's still help us even with feature detection. And as you know, the neural network help us to learn patterns automatically, right? So it can be used even in like faster feature detection. So slowly we are departing from the traditional mets of handly written algthm and we are playing a lot on, you, of the you know, like the models deep learninging algorithms, which help us to deal with large data, c. large data set even if the data't not manipulated images, the AI algorithm can manipulate it by doing image processing and, you know, like you can even help you to broaden your data set by using some technology that you will not in the outcoming classes, how to like, you know, broaden your data set and whatever the key things you have to keep in mind by selecting data set, etc. So future trying involves, of course, the learning and neural interor and we need to think of enabing spider feature intern system that can improve over time cleaninging from new data and experience. So it's improving over the time. We are slowly moving away from the traditional algorithm, which was taking a lot of time and computational power. Now we have deep learning techniques like CNL's, which help us to do the job much better and faster. So next week we will be learning about the horn of our tradition which is skin. So we will have our introduction to come. to our architecture and how CNN Camus sold a lot of capitalition. So yeah. That's it for today. Thank you for coming. Any ds or boring. Yeah, okay, let me see you next week. I think I prom next week will be, okay, this week is more like, you know, maths and going, yeah. I know..