Today's topic covers artificial neural networks and their disadvantages for image classification. Then we will introduce CNN, which comes from artificial neural network, and its architecture. We will also discuss its layers in depth. Then we will look at the applications of CNN and some of the performance evaluation metrics. So this is today's topic.

Let's start with artificial neural networks. This is taking a computer system which is inspired by the functionality and processing of the human brain. So it's similar to how the human brain works. Like our brain has neurons, it also has a large number of nodes which are called neurons. We have a lot of neurons in our brain. If we see an object, the optical signal gets transmitted to our eyes and it reaches the brain by transmitting through the neurons. Then it does some processing and we know that we are seeing an object, like maybe I'm watching this projector screen. So it tells my brain that I'm seeing this. It all happens in a fraction of a second. Our brain is that much powerful. Similarly, we are trying to simulate the same processing in our computer world by making use of artificial neural networks. The specialty of artificial neural networks is that they can learn by analyzing large data sets. When we were a child, we were learning "this is a chair," so we know that this is a chair because we learned by seeing different shapes of chairs, and even if we see a different shape of a chair, we can identify it. So our brain has that capacity to learn from the patterns that we saw over the period of our time. Similarly, we can train our neural networks to learn from the data set. If we have larger data, we can more effectively train them to analyze images and even for recognizing objects from images or performing classification, et cetera.

The reason why we need artificial neural networks is that they are particularly effective in recognizing complex patterns. Even if we have the same object present in an image at a different scale or a different rotation, we are able to recognize that. Similarly, neural networks can also be trained for analyzing complex patterns and they can help us in processing images and making decisions. These are the reasons why we can make use of artificial neural networks for image classification. So artificial neural network is a system similar to the human brain.

Now let's talk about how predictive models use traditional methods like decision trees. This is an example of the decision tree algorithm. I don't want to go into that because this is not part of our course, but I just wanted to show you why we cannot use this in image processing or in machine vision. How predictive models use traditional methods is, on the left hand side there is a table which categorizes images based on different features like ear shape, face shape, the presence or absence of whiskers, et cetera. For example, if it identifies an animal which has a pointy ear shape, then it looks for its face shape, and if it is round, then it looks for whether they have whiskers or not. If it is there, then they categorize that animal into a cat. So this is how we make decisions based on the features that are available in the left table. Other animals with a different combination of these features are classified as not a cat. For example, if the ear shape is floppy, then we will look for the face shape and it says round. Then again, look for the other feature, is whiskers present or not, it says it's absent in the third row. So then the system categorizes it as not a cat. This is how the decision was made using the decision tree.

On the right hand side, you can see a decision tree representation of the same process which I explained in the table. How it works is that it sequentially looks for each feature and then makes the decision. First it starts with ear shape. Then it looks if it is pointy or floppy. Let's say it is pointy, then it moves to the next feature, which is face shape. In the face shape, it looks for whether it is round or not, and if it is round, they categorize that animal as cat. If it is not round, they classify it as not a cat. Similarly, on the right hand side, you can see if the ear shape is floppy, again you look for the presence or absence of whiskers. If whiskers are present, it is a cat. Otherwise, it's not a cat. This is the process of the decision tree, how it classifies an object into cat and not a cat based on the features provided.

Don't you think we can make use of this for machine vision? You know that the input we give for machine vision systems is mostly images or videos. So we cannot make use of this method for our purpose. We need something different. That's why we say we need neural networks like artificial neural networks. This is our data. We don't have a table with features or a decision tree to say if it is a pointy ear or a round face. We don't have that kind of data. What we have is just some images of cats and dogs. This is our image data set. This is what we are providing to our network and the network should be able to learn from these images. When we deploy this into production, if we give a cat's image, the system or the network should say this is a cat's image. Otherwise, it should say this is not a cat, maybe it's a dog.

Here you can see a representation of the artificial neural network. We have given the data set as the input. We give both images of cats and images of dogs to train the network. We input that into the input layer. For artificial neural networks there are three main layers: input layer, hidden layer, and an output layer. We are giving our image to the input layer, where the input layer can identify the shape of the ear, shape of the face, et cetera. From that, the data is passed to the hidden layer, and the hidden layer does some processing, and finally it can make a decision or classify into two classes, maybe class dog and class cat. Finally you get two output classes, which are cat and dog. We have a large number of photos of cats and dogs. We pass them into the input layer. In the input layer, they can identify the shape of the ear or the shape of the eye or the presence of whiskers, et cetera. Then it passes to the hidden layer, and the hidden layer does the processing with the features that they got from the input layer. Finally, after processing, we get just two classes which represent cat and dog. This is the overall idea of ANN.

But there are still limitations for using artificial neural networks for image classification. Let's say we have an image of a cat that is 1000 by 1000 pixel size. This is a coloured image, not a grayscale. It's a colour image. So it has three channels: red, green, and blue. If we represent this in an artificial neural network, the first layer is the input layer. The input layer will have 1000 times 1000 times 3, which equals 3 million neurons in that input layer itself. Each pixel in that input image is mapped as a neuron to that input layer. Then it goes to the second layer, which is the hidden layer. Let's say that they use 1000 filters. This is where we do some of the processing. So if we apply 1000 filters, each neuron in this hidden layer is connected to every neuron in the previous layer. So totally there are 3 million times 1000, which equals 3 billion connections. Finally, the output layer has the required neurons. You can easily see that it's computationally very heavy. This is just for one image. We have many images in our data set, so it's very heavy computationally. We cannot rely on ANN. Another problem is the overfitting problem because we are giving a lot of input pixels. We train it too much, so we can overfit. The third one is longer training time because we need to train this huge network. So it takes a lot of time to train. These are the limitations of artificial neural networks for image classification.

What is the solution for this? We cannot use traditional methods because we don't have features or data which is enough for making a decision using decision trees. And we cannot use ANN because it uses each pixel in the input image as a neuron, so the network is computationally very heavy, takes a lot of time to train, and can have overfitting problems. So what is the solution?

This brings us to the convolutional neural network. It is basically similar to ANN but with more layers in the hidden layer, which helps us to focus only on some important data. We don't want all the pixels from that input image. Convolutional neural network is a deep learning technique which is designed specifically for analyzing, extracting, or processing images. As I mentioned, ANN can learn patterns from data. Similarly, CNN also does the same. It can automatically learn patterns from data. Another important aspect is that they can solve complex spatial tasks with deep learning. Even if it is a complex image, they can still handle it and then they can perform object recognition or classification, et cetera. The benefits are that it addresses all the limitations of artificial neural networks by dealing with high dimensional structured data like images, video, and audio. It can hierarchically extract features, and they are robust to translation of objects. All these benefits make it a better candidate for us to use in machine vision systems. So it's always better than ANN because it can handle high dimensional data, learn features hierarchically, and they are robust to translation of objects.

Let's talk about its architecture. This is very important. If you are asked in the exam to draw the architecture, this will be the answer. We have the input layer, hidden layer, and output layer, same as ANN. But there is a difference because the hidden layer may be replaced by different layers. We have the input layer, and instead of a single hidden layer, we can have a convolutional layer, pooling layer, and fully connected layer. The difference is that the hidden layer is replaced by a convolutional layer, pooling layer, and fully connected layer. We have the same structure but more layers. In this architecture, you can see we are giving the input, and the input image is given to the next layer which is the convolutional layer. You can see a lot of passes there. It means we are applying, say, five filters. When we apply five filters on this input, we get five feature maps or five outputs. That is what the convolutional layer is doing. Then we give that to a pooling layer. The pooling layer actually downsamples. If you look at that, the size of that box got reduced in the pooling layer compared to the green and the blue box. So the dimension got reduced. That is the benefit of the pooling layer. We give the output to the convolutional layer where we are extracting all the important features. Then we pass that feature to a pooling layer where we downsize or downsample. The benefit of doing so is that we don't have to deal with as much computational processing because we don't have to deal with a larger size. We are reducing the dimensionality. That is the importance of the pooling layer: you can downsample it or downsize. Then this output is given to the fully connected layer. This is where the magic is happening. This is where we are training the model, adjusting the connections and weights until we get the desired output. The output layer will show, for example, three purple dots which represent three classes, maybe class dog, class cat, or whatever. The neurons in the output layer represent the number of classes if it is a classification problem.

Also, if you notice, there is a section marked for extracting features. This is where the feature extraction and learning process is happening. And the fully connected layer is where the final classification is happening.

The CNN typically consists of an input layer and multiple hidden layers. It's not necessary that we have just one convolutional layer and one pooling layer. We can have multiple convolutional layers and multiple pooling layers depending upon how complex the data set is. If you have a very large data set, you can have multiple layers. We are using already implemented algorithms and most of them use multiple layers of convolution. You will see that when you implement in your lab and project.

This is the basic idea about CNN. We have multiple layers. The hidden layer includes a series of convolutional layers, pooling layers, and the fully connected layer, and each layer performs distinct operations. The convolutional layer applies the convolutional operation, and when we apply the convolutional operation, it extracts only the important features. The pooling layer performs the downsampling and reduces the size. The fully connected layer uses all the features to compute the class scores. Class scores means in the output layer you see, say, three nodes. Each represents a different class. For each class there are some class scores. Depending upon which class has the maximum score, that is the one the model is predicting.

This is what you will be asked to draw for your exam. The key components of CNN are these three layers: the convolutional layer, pooling layer, and the fully connected layer. The rest is similar to ANN with the input layer and output layer. But the hidden layer is replaced by convolutional, pooling, and fully connected layers. The convolutional layer extracts spatial features from the input image. The pooling layer reduces spatial dimensions and simplifies the computation. The fully connected layer integrates the features for final classification.

Now let's deep dive into the convolutional layer. In this video you can see there is a small kernel or filter which is looking for the shape of an eye. When we convolve that with the input image, we see the shape of the face. As that filter travels across the width and height of the image, it looks at each location to see if it is matching or not. The first two rows are not matching, then it comes and identifies two locations where the exact same shape is identified. In those locations, they assign some value or different weight to show that this information is needed. The rest of the pieces are not needed for further processing. This is the basic idea of convolution. We apply different types of filters. We will have, for example, a filter that can identify edges. It looks for that particular shape in the image and identifies it. Similarly, we will have any number of filters to be applied on an image so that we can extract all the important features, and then we will use this to train the model. Finally, it can give us class scores for each class.

As you can see, in these layers, small learnable filters slide over the input to extract features such as edges, textures, and shapes. When it passes through the images, it extracts shapes like it can be a circle, or even a shape like an eye, or just an edge. When we did the Canny edge detection, it just detected the edges. Similarly, it can extract features, and those features include edges, textures, and shapes. Each filter in a convolutional layer detects different features. We use any number of filters and all these filters can detect different features. No two filters are doing the same purpose. For example, if we are using an edge detection filter for horizontal edges, it just detects the horizontal edges, and we will use another filter for detecting vertical edges, and another one for just detecting the corners maybe. Each filter has its own purpose. We use any number of filters and all these filters will detect different features from the image. That's why we can say we can deal with complex structures because we are identifying those complex patterns by using different filters. The convolutional layer plays a crucial role in feature detection and representation. That's why we are able to use CNN to perform image classification or image recognition, because convolution is the basic principle behind this. It helps us to identify the features.

As I also mentioned, it has the ability to extract features hierarchically. First, it can detect low level features, then mid level features, and high level features. If you look at the image of the car, it can detect low level features like small lines or some patches or edges. When you go to the mid level features, it can detect some shapes like circles or even more clear patches. Then you come to the high level features, and it can detect the shape of a wheel or even more precise shapes. In reality, if you go deeper in the network, it can detect more complex patterns like the shape of a house, shape of a window, or shape of a car. We have filters for that. So even when you go deeper in the network, it will extract more precise features. That's why they say that CNN can learn complex patterns. That's an advantage: we can learn even complex patterns from the image. The basic principle of CNN is to automatically learn and extract hierarchical features from the input data. We have different hierarchies of features: low level, mid level, and high level.

What about the convolutional layer in detail? On the left side, you can see the image of a cat. It's a coloured image, not a grayscale. The middle image is the output of a basic edge detector applied on the cat image. You can see that the edges of the cat's face are detected there. On the right hand side, you can see some of the feature maps that we got as a result of applying different filters. A feature map is nothing but the output that we get in the convolutional layer. The output from these layers is called feature maps. These feature maps are given to the pooling layer, and the output is called pooled feature maps. The right hand side shows four different feature maps that we get as an output by applying four different filters. Let's say one is detecting horizontal edges, one detecting vertical edges, one detecting maybe the sharp points, et cetera. These are the output of four different filters. They are similar but there are some differences in detected features.

The convolutional layer helps the network focus only on the most important features. Do you remember what was the disadvantage of ANN? It would be hard to give all the pixels in the image into the input layer, and because of that it was computationally very heavy. The benefit of using the convolutional layer is that we don't have to use all the pixels in the cat's image. What we do is we apply the convolution. We just pick the important features only. Then the network uses just these features. It's not taking all the pixels from the input image. It's just taking whatever we need for processing, whatever we need for making the decision. That's the benefit. We are making the network more lightweight than ANN because it additionally has more layers which can help to downsample. Not all the pixel information in the image is relevant for training the model. We don't want all the pixels from the image, so we're just taking whatever we need. It improves the performance and accuracy by using this convolution operation. That's the importance of the convolutional layer.

This is how the convolution operation is performed. On the left hand side, you can see a matrix which is the input image, and we have a filter or kernel. These words can be used interchangeably; both are the same: kernel and filter. The filter is, let's say, a 3 by 3 filter. The input image is 6 by 6, and we get an output image which is 4 by 4. How we get this output is that we take the filter and scan it across the height and width of the input image. While we scan, we take a dot product and we take the sum of that dot product. This is how the convolution is performed. So this is how we got the first value in that output. After we calculate the first value, we shift one pixel to the right. We move one column to the right and we do the same operation. We take the sum of the dot products between the kernel and the pixel values. That's how we got the next value. Likewise, we pass across all the width and the height of the image and we get the final output image. This is the convolutional operation. You can expect on the exam to perform the convolution operation and get the output. The size got reduced from 6 by 6 to 4 by 4. We just pick what we need; we don't want all the pixels in the input image.

There are several factors that help us to make our model better in the convolutional layer. The first factor is filter size. Filter size determines the extent of the input data that each filter covers. If you select a small filter, it can cover a small area in the input image. If you select a big filter, let's say 11 by 11, it can cover that much area from the input image. It affects the granularity of the features. The benefit of using small filters is that they can detect very fine details from the image, like small edges or small corners. They can capture fine details. If you use a larger filter, it can detect broader patterns. It can even detect the shape of a home or the shape of a tree. Depending upon the filter we use, we will get different output feature maps. If you use a small filter, you can get fine details, but if you use a larger one, you will identify broader patterns.

The second factor is stride. Stride means the step size with which we are sliding across the input image. For example, we move one pixel to the right, which means stride is one. If we say stride equals two, we skip one column and move to the third one. That is what stride means: the step size with which the filter moves across the input image. If we use bigger strides, the advantage is that we won't go through the repetitive pixels again and again. For example, if we have an input image and a 3 by 3 filter, and if we choose a stride of two, it will skip columns and move ahead. So it is not visiting the same pixels again. Otherwise, it would be taking the same pixels again, which is a repetition. The overlap of adjacent filter positions can be avoided, and the size of the output feature map can be controlled by doing so. If you use bigger strides, the output will be very small because we are skipping many of the pixels. Larger strides produce smaller, more abstracted features. So we covered filter size: bigger filters cover larger patches, smaller filter sizes detect fine details. And if you use bigger stride, the output image will be very small. If you use a smaller stride, the output is much bigger.

The third factor is padding. Padding means you are adding a layer of zeros around the input. The advantage is, let's say that we have some important information at the corner pixel. If we are not using padding, when you do the convolution, we visit that corner pixel only once. We are not visiting it again and again. But if you have padding, what happens is the first time it detects those pixels, and in the second iteration it also includes that pixel. And again, if you come down, it can visit it maybe 2 or 4 times. That's the reason why they use padding: if we have images which have more important information in the corners, we can use padding to protect that information. It's related to the stride size as well. If you have a stride size bigger than one, then it's better to avoid big strides if you have important information in the corners. Padding is the addition of zeros around the input border, which allows control over the spatial dimensions. If you don't want to reduce the size and you need all the information from the input pixels, then you can apply padding so that you have control over the size. We are pretty much keeping the same size of the input, or it's not getting much smaller compared to if we are not using padding. Padding helps in preserving edge information and allowing deeper layers to extract increasingly complex and abstract features.

These are the main factors that have impact on the output from the convolutional layer. The output from the convolutional layer is called a feature map. And when you pass this feature map to the pooling layer, the output from the pooling layer is called a pooled feature map.

This is the equation for computing the output size. You can expect questions from this as well. The output size is given by the formula: (N minus F plus 2P) divided by S, plus 1. Let's say that we have an input image of size 5 by 5 and we are applying a filter of size 3 by 3. And let's say we are applying a padding of 1 and a stride of 2. You will be asked to compute the output size. In that case, you can use this equation: N minus F, where N is the size of the input which is 5, and F is the size of the filter which is 3. So N minus F is 2. Plus 2P, where P is 1, so 2 times 1 equals 2. Divided by S, which is 2. Plus 1. So we get (5 minus 3 plus 2) divided by 2 plus 1, which is 4 divided by 2 plus 1, which equals 3. When you are asked to compute the size of the output, you don't just say 3. You have to specify 3 by 3. That's how we compute the output size.

If we have zero padding, meaning no padding, and a stride of 1, the equation simplifies to N minus F plus 1.

Now let's talk about pooling layers. The output from the convolutional layer is given to the pooling layer. The pooling layer is useful for reducing the size, for downsampling or downsizing. The pooling layer is responsible for reducing the spatial size of the feature maps generated by the convolutional layer. There are two methods for performing pooling: one is max pooling and another is average pooling. Another use of the pooling layer is that by doing the downsampling, the system becomes more tolerant to variations and distortions. So even if there is a variation, we can handle it in the pooling layer. It enhances the ability to generalize. The computational cost is reduced due to this.

This is how the pooling layer works. The output from the convolutional layer, let's say it's an edge detector or some convolutional layer that produced a feature map, is given to the pooling layer. If you are using max pooling, you can see the size got reduced by half, say from a 4 by 4 to a 2 by 2. The size got reduced and the feature highlights become more prominent. It generalizes the features.

Let me show you how to calculate max pooling and average pooling. It is simple. Let's say we have the feature map and I'm using a 2 by 2 kernel. First, the 2 by 2 block is represented by the orange colour. In max pooling, under that block, we take the maximum value. Here we have 6, 2, 5, 5. We take the maximum value, which is 6. Then we move to the next block. Here we have 7, 5, 3, and some value. The maximum is 7. So we got 7 there. In the green block we have 8, and the blue block we have 5. This is how max pooling works. We get the maximum prominent feature from those blocks. For average pooling, we take the average of that block. We take the sum and divide it by four. For each block, we compute the average of the pixels in that block. This is average pooling.

Now the third layer is the fully connected layer. This is where all the magic is going to happen for training our model. We get the output from the model based on the computations that happen in this layer. This is one of the most important layers. We can see in the image there is a feature map. The output from the convolutional and pooling layers is called a feature map. That is what we are giving as input to the fully connected layer. If you remember the CNN architecture, there is a layer called flatten. What it does is take the output from the pooling layer, which will be a multidimensional array or matrix, and convert that into a one dimensional vector. That is called flattening. We are just flattening it from multidimensional to one dimensional. That is what the flatten layer does. The output from that will be a one dimensional vector, which will be the input to the fully connected layer.

The feature map is flattened and then given as input to the fully connected layer, which is represented in the yellow colour. In the fully connected layer, there are multiple layers. We train in the fully connected layer by adjusting the weights and other layer parameters, and finally we have an output layer which represents the number of classes. The number of neurons in the output layer represents the number of classes. If it is a binary classification, we will have two neurons. If there are, say, ten classes, we will have ten neurons in the output layer.

The fully connected layer is the layer where the high level reasoning, based on the features that we extracted from the convolutional layer and pooling layer, is applied to make the decision. We convert that high dimensional feature map into a probability distribution. If it is a binary classification, we will have class A and class B as output. Our model might predict, for example, class A has probability 0.8 and class B has 0.2. The model says the object we gave to the model belongs to class A because it has the higher score. We transfer the high dimensional feature map into a probability distribution. Depending upon which class has the higher value, we say that the object belongs to that particular class.

After the convolutional layer and pooling layer extract the features, we flatten them and present them to the fully connected layer. The fully connected layer combines all these features and then makes a prediction or classification based on that information. Each neuron in the fully connected layer is connected to each neuron in the previous layer. So we have full connections between the neurons in consecutive layers. The main idea is that if you have connections from all the neurons to all the nodes in the next layer, it can represent the entire representation of the input and we are not missing any of the important data when making decisions.

Now, about flattening. Flattening comes after the pooling layer. The feature map from the pooling layer is given to the flattening process. What happens in the flattening is that it takes a multidimensional object, array, or matrix and converts it into a single one dimensional vector. For example, if we have a 3 by 3 feature map, first we take the first row, say 1, 1, 0. Then we take the second row, 4, 2, 1. Then 0, 2, 1. So we get one long vector: 1, 1, 0, 4, 2, 1, 0, 2, 1. This is how we flatten our feature map into one dimensional. This is the input to our fully connected layer. The convolutional and pooling layers produce feature maps. These feature maps are multidimensional, and flattening converts these feature maps to one dimensional. It concatenates all elements along the depth. This enables feeding into the fully connected layer directly.

Another important factor in the convolutional neural network is weight matrices and biases. This is the way for all deep learning techniques. Weight matrices and bias vectors are the key players in the system. They are the ones which we are actually training. These values are not constant. We start with some random value, then we get an output, and we say this is not the output we are expecting. So we need to train them again. We go back and update the values and get another output. We see if this is better than the previous one. We continue trying until we get the best or most optimal results.

Weight matrices and bias vectors are the learnable parameters in the network. Over the period of time, they can learn from data and they can change their values. These are the foundation for deep learning algorithms, and in CNN we use weight matrices and bias vectors. Let's say the X values represent the flattening vector, and this is our current layer, the first layer of the fully connected layer. We have A1, A2, A3 as the three neurons in the layer. Each neuron is connected to each of the input values in the previous layer. There are connections between all the neurons; none of the neurons is missing. They are interconnected.

The letters in colour, W1, W2, W3, W4, et cetera, represent the weight matrix, and B values in colour represent the bias vector. How we determine the size of the weight matrix is: it is an N by M matrix. N is the number of neurons in the current layer. How many neurons do we have? One, two, three. So we have 3 rows. And M is the number of values in the flattening vector: X1, X2, X3, X4. So the weight matrix is 3 by 4. The length of the bias vector is equal to the number of neurons in the current layer. So we have A1, A2, A3, meaning the bias vector has 3 values.

This is how the matrix multiplication works. We have our weight matrix and we multiply it with our input from the flattening vector, and then we add the bias. The output will be: W1 times X1, plus W2 times X2, plus W3 times X3, plus W4 times X4, plus B for computing the first position. The resulting A1, A2, A3 finally make the three neurons.

The reason why we use weight and bias vectors is that we need some sort of nonlinearity. Why do we need nonlinearity? If we have a linear equation, for example f(x) = 2x + 1, this is just a linear equation. It would be a one to one connection or straight connection. If we give 1, it will be 3. If we give 2, it will be 5. This is a simple system. It cannot deal with complex patterns. For that, we need some sort of nonlinearity, like curves, sigmoid shapes, or bell shapes. All these represent some complexity. We don't want straight lines or straight output. We need some sort of nonlinearity in our system; then only can it handle more complex patterns. That's why we need nonlinearity. For this, we use another concept called activation functions.

Activation function is the function that determines if a neuron fires, or if a neuron is needed for the next level. For example, in the previous example, we had output A1, A2, A3. But sometimes we don't need to keep all of them for the next level. We just need maybe A1 and A3. How we make that decision is based on the activation function. It identifies if this neuron is qualified enough to go to the next level or if we need to keep it. And this can be applied at different layers. It is applied after the convolutional layer, after the fully connected layer, and even after the output layer. It can be applied at different layers. The most commonly used one is ReLU.

From the previous example, we have input X, the weight W, and we multiply and take the weighted sum, and we add a bias B. Then we apply an activation function. For the ReLU activation function, the formula is f(x) = max(0, x). If the input value is greater than zero, we take that value. If it is less than or equal to zero, we make it zero. So we take only the positive values and disregard the negative values. This is how we make a decision about whether a neuron can be fired or not.

Now let's talk about the output layer. This is the final layer where we generate the prediction or classification. The number of neurons in the last layer matches the number of classes. If it is a binary classification, how many neurons will it have? Two, because it matches the number of classes. If there are 10 classes, we would have 10 neurons in the output layer. It matches the number of classes. The activation function differs from the fully connected layer. For classification, we use something called softmax. We will learn about the details in upcoming classes. The highest probability neuron represents the prediction. We will have different classes and at the end what we get are some class scores, let's say 0.5, 0.3, and 0.2. We take the one with the highest class score. So the one with 0.5 is maybe bird. We have input pixels from the flattening matrix. The blue colour represents the flattening layer. We give that to the fully connected layer. We have multiple fully connected layers and we do all the training there by adjusting the weights and biases. Finally, we get the classes with class scores. The one with the highest class score is the prediction. In this case, the class bird has the highest class score, so we can say that the input belongs to the class bird.

Another concept is back propagation. We are not going into depth today. We have another class to explain how back propagation works. But just an idea: it is a supervised learning algorithm which is used for training neural networks. It optimizes the parameters. What are the parameters? The two learnable parameters: weights and biases. We optimize these parameters by minimizing the error between the predicted output and the actual target value. That's why they say this is supervised learning, because we know the input image is a cat's image and we are expecting that the model should give a good score for the class cat. But sometimes our training is not so good and it gives us the output dog for a cat. Then we say this is not the output we are expecting. So we again adjust the weights. By minimizing the error, how we calculate the error is that we know what the expected value should be and we subtract what we got from the expected value. Then we get the error. Our idea is that after the first iteration, we calculate this error. Let's say we got an error of 5. In the next iteration, we get 3. This is getting better because the error got reduced from 5 to 3. Again, we do another iteration until we get the smallest error.

The basic step is: we pass the sample to the network, then we calculate the mean squared error depending upon the output we get. Then we calculate the error terms. It starts from the last layer of the network. Then it propagates back to the layer before the output layer, and then it goes further back. So if you have three layers, the error is found at the output layer, then it is propagated back to the previous layer, and from this it propagates back further. That's why it is called back propagation. We adjust the weights from the output layer to the previous layer, going back through however many layers we have in the fully connected layer. We calculate the error terms in the hidden layers, apply the gradient descent, and adjust the weights accordingly. After training, we get some delta value, we add that to the weight, and that's how we adjust the weights. We will learn about back propagation in detail in the upcoming classes. Just as an idea, we use weight matrices, biases, and an algorithm called back propagation. All these are important concepts in CNN.

So image processing using CNN would look like this. We have the input image, let's say of size 28 by 28. We pass it to the next layer, which is the convolutional layer. In the convolutional layer, we are applying 32 filters. So we have 32 feature maps of size 28 by 28. Then we pass it to the pooling layer. The pooling layer downsamples it, so we have 32 feature maps of size 14 by 14. The dimension got halved from 28 to 14. As I mentioned, we can have multiple convolutional and multiple pooling layers depending upon the complexity of the data set. In this example, we have two sets: two convolutional layers and two pooling layers. After the first batch, the output of the first pooling layer goes to another convolutional layer. Here we apply 64 filters, so we have 64 feature maps of size 14 by 14. Then again we apply pooling, so 64 feature maps of size 7 by 7. Then we flatten it into a one dimensional vector, which is 3136 by 128. Then we have one fully connected layer, and finally the output layer. The output layer has 10 neurons representing 10 classes, including zero through nine. These are the layers where we do the feature extraction, and the fully connected and output layers are where the classification happens. So: input, then feature extraction, we downsample, and finally we classify. This is how image processing in CNN works.

What could be the applications of CNN? Wherever we use machine vision, we can use CNN. For example, in healthcare for medical scans to detect anomalies, and also video surveillance and action recognition based on classification. All these fields can use CNN. It can be used for image classification, object detection, semantic segmentation, multi object tracking, re identification, and image generation. Basically, any machine vision task, we can use CNN. Some real world applications include medical imaging for anomaly detection in scans, autonomous vehicles for real time environment perception where obstacles can be detected and acted upon accordingly, facial recognition on smartphones, education, security surveillance in airports, and quality control and defect detection in manufacturing, which we have already discussed in our earlier classes. Wherever we use machine vision, we can use CNN.

Now we get into the metrics part. Why do we need metrics? Because we need to evaluate how our models behave. Is it a good model or a bad model? We need to decide if our network or model is mature enough to give good or accurate results. For this, we have different metrics. There are many metrics available. We can evaluate classification models, regression models, clustering models, et cetera. In our course, we are focusing on the classification model. But there are different types of models. The clustering model uses metrics like average distance to cluster centres, maximum distance, et cetera. One thing to remember is that anything related to distance is for evaluating clustering models. We have another set of metrics for regression models where we use errors to measure the performance. For example, Mean Absolute Error, Mean Squared Error, Root Mean Squared Error. Anything that relates to error will be for evaluating regression models.

What we need to learn for our course is the classification model. The metrics we use are accuracy, precision, recall, F1 score, and AUC (Area Under the Curve). Accuracy means the proportion of total predictions that the model got correct. Total predictions include both positive predictions and negative predictions. For example, a person can be categorized as diabetic or not: positive or negative. Accuracy is the proportion of total predictions the model got correct. Precision assesses the accuracy of the positive predictions made by the CNN. So we focus only on the positive predictions. These are subtle differences. Accuracy is the proportion of total predictions, both positive and negative, but for precision, we focus only on the positive predictions.

Recall measures the CNN's ability to correctly identify all actual positive cases: whether our model was able to identify all the positive cases. F1 score provides a balance between precision and recall by calculating their harmonic mean. There is an equation for computing this. ROC (Receiver Operating Characteristic) plots the true positive rate against the false positive rate. The area under that curve provides a single value summarizing the overall performance of the CNN across all possible classification thresholds.

Another metric is the confusion matrix. This is a tool used in machine learning to evaluate the performance of a classification model. In the table, you have several readings and based on that we make predictions; for example, whether a person can have a heart issue or not. The confusion matrix is a table which shows the comparison of actual versus predicted values. We can have actual "no" and "yes," and the prediction can also have "no" and "yes." True negative is where the actual was "no" and the model correctly predicted "no." For example, 28,000 were predicted correctly as "no." False positive is where the actual is "no" but our model incorrectly predicted "yes." False negative is where the actual was "yes" but our model predicted "no." True positive is where the actual was "yes" and our model correctly predicted "yes." Based on this, we have equations for computing accuracy, precision, recall, and F1 score.

The ROC curve plots the true positive rate against the false positive rate. How you determine if your model is good by looking at this graph is: there is a diagonal line. If your curve is above this diagonal, your model is doing well. Anything below the diagonal means the model is essentially random. The best model will have a curve that goes towards the top left corner. The more it goes towards the top, the better the model.

What are the ethical considerations in CNN? We talked about what CNN is, the architecture, how we can use CNN for image classification, performance metrics, and now another aspect is ethical considerations. We are dealing with data that can have a lot of personal or private, highly secure data. We should be mindful about these factors. The first one is privacy. Privacy concerns arise when dealing with sensitive personal data. Sometimes we need to train on personal data like date of birth, age, or security numbers. If you train a model on such data, you should be mindful about privacy. Surveillance using CCTV is again using machine vision, but we may be risking the privacy of others. Sometimes, even without the consent of people, we are taking video of them. So surveillance is also a privacy concern.

Bias in AI is always a concern. Depending upon what data you are feeding the network, it will make the prediction. Never have a biased data set. Always have a diverse data set. Do not have a bias towards a particular stereotype or particular political interest. When you prepare a model, be mindful about privacy, surveillance, and bias in AI. Whatever you are making should be good for the society, nothing that can harm or mislead people. These are the ethical considerations.

These are the references used. Next week we will see CNN training processes: loss function, activation function, back propagation, and optimizers.
