Okay. Today's talking, we can see, because we can talk about artificial neural network and disadvantages for like image classification. Then we will introduce CNN, which is from additional neural network, and it's architecture, and we will also discuss its players in depth. Then what are the application of CNN and some of the performance evaluation in it? So this is today's topic. Yeah, so let's start with artificial neighbourhood. So I hope you guys know what does it mean by official doing it, right? Maybe you would have, like, learn a little bit in the level one or no? Or is it just new topic for you? New on the top? Yeah, this semester we're learning about this in machine. Oh, you machine learned?mes. Okay, perfect, yeah, okay. So, you have an idea about, like, artificially learning, right? So, this is, this is taking a computer system, which is inspired by the function and the functionality processing of human brain. So, it's similar to how human brain works. So, like, by a neuron, it also have, you know, a large number of nodes, which are called neurons. So we have a lot of neurons in our brain, right? So if we see an object, the optical signal got, you know, transmitted to, um, you know, the our eyes and it reaches the brain by transmitting through the neurons, right? And then it does some processing, and we know that, hey, we are seeing an object, like maybe I'm watching this project or screen. So it tells my brain that, yeah, I'm seeing this, right? So it all happens in a fraction of second. Our brain is that much powerful. Similarly, we are trying to simulate the same processing in our computer world by making use of artificial neural metre. So this is similar to a magical bureau and what is the specialty of artificial neural network is that they can learn by analyzing large data set. So when we were a child, we were maybe like learning, this is a chair, so we know that this is a chair, because we learned by, you know, seeing different shapes of the chair, and even if we see a different shape of the chair, we can identify, yeah, this is a chair, right? So our brain have like that capacity to learn from the patterns that we saw over the period of our time. Similarly, we can train our neural networks to learn from the data set, right? So if we have larger data, so we can more trained up, to, you know, analyze the images and even for recognizing objects from the image or even for performing the classification, et cetera. Um, yeah, so the why we need IML is that they are particularly effective in recognizing complex packets. So do you remember even if we have like the same object is present in an uploaded image or a different scale or a different rotation, we were able to see that, you know, recognize that, right? Similarly, the neural networks can also be trained for analyzing complex matters and they can help us to, you know, processing ages and making idea, you know, so these are the reasons why we can use make use of gain and for image classification. So artificial neural network is just a human system that's similar to human brain. So now let's talk about how creditive models uses traditional methods like decision training. So this is an example of the decision free algorithm. Maybe you know about the decision where you've got them before or yeah, so I don't want to go in there to that because this is not part of our course, but I just wanted to show you that why we cannot use this in image processing or in machine. So, So, how predictive models uses traditional method is, like, you can say in the left hand side, there is a table, which a categorizes images based on different features, like ear shape, face shape, the presence or absence of this curse, et cetera, right? For example, if it says, you know, and if it if it identifies an animal, which has a po in the ear shape, then it looks for its spay shape, and if it is brown, then it looks for whether they have whiskers or not. If it is there, then they categorize that because animal into a cat. Okay, so this is how we make decision based on, you know, the features that is available in the left table. And other animals, with a different combination of these features, we call, it's not a cat, for example, if the ear shape is floppy, then we will look for the face shape and it says round. And then again, look for the other feature, is scarf, it is there or not, it says it's absent in the 3rd row. So then the system or the, you know, the system categorizes it does not take care. So this is how the decision was made using the decision frame. Right. So on the right hand side, you can see a decision free representation of the same process, which I explained in the table. So 1st so how it works is that it sequence, we look for each feature. So it sequentially like both each feature and then it make the decision. So 1st it start with ear shape. And then look, if it is pointy or floppy, let's say it is pointy, then point just is, they move to the next feature, which is face shape, right? So in the face shape, he looks for whether it is brown or brown, because now they categorize that animal as cat. If it is not brown, they call it, like, they categorize, so classify it as, not a cat. Similarly, on the right hand side, you can see, if the earship is floppy, again, you look for the presence or absence of is good. So if it is reset, they can't invest, it is cat. Otherwise, it's not a cat. So this is the process of, you know, decision free, how it classify an object in your cat and not a cat based on the features provider. Don't you think we can make use of this majority machine machine. So, you know that what's the input of, you know, the input that we give for machine machine system. It's mostly images, right? Or videos. So we cannot make use of this method for our purpose. So we need something different. That's why we say we need some neural networks, like artificial neural network. So you can see, this is our airport. We don't have a table with the features or, you know, a decision for you to have a said, if it is, like, you know, a coin ear or pro ear, we don't have that kind of data. What we have is just some images of cats and dogs, right? So this is our image, data said. This is what we are providing to our network and the network should be able to learn from these images. And when we deploy this area of production, it should say we give a cat's image, the system for the network should say, hey, this is a cat's image. Otherwise, it should say, no, this is not a cat. Maybe it's a dog. Yeah, so this is the data set we have. Here, you can see a representation of the artificial neural network. So we have given. The data set has the input. We give both images of cats and images of dogs to train them. And we input that, you know, the input layer. So we can see that for artificial neurodual. There are dreaming layers, which is input layer and hidden layer and an output layer. So, ANL will be like this. Input. And we have a hidden layer. And... So, we are giving our image to the Indian layer, where the input layer can, you know, the shape of the ear, or shape of the face, etc. So this is where that happens. And from that data, is being passed to the hidden layer, and the hidden layer do some processing, and finally, it can make a decision, or it can classify, even two classes, maybe class dog at a class cat. So finally, you create 2 output classes, which is cat and dog. So that is what is shown here. We have a large number of photos of cats and dogs. We pass it into the input layer. In the input layer, they can identify the shape of, like, you know, the pay shape or the shape of the eye or the presence of the score, et cetera, and then it pass to the hidden layer, and the hidden layer, do the processing, with the features that they got from the input layer. And finally, after processing, we get just two classes, which represent the class gathered, faster. So, this is the overall idea of A in. But there are still limitations for using artificial neural network for image classification. So, let's see, um, you can see an image of a cat there, right? So, let's say that it's 1000 by thousand pixel size. So this is a coloured image, right? Not a case scale. it's a colour idea. So it has how many channels, great, red green, and blue. So we have, like, you know, so if we represent this into an artificial neural network, or if we feed that image in you, an artificial neural network. The 1st layer, 1st layer is the input layer. The input layer will have how many, so we know that 3 times 2, 3, like 4,000. So we have like, you know, 1000, 5,000. fix up, right? So it can rest to 6 window. So it will be 3 times 3000000s of neurons in that input layer itself. So each pixel in that input image is keyboard as enable to that input layer, right? So each pixel in the input may just email as the input to the. plus layer. And then it goes to the 2nd layer, which is the hidden layer, right? So hidden layer, you can see, let's say that they use 1000 filters. So here is where we do some of the processing. So let's say that we apply 1000 figures here. And what is the use of interest that it can detect features or shape or whatever. So let's say that we use 1000 metres. So again, we know that each neuron in this kettle layer is connected to the neuron in the previous layer. So you can say that each layer is connected there, right? So totally there are 3 periods, 3 into terrorize to 9 because we apply 1000 pages. So finally, the output layer, we have 3000000s of 2 rodes. So you can easily see, say, that it's computationally very heavy, right? So this is just for one image. Like this, we have in number of images in our data set. So it's very heavy computationally. So we cannot relay your aim. And another problem is overfeitting problem because we are giving a lot of input, like input pixels. So we train it, you know, a lot, so we can overfit. And the 3rd one is longer training time because we need to train all this like huge network, right? So it takes a lot of time to train. So these are the limitation of artificial neural network for image classification. So what is the solution for this? We cannot use traditional method because we don't have like, you know, features or data, which is enough for making a decision using decision-tree weather. And we cannot use ANN because it uses each pixel in the input image as a because it uses each pixel in the neighbour. So the network is computationally very heavy and it takes a lot of time to train it and also it can have like overfitting problems. So then what is the solution for this? This brings us to the convolutional mural into it. It is basically similar to ANN, but become more layers in hidden layer, which helps us to focus only on some important data. We don't want all the mixes from that in particular. So we will see that in locker basements. So, crimulusional normal into our cases, deep learning technique, which is designed specifically for images, specifically for analyzing or extracting images, or processing images. And as I mentioned, in N and also it can learn patterns from data, right? Similarly, CNL also do the same. It can automatically learn data. Sorry, learn patterns from data. And another important objective is that they can solve Congress special class with deep learning. So like I mentioned, even if it is a complex image, they can still learn handed, and then they can perform the object recognition or no classification, et cetera. And the benefits, it addresses all the limitations of artificial neural network by dealing with high dimensional structured data, like images, video audio, and it can hieratically, it means extract the feature. And then they are robust to translation of object. So all these benefits make it a better candidate for us to use it in that machine machine system. So it's always whether that aid, because it can handy high dimensional data and they can learn the feature hierarchically and also they are robust too, translation of object. So let's talk about its architecture. This is very important. So if you are asked in the exam to draw the architecture, so this will be the answer. So you can see we have input layer, you can layer, and now to layer, same as the ANN. But there is a difference, because this hidden layer may be replaced by different layers. So, we have the input layer, and instead of a hidden layer, we can have an emotional layer. And the moving area. So, it's basically, same as... So, the difference is that the Nerlayer is to replace the pie, congressional layer, fully connected layer. So we have the same standard, but more layers here. And in this architecture, you can see, we are giving the input, and the input image is given to the next layer, which is the primary rotional layer, you can see a lot of, you know, the passes there, right? So it means we are applying one, two, three, .5 filters there. So when we apply 5 filter on this input, we get 5 feature maps or 5 outputs. Okay, so that is the Faberation is doing. Then we gave that to a pulling layer, pulling layer, actually down summit. So if you look at that, the size of that box got reduced in the pooling layer, right? The comparing with the green and the blue box, you can see the box is small, right? So the direction, what produced? So that is the benefit of pulling layer. We gave the neurons to the conditional air. Here is where we are extracting all the important features. Okay, then we pass that feature to a moving layer. Here, we downsize it on down salmon. Okay. So what is the benefit of doing so. We don't have to deal a lot with the computational process less, right? Because we don't have to deal with the larger size. We are reducing them dimensionality. Okay. So that is the importance of pulling level, you can downsample it or downsize. And then this output is given to the fully connected layer. So this is where the action, you know, the magic is happening. This is where we are training the model, we are adjusting the connection and weight that we trained up until we get the set output. So that is where downput terrace showing. So it can have, um, the output layer will show the in output layer, you can see 3 purple violet dots, right? which represents 3 classes, maybe class, dog, man, or whatever. So the output, the neurons in the output layer professors the number of classes. Okay, if it is a classification problem. And also, if you notice, we can see that the, there is the market for extracting features, right? So this happens since the Hadalusia and going here here. So this washing is where the extraction of the learning process is happening, extracting the features is happening. And this is where the way connected around, this is where the final classification of this happening. Okay. Yeah. So, the alien typically consists of an input layer, multiple hidden layer. So when I say that, it's not necessary that we just have one convolution layer and one pooling layer, we can have one pool, revolution layers and multiple pooling layer, depending upon how complex is your data set is. Okay, if you have like very large data set, you can have multiple layers. So anyway, we are not going to read whether we are using a lot of, you know, already limited algorithm and most of them uses multiple layers of kind of lotion. So you will see when you implement that in your lab and project. Yeah, so this is the basic idea about that, additional neural data, we have multiple layers. The hidden layer include a series of emotional layers, only layer, and the fully connected layer, and each layer perform distinct operations. As I mentioned, each of the layer will have different functions. The conventional air, the function of conventional air is that it can extract features. Yeah, so... The congressional layer, it applies the congressional operation, and when we apply the congressional operation, it extracts only the important feature, right? The important features are extracted. And the polling layer perform the downsampling, it reduces the size, and the fully connected layer uses all the features to compute the classcode. Class code means in the outputlay until you see, like, three doors. I mentioned, like, that processes 3 different glass. So for each class, let's say that there are some class scores. So depending upon the class, or we can make decision like, yeah, this is a cat, or this is a dog like that. So which class, how maximum is called, that is the one, the model is predicting. Any down from the architecture? So, this is the title you will be asked to draw for your example. You know, it's asking you to draw the app, and just so this is the... So I'm just going through this again, like the key components of CNN. So if you ask, like, what are the key components of convolutional neuro network? It is these three layers, the convolutional layer, pouring layer, and the fully connected layer. And the rest, everything is similar to A, then we have a good layer around today. But the layer is replaced to buy, amulosia, pulling is fully connected, and congratulations layer extract the spaceship features from input image, pulling layer, ready, spatial dimension, and simplify the computation. Reconnected rate, integrate the features for final classification. So just like rehydrating to show you the importance of this 3 layers. Okay, now let's... Now let's deep dive into a congregational layer. So in the in this video, you can see there is a small, you know, kernel or filter, which is looking for the shape of I. So, when we convert that with the input image, the motivate is the shape of the face. And so, I mean, the image of the face. So if that colour as it travels across the width and height of the image. So it looks for each location, if it is matching or not. So first, two rows, it's not matching, then it comes, and it identified, two locations where the exact same shape, beside identify, right? So in that location, they gave some value or some different weight to show that, yeah, that needs to be identified, and we need that information. The rest of the pieces are not needed for our further processing. So this is the basic idea of evolution. We apply different types of filters. So we will have either the filter that can identify the loads. So it looks for that particular shape there and it's identified. So similarly, we will have any number of filters to be applied on an image. so that we can extract all the important features from that image, and then we will use this to train the model. And finally, it can give us some classification score for, I mean, class score for each class. So, as you can see, in these layers, small learnable features, you can slide over the input, like, you know, just to extract the features, such as edges, textures, and shape. So when it passes through the images, what it does is, it extracts your shape, like it can be a cycle, it can be, you know, even a shape like I or even just edge. When you did the carri edge detection. It just detected the edges, right? So similarly, it can't just exercise the features. And their features includes interest, textures and shapes. And each filter in a communational layer detect different features. So again, I'm saying we uses any number of filters, and all these, any number of filters can take in different features. So 10 of the filter is doing the same purpose. So for example, if we are using and energy detection for horizontal ledges. So it just, If we take the horizontal edges, and we will use another filter for detecting the vertical edges. And another one for, just detecting the corners maybe. So each of the filter have its own purpose. Okay, so we uses any number of filters, and AI, these same number of filters will detect and define features from the image. So that's why we can say that we can deal with complex structures because we are identifying those complex patterns by using different filters. And the convolutional layer does play social role in features, detection, and representation. And that's why we are able to use CNN to perform the image classification or image recognition. Because the convolution is the basic principle behind this. It helps us to identify the features. So, uh, I also mentioned, it have the ability to extract features hierarchically, right? So heretically, which makes, first, it can detect law level features, the mid-level feature, at high level features. So if you think upon the filter, we use KitChat, take a lot of a feature, like this. So, if you look at that image of the car, it is, it can change, the law level feature, like, small lines, or some patch, or 6 o'clock. And when you go to the mediaeval feature, it can even detect some shapes like circles, or even more clear, you know, some patches. And then you come to the high level feature. It can even detect the shape of, you know, a doll or, you know, even more precise shape. In reality, like if you have, like, if you go deeper the network, it can detect more complex patterns, like even the shape of a house, shape of the window shape of the car. So we have filters for that. So even when you go deeper than anyone, it will extract more precise features. So that's why they say that the CLN can learn packets can advocate. So that's an advantage so that it can, we can learn even the complex patterns from that image. So the basic principle of CLM is to automatically learn and extract hierarchic procedures from the input data. So that's just what is shown in this image, we have different hierarchy of features, law level, high level, and high level. Okay. Yeah, so what about congressional layer? In the left side, you can see the image of a cat. Again, it's a coloured image. It is not a grass game. It's a coloured image. And the middle image, which we can say that this is the input to the CNN. And the middle image is the like output of a basic energy detector, the applied online, I mean, just a basic one. So you can see that the edges of the cat's face is detected there, right? And the right hand side, you can see some of the feature maps that we got as a result of public different filters. So, you know, what is feature? My feature buff is nothing, but it's the output that we get in the convolutional layer. So, the output from these layers are called feature mouse. And this pizza amount is given to cooling layer, and output is cold pizza. Okay, so feature top is nothing, but it's the output that we get by applying filters for the input image. So this right hand side shores for different feature maps that they get as an output by applying for different filters. Let's say one is detecting horizontal, one detecting vertical edge, one detecting maybe the shaft points, et cetera. So these are the output of 4 different features. similar, but there are some kind of differences in detected features. So the convolutional layer, healthy network, to focus only on the most important feature. So do you remember what was the disadvantage of detain and because it would be hard to give all the pixels in the image into the input layer, right? So because of that, it was computationally very heavy. What is the benefit of using an emotional error is that we don't have to use all the pixel in that cat's image to the labour. What we do is we apply the contribution. We just pick the features like, you know, the important features only. And then the network is using just these features. It's not taking all the pixels from the input image. It's just taking whatever we need for processing. For what we need for making the decision. So that's the benefit. We are making the legible more lighter than the ANL, because it's some additionally light and also may use more layer, which can help, again, to down sandwiches. Okay. So not all the pixel information in the image is relevant for training the border week. We don't want all the pixels from the image. So we're just taking whatever we need. Yeah, so it grows the performance and accuracy by using this convolution compression. So that's the importance of evolutional play. Yeah, so this is how revolution operation is performed. So the left hand side, you can see a metrics, which is the input image, and we have a filter or kernel. So these birds, maybe can you say them changingly, both are the same kernel and just filter. So the filter is in the widely filter, and it's 40 images, 6, 5, 6, yeah, 6 by 6, I will be by 3, and we get an output image, which is again, like 4, 5, 4. So how we get this 0 is that what we do is we check the filter. We scan this filter across the height and we'll top the security gate. And while we scan, we take a dot product and we take the sum of that dot product. Okay. So it is clear from this calculation here. Do you want me to show it in the board or? Yeah, okay, so anybody want me to show it in the board, like how to calculate each point or this is good enough to understand? Okay. So this is how the congregation is performed. So this is how we got the value 0 in that custom decision. So after we calculate in the 1st value, what we did is, we jammed one pixel to the right. Okay, so we move one corner to the right, and we do the same operation. We take some of the dot products between the kernel and dash XL values. And how that's how we got by this one. Likewise, we powers across all the high, all the weight and the height of the age, and we get the final output image. So this is the congional operation. So you can expect a postitorial exam to perform the convolution operation and, you know, they get their output memory. So this will be the final outward. Did you notice the size got reduced from 6 by 6 to 4 by 4. We just pick what we need, but we don't want all the pixels in the input image. Filter. The feet of the house, we set it manually. Yeah. Okay. In reality, it's like, you know, the algorithm, we are making use of already pre-built, you know, political race, right? They have different features, but technically, yeah, we can... Yeah. If you think about it. Yeah, but we wanted for the image. Yes. So, I think this is the equation for community cloud, but in basis, you can expect white person from this smart also, how to commute, you will be given an inward filter and sorry, input image, size and the filter size and then you'll be asked if you compute that. So how you are doing is, let's say that you have a... Slice, okay. Yeah. So this one. So let's get to that equation later, so let's explain this one first. So convolutional layers, we are talking about contributional layers. We know that the convolutional layer help us to extract only the important feature, so we don't have to deal with all the mixers in the input image. So, but there are several factors that help us to, you know, make our model better. So one of the factories is that the filter size, so it can be the main, the extent of the input data that each filter cover. If you select small feeder, it can have a small area in their good image, right? If you select big, big, you know, let's say, 11 by 11 filter, it can cover that much area from the input image. And it affects the granularity of the feature. and small feature. So what is the benefit of using small filters that it can detect very fine detail from the image, maybe, like, small, like, edges or small corners, et cetera. So it can be set. It can capture fine details. So if you use the larger filter, what happens is that it can filter broader pattern, maybe it can even detect the shape of a home, shape of a tree. So, depending upon the filter, we use, we will get the output feature now. So if you use a small filter, you can get fine details, but if you use larger one, you will say identify product pattern. So that is one of the, you know, one of the criteria that helps us to decide how our good image would be like. And as the one is stride. So stride means it is the step size with which we are sliding across the input image. For example, like I told you, right, we move one pixel to the right, which means stride is one. So if we say that's tried equal to two, we skip one column. So we skip two columns, and then we move to that 3rd one. So that is what stride means. It's the step size with which filters more across the gameboard. And if we deem like, you know, bigger strides, what is the advantage is that if you won't go through the repetitive says again and again. For example, if we have a fit drinks like this, What's your passwords? I can see. And we have a 3 by 3 filters. So fast, it's both through this filter, right? And if we choose a stride of two, it will skip these two column and there it can come here. So it is not visiting this forum again. Otherwise, it will be taking this quality game, right? So it's a repetition of the same pixels. So the overlap of facility feels can be avoided, and the size of the output feature map can be controlled by doing so. If you use bigger strides, the output will be very small, because we are swipping many of the mixers, right? So largest rights are certain, smaller, more abstracted features. So we covered the filter size. We use bigger filters, we can cover slab the patches. We use smaller printer sites, it can detect fine details. And if you use bigger stride. What happens is the output image will be very small. If you use a smaller stripe, you know, like it is much bigger than they because of. I mean, if you use the bigger straw. And the 3rd point is padding, padding means you are adding a layer of cereal around the input. So you are padding in with zeros all a lot. The advantage is that let's say that we have some important information at this pixel. So if we are not having the padding, what happens is when you do the convolution, 1st it comes here, and then it goes here. So we visited this week, so only once, right? We are not visiting it again and again. But if you have a padding like this, what happens is that, 1st it will detect, there's 3 pixel, and then the next is try, it move to the next, so it, the 1st time it detected, and again, in the 2nd iteration also integrates it at this one, right? And again, maybe if you come down, it can maybe suggest like 2 or 4 times. So that's the reason why they use padding if we like we have images which have more important information in the corner of the images, we can use padding to protect that. So it's related to the stride size as well. If you have 2 stri-sizes with pedding them. If you have the stride size bigger than one, then we would have to like more padding on the left. Then it's better to avoid biggest flight, right? If you have like good information in the corner, better to avoid because bites. Otherwise, there's parking in adding... 20 7 p.m. So, that is the padding, validation of zeros around the input border, which allows the control over spatial damage. So if you do that, what happens is sometimes we need the same. If you think like you don't want to reduce the size, we need all the information from the input pixel, then you can apply the padding so that they have control over the size. We have, like, we are pretty much keeping the same size of the input or, like, it's not getting much smaller companies, though. Like if we are not using the bag. Yeah. So, the preserving edge information, elaboring deeper layers to fitness, patient hair up, increasingly formless, and of scratch features. So these are the main points that have impact on the output from pollution layer. So do you remember what is the name of the output from the congressional layer? It is feature mouse. Okay, feature books. And when you go, you pass this feature part to the cooling layer and the output of the feature mark from the pouring layer, it's one bold feature. Okay. Yeah, now let's talk about this equation. So now we can see what's Fridays and padding this from this image, right? So, the image output size is given by the following, N minus F plus 2B divided by S plus one. So let's say that we have an input of size, you know, input image of size 5 by 5 and I'm applying a filter of size 3 by 3. And let me say, like, maybe I'm applying a party of one. Padding was one. And they stride off two. So, I will be asked to commute, but will be the output. In that case, you can use this equation, N minus F, endmates, the size of your input, which is 5 here. And yes, males, size of your filter, which is 3 here. So in minus F, plus 2 feet. 9 is, how much? one IBA, and divided by, yes, this is, and maybe 2 here, and plus one. So, plus, two, by, two, plus, one, it's just, uh, 4, 5, two, plus five, it's 3, 5, 8. So, when you are asked to compute the size of the address, you need to, you don't just leave three. You have to specify three by three. So that's how the damage is. Okay This is how we calculating output. What's the past one? That's a question. Like, you know, this is how we compute. very output to make size. MISF plus 2B. So, if we have a padding, usually we use, like, you know, if you have zero padding, zero padding is, like, no, so we are not playing like padding, and, like, stride is one, we have the question again, minus F +one. So this is like question, just take question. If you add plus one, then you will get the um. Yeah. Yeah. Now, let's talk about pooling layers. So, I already mentioned, right? So the output from the convolutional layer is given to the pooling layer. And pooling layer is useful for reducing the size clothes a day, downsample or downsize. Okay, so that is the importance of using a plate layer. Responsible for reducing the spacious size of the feature map generated by convolutional layer. So, again, so, output of the convolutional area, feature map, and they are being fed to the pulling layer, and they downsampled it. There are 2 weathers for performing the pooling. Like, one is fax schooling. Another one is average pooling. We can see that in the next slide how to do that, and... So if you put another use of cooling layer is that. By doing the downsampling, what it helps is, it becomes our system become more tolerant to the variation and distortion saver, in the later, even if it is within some area, we can recover it. Here in the pooling layer. So it enhances the ability to generalize. So that is main idea, the downsamping and the computational cost is reduced due to this. So this is how the pooling layer is acting. You can see the output from the founderotional layer is here, so that is, it's an energy detector. Maybe we applied and it's restricted to this or maybe an emotional layer and we put a feature of like this, and we are giving that to the fully. counting of the cool area would look like this. If you are using a max. You can see the size for half of it, right? from it more by 42 by. So the size got reduced, and the feature, it highlights more features there, only they bought a feature, but it generalizes the features there. So that is max food and average food. And I will show you how to calculate my spooning and average food. It is simple. Let's say that we have a The feature map like this and I'm showing different colours for the squares to just show like how this has become like. Just one person might explain now. I'm considering it 2 by 2 block, but Dubai 2 kernel, and I applied. So first, Dubai to block this representative orange car, right? So hand scoring, what we do is, under that block, what is the maximum value we just take? So here, 6255 plus block, right? So we take the maximum value 6 there. And the next, then we move to the next drop. And here, 753 months. So which one is the maximum? Seven. So we got 7 there. And in the green block, we have eight, and the blue block, we have five. So this is how the max calling is working. So we've got the maximum prominent feature front on those blocks. And for average pulling, we take the average of that block. We take the summer of it and divide it by four. So for each block, we compute the average of the mixes to that. So this is average moving. Yeah, you can see our, uh, A sport video of it, like, how they computer. So this is the feature map, which is the output from the evolutional layer, and then this shows the pool the feature map, the output of the pooling layer. Okay, any downtown, uh, carol ocean and boring? Maybe we can take five minutes to break it. Coffee, sauce, and then putting layer. Now, the third one is the fully connected layer. So this is where all the magic is going to happen for training our modern life. Like, we get the output from the models, from based on the art clashes that happen in this layer. So this is one of the important layers, fully collected layers. We can see the image. There is a feature map. So I already mentioned, like, the output from the convolutional and fully layers is called feature map, right? So that is what we are giving us input to the fully connected layer. So we have the feature map, which is where, you know, that's fully connected there. So if you remember any architecture. So in the architecture, you have a layer here, right? which is called flat tent. Do you see that? So what it does is... So, the output from the pooling layer, it will be multidimensional area, like metrics. It will be multidimensional, maybe, and by, or something like that. Okay, so we will convert that into a one dimensional vector. So that is part that is called flattening. So we are just flattening it in your one direction. From multidirectional to one dimensional. So that is what that layer is doing, the flattening, the flatten layer. The output from that will be a one dimensional vector, which will be the input to the fully connected it. So we have something called factory here. Planting happens here. Okay, so it converse, multi, dimensional, tripto, single. Okay, so that's the importance of bladgeting layers. What it does is, it just flattens our feature mouth, and give us email to the... So that is what is represented in this yellow colour. So the feature map, we flatten it, and then we give that flattened output as the input to the fully connected layer. So in the fully connected layer, there are multiple layers in the fully connected layer. And finally, we train in the fully connected layer by adjusting the weight and, you know, other layer parameters, we are just there, and finally, we have an output layer, which represents the number of classes. So the number of neurons in the output layer, this layer, the number of neurons in this layer of classes, the number of classes, which is a classification problem, which is a binary classification, we will have two neurons and then how? Okay, so because my difference, like, let's say, in this example, the number of classes, yes. So we have one, two, three, up, two. Yes, yes, is the number of classes, depending upon the number of classes we have, that very neurons, is that on one layer. Okay. So this is the layer. The fully corrected layer is the layer where the high level reasoning, based on the features that we extracted from the food, uh, convolutional layer and cooling layer, is compiled to make. the decision, right? So we train, like we convert that high dimensional feature map into the probability distribution. Like I said, if it is a binary classification, we will have class A and class B have stay output, right? So, our model capsule... Maybe eight. So, these are the probability, uh, that I'm more than predicted for our classes. So, class A have probability . Plus, we have .2. So, which means, the model is safe, the object we gave to the model is belonging to which class. Class A, because we have high score for plus A, right? So that's what is mentioned there. We transfer high-dimensional feature mapping to probability distribution. So depending upon which class is having the higher value, we say that, yeah, this object belongs to that particular possibility. So after the prevolution layer, I pull it there, extracts the features, and then we flatten it, and present it to the fully connected, so the fully connected layer combine all these features. Okay, and then they make prediction or classification based on that mixture. And each neuron in the carefully connected layer is connected to each neuron in the previous layer. So we have in terms of connections between the neurons in the consecutive layers. So the main idea is that if you have connections from all the neurons to all the other nodes, the next layer, it can represent the entire representation of the input and we are not missing any of the input data. Right? Because we are presenting only the input, important data from the input, and we are not missing any of those important data when you make decisions, right? For that, we need to have connection from each of the neuron, we identify. So that is what this layer is doing. The fully collected layer, combines all the features, and perform the prediction or classification. Okay, thinking about flattening. So I mentioned about flattening in that academic, right? So this comes after which layer, pouring layer. So the feature mark from the pulling layer is given to the flattening. So here, then that, what happens in the flattening is that, it takes a dimensional object, or an array, or, like, matrix, and then it convert the matrix into a single direction, like, you know, one damaged. That's the exact word, one dimensional. Okay, one dimensional vector. So it converts it into one dimensional vector. So, how it converts is that? Let's say, for example, we have a 3 by 3 feature now. How it does is that, first, they take the first row. Okay, and we, like, the bus row is one, one, zero. Then we take the 2nd row. Just 4, 2, one. We are a building, it's, then 0 to one. So we get. One time I share, right? So this is how we planted our feature now, we look one dimensional. And this is the input to our fully connected layer. Okay. So, like, evolution and polling there produces white, feature masks, right? Future marks, and this feature marks are multidimensional, and flattening converts this feature map to one dimensional. Like this. And it can get, how it does this, it congratulates 11 elements along depth. So it's just calculating elements along the depth direction. And this enables speeding into fully connected layer. So we can feed all the identified features in Europe, later directly, by police. Yeah, so that's... Another important factor in the convolutional neural network is weight metrics and bikes. So this is the way for all deep learning techniques. Okay, weight, metrics, and bias vector. So these two are the key players in, you know, Pomignon's system. So they are the ones which we are trading, actually. These valleys are not constant. We just start with some rice. value. Then we get an output, and we say, no, this is not the output we are expecting. So we need to train them again. So we ask them to go and update this value and get another output. So they bring the output, we see heist. This is, like, better than the previous one, yes, then, but it can be better again, right? So we try, until we get the best of most of our results. So this is the basic idea. So, weight metrics advise, but these 2 are the learnable parameters in the network. Okay, so these 2 are learnable. Over the period of time, they can learn from data and they can change their value. Okay. And these are the foundation for deep learning Alberta, and even in CNN, like, we use wave metrics and bi spectrum. So let's say that the XY is to explain, explore in this image, or a process, the flattening of the flattening vector. And this is our current layer, like the 1st layer of the, maybe the flavour of the layer, I see. And we have A1, A2, A3. So these 3 are the new ones presently are like... And how we have, like, you can see each neuro is connected to each of the infravector in the previous layer, right? So there are connections all between all the neurons. None of the neuron is missing. They are interconnected, right? So this is how the weight, fedrix, and bice vector is computed. The numbers in, I mean, the letters in colour represent, like W1, W2, W3, W1, that, ventry, represent the wave in race, and B, B, B, in colours represents the B, spectrum. Okay, so how we choose the length of the bias director is that? I mean, by spectral, the big metrics is that it is equal. It is a led by your metrics. So you see W1, W2, W3, W4 like that, right? So there's a metric. So this, let's say this is an end by AM metric. So far in will be is the number of neurons in the, in the layer. So how many neuros we have in our current layer? One, two, three, right? Right? This is the current layer. So number of erons is three. And so we have like 3 rows. And BM is the flat intervector, like, how many vectors we have in the flat? Yeah, X, one, X, 2, X, 3, X, four. So this is it, 3 by 4, metrics. Now, page metrics is 3 by 4, and then, and the length of the, uh, bi sector is equal to the number of yours presenting them and there. Okay, so we have A, A, to A, so that many bicenter. Yeah, so this is how the matrix is multiplied. We have our weight metrics and we will take our input from the flattering, and then we add, right? We add the bias. So, the output will be, like, just, we multiply this W, one, X, one, and then W, 2, X, 2, W, 3, X, 3, W, 4, X, 4, plus this B, for computing the 1st position. Okay, so, um, You can identify that here, and then let's say that A1, A2, A, they finally make that free, Neurons, which says A, but A to A. I'm not going to show the multiplication, it's explained in this life. Okay, or do you want me to explain the phone? Yeah, so, weight metrics, length, bi spectral length, it's okay, right? Depending upon the number of neurons in the current layer, we can commute that. Yeah, but it's. The reason why we use this weight and breaks by a vector is that we need some sort of nonlinearity. Why? We need nonlinearity, is that, otherwise, if we say, for example, what does linear means, this is a linear question? I'm sorry, bicycle. FFX is equal to 2x plus one. So this is just a literary question. The water rate would be if we have a one to one connection or straight connection, right? There is no, you know, it can just say like if we give one, it will be three. And if we give two, five, like that. Okay, so there's, like, a one to one. There is a one to one connection. So this is just, you know, simple system. It cannot be with complex patterns. For that, we need some sort of nonlinear, I feel like, you know, some curves, some something that can, like, you know, cycle or uh, some curves or some swamp or bell shape. All these are representing some complexity, right? We don't want straight lines or straight output. We need some sort of non linearity in our system, then only candidate, more complex, padded. So that's why we need non leniality. For this, we use another concept called activation function. So that we will see in the upcoming slide. So did you get an idea of paying interests and vice versa? any doubt on this one? Okay. So keep in mind that these 2 are the learnable parameters that they were. Yeah, activation function. So, as I mentioned, we need some sort of nonlinearity to the network, right? For that, we need activation function. So activation function is the function that determines if a neuron fire, or if a neuron is needed to the next level. For example, in this previous example, we had able to get PSD output, right? Which means we had input layer. So in the input layer, we had this A1, A2, right? But we, sometimes we don't need to keep this, even A to A to the next level, we just need, maybe, even, and A to. So how we make that decision? We can drop some of the neurons when you train them. Okay, so how we do is that based on the activation function. Okay, so it identifies if this neuron is qualified enough to go to the next level or if we need to keep that. So upload, and this can be uploaded in different layers. It is even applied in convolutional layer, like after convolutional layer, and after a fully connected layer, and even after output layer. So it can be upgrade its different layes. And most probably used to one is prelu, which we will learn about all this activation function in the upcoming classes, but this is just the information of one activation function is doing. So you can see, from the previous example, we have input H1, the weight W1, and we multiply, we take the weighted some more pay, and we have a wise P. So this is how we take it, right? And then we apply a function, which is represented as yeah. So that is the activation function. This activation function. Let's say for Grellu, is an example, like Grellu, means F of X, equals, like max, 0, max, 0 form of X, slips. So if the input value is greater than zero, we take that value. If it is, um, less tensive, like, later, or equal to zero, we take that value, if it is less than zero, we make it zero. Okay, so that's the basic activation function that we commonly use. So we take quality faucity values, right? So over the negative values, we just disregard it. So this is how we make decision life with that neuron can be fired or not in the intro. So now let's talk about output layer. So this is the final layer, right? Final layer. Yeah, so the final layer, where we generate the prediction of the classification is here. The neuro in the last layer matches the number of glasses, so I already mentioned if it is a binary classification, how many neuros they will have two, right? Because it matches the number of classes. If there is 10 classes, how many neurons we would have. Yen, neurons, we will have in the output lamp. So it matches the number of classes. The activation function differs and finalists. So the activation function we use in fully corrected layer is different, the activation function we use here is different. And for a binary classification, we use something called soft max, we will learn about the same detail in the class, but just like overview of what you can expect the hardware. And the highest probability neuron, represent tradition. So as I mentioned, we will have different classes, right? And at the end, what we get is some class sports, let's say 0.5, 0.3 or 0.2, for these 3 dogs, what we take is the one which is a high class code. So one with the point 5 is maybe bird. So we have like an input, it pixels from the flat in the matrix of it, like a hot wind, we are presenting that to the the great, the blue colour represents the plantin layer, we are giving into the fully connected layer, and we have multiple, fully connected layers, and we do all the training there by adjusting the weins and the bias. And finally, we get big glasses with class code. So the one which have highest passcode, invading prediction. So in this case, the last bird has highest to the sport, so we can say that, say this belongingput belongs to the object of a hostel is a bit. Okay. And another one is back from location. We are not going in depth today. We have another class to explain, like, how to do, like, how, how bad population are going to work. But just an idea like. We can say supervised learning algorithm, which is used for training neural networks. So this is the way I work, that we use. Here to try our network. So it optimizes the parameters. So what are the parameters I mentioned? Like, there are two ridable parameters, weight sad, bias, right? So we optimize these parameters, by minimizing the error between predicted output and actual target value. That's why they say that this is supervised to learning. Because we know that, the input it makes, we are living is a cat's image. We are expecting that. It should, like, gave us good score for the class cat. But sometimes it's our algorithm is maybe our training is not so good that it is giving us the output dog for a cat. Then we say that, no, this is not the output we are expecting. So we again, just the weight, right? So by minimizing the error, how we calculate the error is that we know that this should be the value and what we got is subtracted from the expected value. Then we get the error. And our idea is that after the faster impression, we calculate this error and we say that, yeah, let's say if we got five, and in the next direction, we've got three, let's say, oh, this is getting better because now the error got reduced from 5 to three, right? Again, we do another right rush until we get the smallest here. Okay, so the basic step is, we take the salmon to the network, and then we calculate the means quiet error. Like, depending upon the output that we get, we come here. And then calculate the error chame of each or computer. So it starts from the last layer of neon. Okay, and then it propagates back to the layer before the output layer, and then it goes back. So it's like if you say you have like 3 Yeah. So this is the output layer. You find the way here, then it is propagated back to this layer, and from this, it propagates back here. So that's why the call exists back propagation. Okay, we add just the weight from output layer to the layer, to the previous layer, and going back to how many layers we have in the fully connected layer, it goes back. Okay, so this is how the back propagation works. So I try to calculate the error terms in the hidden layer, apply the data, and I just survive. So after I training, like, you know, we get some delta value, we add that to the weight, and that's how we add just the weight. We will learn about this back propagation under the window. I'm going to make glasses. So it's just an idea like we use weight metrics, bias and algorithm called back propagation. So all these are the important concept in CL. So image processing is CNN. So this is how image processing in CNN would look like. We have the input image. Okay, let's say this is size 28 by 28, we are passing here to the next layer. Next layer is done, and the rotional layer. Okay, evolution layer, we are applying 32 visitors there. So we have 32 times per A by 2K. And then we pass it to the next layer, which is the pulling layer. Pulling layer, we downsampled it, still we had like 32 times 14 by 14. So the divestion got half, right? 28 to 14. And remember, I told you earlier, right? We can have multiple convolutional and multiple pooling layer, depending upon the complexity of here, take us in. So here in this example, we have 2 sets of work, like 2 conventional air, I 2 put in there, right? So after the 1st batch, PLPB, the output of the 1st moving layer to the another convolutional layer. So here we apply 64 filter, so we have 64 transported by 14, and then again, we apply the building. So 65 tax, 644 tax are advisor. And then we make into a one dimensional vector by flattening. Right. So we flattened it, and we got the one dimensional vector, which is 3.36 by 128. And then we have, like, you know, one layer of liquid and layer, and, finally, out of layer. So do you know, like, this is how many classes we have? Nine, nine, nine, 10. What is zero? Oh, no, no. So, number of, you don't sit out, but it represents the number of... Okay, so these are the area where we do the feature extraction, and this is where the classification is going. Yeah, that's basically, we have the equity page. Yeah. After we extract it, then we classify, based on the feature we extract. Yeah, so input, then feature extraction, we downsampled, finally we classified. So, this is how the image process is. So, what could be the application of CNN? So, wherever we use fish emission, we can use CNN, right? For, like, in health, care, for, you know, for medical scans to detect the anomalies in the scans and also, you know, where the video competition of it. intention based by population. All these fields we can use, and it can be used for catholic, the classification, like we can use it for classifying, and then object detection, we can detect the objective, CNN, then semantic, segmentation, multicroject tracking, re identification, any mission transport. So, basically, any mission mission class, we can actually see, see. I not. And some of the real world, that's the medical image, we can use it for animal detection in scan, and autonomous vehicle, real-time environment prescription. So if there is any obstacle, they can detect and act accordingly, and case you are competition, or even your, you know, smartphone, or education, or even securities, surveillance in the airports, et cetera, and quality control, affecting actually manufacturers, which we have already discussed in our area classes. So these are the examples where we use machine missions. So wherever we use machine mission, we can use CNN. It's basically CNN. Yeah, uh, now we get into the boring part, like, you know, the metrics part. So do you know why we need matrix? Because we need to evaluate how our models behave, right? So it is a good, good system, or good model, or, you know, it's a bad model. We need to all In other words, we need to say, we need to decide if our network, if our model is mature enough to keep good results, right? Or accurate results. So for this, we will have different metrics. There's not any number of matrixes there. So we can ratify the models like classification model, regulation model, clustering model, etcetera. So, but in our course, we are focussing on the classification model, but what I am giving you, like we have different types of orders. The clustering model uses, metrics like this, average instance, to others that their average is just a cluster centre, maximum distance and blah, blah, blah. Okay, so these very ventrics they have. So, one thing to connect is that anything related to distance, it's for the frustrated distance. Okay, so any metric, which is related to the distance, like, you know, after any distance, to the other, to clusters, and there are massive distance, to clusters. Anything related to distance is, used to for, evaluating, clustering models. And we have another model for regression model where we use errors, like calculated errors to measure the purpose of that model. For example, me and Absolute, there are two-paste spider, a reality of salute, error, recent draw. So anything that relations with error will be for evaluating regression. Okay, but what we need to learn for our courses, this one, the classification model, and these are the metrics that we use, this one is adequacy, precision, raincall, efforts call, A, U, C, area under the curve. So we will see what does it mean by the, right? So we have accuracy, which means the proportion of total prediction. Total prediction means we can have like more positive prediction and negative prediction, right? If a person can be categorized as a, like, he can have diabetic or not. So he can have dynamicism positive, not his negative, right? So we can make both prediction, like a positive predictions for the negative prediction. So it is, as it means, is the proportion of the total prediction that the model got correct. So that is what ATOS is doing. And for precision, it asses the accuracy of the positive prediction made by a CNN. So you just look for the positive prediction. Okay, so there are smaller differences you can bring to weight. I, this is kind of like boring to explain again and again, but this is the type of, like, everything is the proportion of total British, both positive by negative, but for precision, we focus pain on the positive person. And we have rain call, effort score, um, our OC, which is receiver operating characteristics. It's a curve, which gave you an idea of how good your model is. I will show you the upcoming slide, how the term would look like. So the recall, while the CNN stability to correctly identify all actual staircases. So whether our model was able to identify all the positive cases. If what's got provided balance between precision and record by calculating their harmonic means. So there is an equation for communing, that's so that we will see them from this file. And ROC plots, true positive rate against force positive rate. Okay. And the area under that curve provide a signal value summarizing the over of performance of the CNN across all possible classification threshold. So this means we haven't heard something like this. So, In area under this curve or represent a value, which gives us an idea of how good your model is. Okay. Another metric is the confusion metrics. This is a tool, which is usually, maybe you already know about this. I would want to explain it again, right? So this is a calculation address, which is a tool used in machinery, and the intibal with the performance of the classification order. So, you know that in the left table, you have, like, several breedings and based on that, we make prediction of the person can have heart issue or not, right? So this represents the actual and this, this is a plot or like, you know, the veteran which shows the comparison of actual accuration. So we can have actually we can have no and yes, but the prediction also can have no and yes, right? So how many, you know, how many we got, uh, the actual knows and the predicted knowings, prone editing. So there are actually there, this, this was, you know, and the, they are model moded currently predicted us now. So 28,000, they've predicted correctly. So that is true legacy. And when you move to the next part, it shows, actually, it does no, but our models radically, it does. Yes. So because positive. And when you go down, it shows force everything. Actually, it was yes, but our model predicted it is, no, so that is what falls negative. And the last call, actually, it was, yes, and our model, what it correctly, predicted us. So that is true possibly. So based on this, we will have some equations. Okay, so this is how actress is computed, precision is computed, break all efforts for everything is computed. And this is the para signal that. Plots, true positive, right, against, force positive, right? So how do you say you have water is good by looking at this graph is that if you are, there actually is something like... So... This is too positive. And if your graph is something like this, and this is a diagram. If it is above this diagonal, which means, yeah, your model is doing what. If anything is below this, which means that... So it's just like a random answer. So this is the worst. So anything which goes above this diagonal, there's a spectrum order. So the best model will be, something that goes, like, you know, that's... The more it goes towards the top, yeah, it is, the more good here. So, what are the STQ consideration advice in CNN? So, we talked about CNN, like, what CNN is, how the architecture and how we can make use of CNN image classification and what are the performance metric and now? Another aspect is that what are the ethical consideration? So you know that we are dealing with the data, right? They can have a lot of, you know, personal data or private, like, you know, very high secure data. So we are dealing with a lot of data, so we should be like mindful about these factors. First one is privacy. Okay, so the privacy isolves arise, that's even more important, sensitive personal data. Now, sometimes we need to provide, like we need to train on our personal data, sensitive data, like your date of birth, or age, or like even the security number, or something like that. So, if you train a model like that, you should be mindful about the privacy, and surveillance, surveillance, we use CCTV surveillance, right? Which is, again, using information system, but if we are risking the, like, you know, privacy of others, right? Sometimes, even without the, you know, conserve, like coming or how it's called, consent of the people, like we have taking the video of them, right? So this is actually subveillance is also a privacy concern. And then wise in AIA is always like, you know, you are training with the data. So depending upon what data you are feeding the network, it will make the tradition. So never have a biased data set. So always have a diverse data set. Do not have a like, like, you know, bias towards a particular stereo type or particular political interest or something like that. So when you prepare a model, Be mindful about these things, the privacy, the surveillance, and the bias in AI. So whatever you are making should be good for the society, nothing that can have or mislead people. Okay, so these are the ethical considerations. Yeah, so these are the references are used, and next week, we will see how CNN training, processes that loss function, activation function, back propagation, policies. Thank you. Thank you. Getting down today's class. Okay. Okay. I mean, not a whole, whole laptop. Just a... Yeah, yeah, it should be minus one for each test.