So this week we will be covering the fundamentals of CNN classification. Last week itself, we did an overview showing the input layer, the convolutional layers, the pooling layers, the subsequent layers, and then finally the output layer. So continuing from there, we are going to cover data set preparation, data augmentation, how to design a CNN, the different activation functions that we may be using, loss functions, important algorithms like back propagation, computational resources, and common issues and troubleshooting.

So CNN is a subset of machine learning, and it uses neural networks. Last week, we saw the layers of CNN. So we can have multiple layers like convolutional layer, pooling layer, and so on for a particular number of layers. CNN can be used for tasks like object detection, image classification, and semantic segmentation. Do you know what semantic segmentation means? Semantic segmentation is a task in computer vision which classifies each pixel in the image to a category. For example, an easier way to say it is, if you ask a model to say if this image contains a cat or not, the model can say, yes, this image contains a cat. But for semantic segmentation, how it works is that it tells exactly which pixels belong to that cat. So not only does it tell if it is present or not, it can exactly delineate the pixels belonging to that image of the cat. That is why it is called semantic segmentation. So all these are machine learning models that can identify patterns and features present in the images.

So I hope you are familiar with this diagram now. We have one image, which is being fed into the input layer, and the input layer will convert the image data into numerical data. And this data is being passed to the next layer, which is the convolutional layer. That is the layer where the network is extracting the features. The reason we use convolutional layers is that we do not want every pixel in this image for training the model. We just want what is needed, what is special, or what is important from the image. We extract that in the convolutional layer, and the output of the convolutional layer is called a feature map. This feature map goes into the next layer, which is the pooling layer. In the pooling layer, what happens is it downsamples the dimensionality of that feature map. It does that depending on which type of pooling you use, and depending upon the size of the filter. From the pooling layer, the feature maps will be flattened, right? So by flattening, the output of the convolutional layers is multidimensional. We need to make it one dimensional so that it can be fed to the fully connected layer. That is where we do that.

In the fully connected layers, when the data is being passed on, we do some calculations and we compute the loss. Using multiple iterations of this, we will try to minimize the loss. Finally, in our output layer, the number of neurons equals the number of classes. So you can see here we have a few classes. For example, there is a sunset class, a dog class, and a cat class. And you can see a probability score for each class. Whichever class has the highest probability, that will be the prediction of that model. In this case, it should be the correct class. That should have the highest probability. Otherwise it is a wrong prediction. So this is how image classification works. We are categorizing and labelling images into their corresponding classes.

So now, data set preparation. This is one of the most important parts if you are doing image classification. We need to prepare our data because what happens is the more and better data we have, the more efficiently the model can perform. You can create a model, but it may not be accurate or the performance will not be that good. So we need to make sure that we prepare the data properly. The main area of preparing the data is that we need to make sure that a diverse set of images is present in our data. Otherwise, the model will be biased towards what you are providing it. So we need to have diversity. In this example, we can see we have different classes. When you train your models, we need to provide a diverse set of images so that the model will not be biased and can better handle any kind of variation in images because it has already seen that.

There is one technique which helps us, and that is annotation. Annotation means it is the process of labelling data. You can say this image is labelled as a certain class. So each image is labelled with its class label. This process is called annotation. And this is mainly used for supervised learning, because we know the labels. We know that the quality and accuracy of the labelled data directly impacts the model's ability to learn and identify patterns. That is why they say you need to train a model with a good data set which has a diverse set of images. For supervised learning, we can use techniques like annotation to label the images.

There are different types of pre-processing techniques for preparing the images. If your data has images of different sizes, the input is not consistent. CNN would have trouble extracting the features or performing on that kind of data. So if you make them consistent, we can prepare our data. For example, if images are of different sizes, we can resize them to a uniform size. Or you can normalize the pixel values into a specific range. If the intensity values are varying drastically, that is not ideal for the CNN. So we can make it consistent by normalizing the values. Also, for some techniques, we can convert color images to grayscale or other color spaces. While doing all these methods, we are making sure that we are providing consistent input data to the CNN. So by doing this, we can improve the CNN's performance. That is the reason why we give importance to the pre-processing step.

Do you know that the input data is divided into training, validation, and test sets? Do you know why? Why can we not just use the training data? What happens if we just have training data? We do not know if our model is good for anything because it can learn that data and just memorize the values.

So the standard split is roughly 80 percent of the data for training, 10 percent for validation, and 10 percent for testing. The majority of our input goes towards training the model, some percentage for validating, and some percentage for testing.

In the training set, this is where the model learns. We present 80 percent of the data to the model over and over again, and then it learns the patterns from that data. By passing the data multiple times, the model can update its parameters on each iteration.

What happens if we just have the training data? Because we are repeatedly feeding this data to the model, the model can memorize the values. So it can perform very well on any input from the training set. But if you give it data that it has never seen, or new data, it cannot perform well because it just memorized the training data.

So validation is necessary. The validation set is used for tuning the hyperparameters. This is where we tune things like the learning rate, the number of layers, and all those things. It also helps us decide if we should stop early, because we can compare the training data and validation data. If the accuracy of the model is high for training but low for validation, that is a sign of trouble, because there is no point in continuing to train since it would not perform better. We should have the validation accuracy comparable to or higher than the training accuracy, because validation data is like real world data that we are giving to the model, and it should be able to perform well on that. That is the reason why validation is important. This is also where we can detect overfitting. As I mentioned, overfitting means the model works very well on the training data but cannot perform well on unseen data.

The test set is different from validation because validation is done multiple times during training. The test set is where you actually evaluate the final performance of your model. This data is purely unseen by the model. Depending on how your model performs on this data, you can assess its quality. That is the reason why we need test data. This is the ideal way, but usually at minimum we should have training and test sets, though it is recommended to have all three.

Why is more data important? As I mentioned, the model has to be exposed to as much diverse data as possible. We need to train it with a diverse set of data, then only our model can better deal with variability. But sometimes we do not have enough data. So there is a way to augment the images. From just one single image, you can generate 10 or more images by using some transformation techniques like rotations, scaling, flipping, cropping, and so on. For example, rotation plus 45 degrees, minus 45 degrees. By applying these techniques, you will get multiple images from a single image. So you are actually expanding the data set. If you have a small data set, you can expand it by using augmentation techniques. This process helps in reducing overfitting because we are already exposing the model to a wide variety of features. So the model can better deal with generalization. We are training it with different features and different scenarios that it could encounter. So augmentation helps you expand your data set.

Deciding the CNN architecture. In most cases, we are not designing it from scratch. Usually, we take a state of the art model. Models for image classification or other tasks are already available. But in case you are designing one from scratch, you need to consider the architecture. You have to think about the number of layers, type of layers, type of convolution, and their parameters like filter size, stride, and activation function. You remember stride, right? The step size with which the filter is moving.

All these are key points that we consider when deciding the CNN architecture. There is no perfect formula. I cannot say for your model you should use a specific number of layers. No one can tell you a perfect formula. But there are guidelines. For smaller tasks, you can start from 5 to 10 layers. For more complex tasks, you can have 50 to 200 or even 500 layers, depending upon how complex your data is, whether you have image data or other types. Depending on how complex your application is, you can decide how many layers and what type. Sometimes we just need one convolutional layer and one fully connected layer for a simple task. But sometimes that is not enough, so we need to have multiple convolutional layers and pooling layers. Another thing is that typically the number of filters increases as you go deeper. For example, if we have multiple convolutional layers, the first layer might have 16 filters, then 32, then 64, and so on. In the first few layers, we are extracting simpler features. The deeper we go, the more complex features will be extracted, and that requires more filters. So the number of filters in each layer may increase as you go deeper into the network. That is a key point.

The architecture should match the complexity of the task. For more complex tasks, you need more layers and computational efficiency considerations. If your model is complex and you choose a very simple architecture, it will not be computationally sufficient. Your model cannot handle that. So you need the computational power and layers to deal with that kind of task. You need to keep all these considerations in mind: the number of layers, types of layers, their parameters. There is no perfect formula, but depending upon the requirements, we can decide. It is always trial and error.

We talked about activation functions last week. The activation function is a function that introduces non-linearity. In a fully connected layer, there is a connection between all the neurons. We have input data that is being transmitted through these connections. But sometimes we do not need all the neurons to transmit to the next layer because they carry unneeded information. Maybe it is the background, which is not needed for further processing, so we can ignore it. That is what an activation function does. It decides that some neurons are not needed, so it can zero out or reduce the output of those neurons. The activation function, depending upon the output, decides whether to pass the information forward or not.

Another point is that the activation function introduces non-linearity. As I mentioned last week, if our model is linear, it is just like a simple linear equation. A linear model cannot learn complex patterns like images or similar data. In that case, we need to introduce some non-linearity to our network, and for this, we can use activation functions.

In neurons, we have inputs with weights. Each input is multiplied by its weight, and we add a bias. This weighted sum plus bias is then passed to the activation function.

There are different types of activation functions. Sigmoid is one of the popular activation functions, which is used for binary classification, and it always ranges between 0 and 1. Towards the edges, it flattens out and becomes stable. This leads to the vanishing gradient problem, but we still use sigmoid for binary classification.

Softmax is similar, ranging between 0 and 1, and it is mainly used for multi-class classification. If you have multiple classes, you can use softmax to assign a probability to each class, and whichever class has the highest probability, you can assign that classification. So softmax is mainly used for multi-class classification.

Tanh has a wider range compared to sigmoid. Instead of 0 to 1, it ranges from minus 1 to plus 1. So it is more balanced because it includes negative values as well. It is a differentiable activation function, and it also has the vanishing gradient problem because towards the edges, it goes flat and stabilizes, similar to sigmoid. It is useful when dealing with negative input values.

ReLU is very popular. It is a rectified linear unit. It takes only values that are greater than zero, and for all other values, the output is zero. So if a neuron's output is negative, it just keeps it at zero. ReLU ranges from zero to infinity. It takes away the negative values while keeping positive values. It is computationally efficient and is commonly used in hidden layers and feed-forward networks.

Now, the loss function. The loss function measures how well the model's predictions match the true outcomes. So we have the expected output, but what we actually get from the model is different from that. We take the difference between the two. If the loss is high, it means the difference is large, and the model is not matching well. If the loss is low, it means it closely matches or is giving a good prediction. So we find the difference between the prediction and the actual value. This is also called the cost function. Based on this, we can use back propagation to adjust the weights and bias parameters. Our goal is to get the minimum loss. For that, we use loss functions. Common loss functions include mean squared error and cross entropy loss. Cross entropy comes in two types: binary cross entropy for binary classification, and categorical cross entropy for multi-class classification.

Our goal is to reduce the loss. We compute the loss for each iteration, and we continue until we get the minimum loss.

Gradient descent is an optimization algorithm that we use to minimize the loss. How we do this is, when we calculate the gradient at a current position, if it is pointing towards the left, we move it towards the right direction, and vice versa. That is how gradient descent works. Depending upon the gradient at the current position, we can compute how to move towards the optimal position. Our goal is to reach the minimum. When the model is trained, our goal is to have the minimum loss. When we compute the loss, if it is on one side, it should move towards the other side to reach the minimum point.

This brings us to back propagation, which is very important. Back propagation is the algorithm that helps you adjust the weights and parameters so you can optimize your network to give the minimum loss. All the functions which I mentioned earlier, like the loss function and gradient descent, are all used in back propagation. At a higher level, all these things are happening in back propagation. It will help us to minimize the loss.

For a model, what happens is that we present the input to the model, we pass the input through different layers, like the convolutional layer, pooling layer, and fully connected layer. While the data is being passed through this model, we do some calculations, and in our output layer, we get the loss. Since we have the loss or error, our goal is to minimize it. For that, we use back propagation. We propagate that error back from the output layer to the layer before it, then to the layer before that. And we adjust the weights and biases. If the prediction is still not correct, then again, we propagate this error back, and we adjust the weights again. This is the idea of back propagation. It happens only during training, not during the validation or testing phases. During testing, we are just passing the input to see how the model predicts. We are not training or adjusting any weights. So back propagation happens only during training.

The basic steps of back propagation are: first, we feed the sample to the network and calculate the error. For supervised learning, we know what the expected output should be. For example, if it belongs to the class cat, we expect that output. From the expected output, we compare it with the actual output. That is how we calculate the mean squared error. Then we compute the error term for each output neuron, which represents how much each neuron is contributing to the error. Then we propagate this error to the previous layers. While propagating, we apply a delta rule and adjust the weights. Then we use feed forward again, and the process repeats. Feed forward means we are taking the input, passing it through the inner layers, and finally getting the output. That is called forward propagation. In the back propagation part, we take the error, compute the error term for each neuron, and hierarchically propagate the error through the hidden layers. We adjust the weights and do it again until we get the minimum loss.

So let us say this is a binary classification. In binary classification, we have just two outputs. Either it should be a 0 or 1. 1 means it belongs to that class, 0 means it does not belong to that class. Let us say we have input values, and we have weights for the connections between layers, and a bias. What we do is we take each input, multiply it with its weight, add the bias, and apply the activation function. Finally, in the output layer, we can see only 2 neurons because the number of neurons in the output layer equals the number of classes. In binary classification, we have just 2 classes, so we have 2 neurons in the output layer. Let us say we got output values of 0.2 and 0.49. This is not what we expected, but that is what we got in the first iteration. So we need to calculate the error. We take the difference between expected and actual output.

The second step is calculating the mean squared error. The equation for computing the mean squared error is: the sum of (Yi minus Oi) squared. We have the expected value, which is one, and the actual output value, and we compute the error. So finally, the mean squared error comes out to something like 0.042601. That is the error for this output layer. We compute the error term for each neuron in that layer.

The third step is computing the output error terms. For each neuron in the output, we compute the error term using the derivative of the activation function. After applying this equation, we get the output error term for the output layer.

The fourth step is calculating the hidden layer error terms. Once we have the output layer error, we propagate it back to the hidden layers. We can see the connections between the layers, and we propagate the error from layer to layer. The main idea is that whatever error we have in the output layer, we are propagating it back through the layers. That is why we call it back propagation.

After computing all the error terms for each neuron in the network, we apply the delta rule. The delta value represents how much each weight needs to be adjusted. We compute the delta W for each connection in the network. Once we get the delta value, we apply it to each weight. That is how we adjust the weights.

After adjusting the weights, we again do the same process, the forward pass, and compute the error again. This is the idea behind back propagation. Basically, the input is being fed to the network, we compute the mean squared error, compute the error term for each output neuron, propagate the error to the hidden layers, apply the delta rule, and finally adjust the weights. After adjusting the weights, you run it again to make sure the model is improving. That is how we do the training. This is just one iteration. We need to do many iterations until we get the minimum loss.

Common optimizers include SGD (Stochastic Gradient Descent), which is a classic optimizer that is simple to implement, and Adam, which is known for adaptive learning rates and works well for different problems. Adam is very popular and is a recommended one. RMSProp is another optimizer. The choice of optimizer affects the speed and quality of training because if you select the right optimizer, the training will be faster and more efficient. It is another factor to consider when you are designing your CNN. Most commonly, SGD and Adam are used. Adam is mostly recommended.

Best practices for the training process. As I mentioned, it is best to have training, validation, and test sets. It is not mandatory, but it is the best practice. As I mentioned, we should have high accuracy for validation data compared to the accuracy of the training data. We should always have low loss for validation data.

Each iteration in training consists of a forward pass, where data flows through the network from input to output, and then back propagation, which is used for calculating the gradients. Then, depending upon the optimizer we use, we adjust the weights. Writing a CNN involves: forward propagation, where you get the prediction; computing the loss; doing back propagation to calculate the gradients; and finally using optimizers to adjust the weights.

Best practices include using a validation set. Most of the time, people tend to use training and test only, but the best practice is to use validation. It helps us to tune the hyperparameters, which are parameters that can be set or initialized, like the learning rate, the number of layers, and so on. These are hyperparameters that we initialize at the beginning before training. During training, some algorithms can adjust them. Weights and biases are learnable parameters that are learned during training.

Apply early stopping to prevent overfitting. If you train too long, the model becomes too fitted to the training data. How we decide to stop training is by monitoring the accuracy. If we see that the accuracy of the validation set is less than training, it is an indication that we should stop training.

Another best practice is to periodically save your model state. You can save the model state for recovery. Things can happen at any time, so you do not have to start the training from scratch if something goes wrong. You can save the model state so that if some interruption happens, it will be saved, and when it resumes, you can continue from where you left off.

Another important practice is monitoring the training process by tracking loss and accuracy, both on training and validation data. This helps in understanding model performance and making adjustments. If we see that the model accuracy is too good on training but keeps increasing in loss for the validation, which means it is not going in a good direction, we can stop it. But if the accuracy is good for both validation and training, it means the model is performing well.

Understanding overfitting. Overfitting is when the model becomes too good at learning the training data, including its noise. It can even learn the noise from the data, and it shows poor performance on new or unseen data like the test data. It happens usually with complex models that have too many parameters. For example, if you look at a model loss graph, the loss for the training data keeps decreasing, but the loss for the validation data starts going up. That means it is not going in a good direction, and that is a sign of overfitting because the training loss is getting minimized but the validation loss is not. The symptoms of overfitting include much higher accuracy on the training data compared to validation data. If the accuracy is too high on the training data compared to validation, the model is overfitting.

We can use some of these techniques to prevent overfitting. One is to use dropout layers. As I mentioned, not all the neurons are needed. We can randomly choose some of the neurons or layers to deactivate. In one iteration, you can drop one set, and in the next iteration, you can drop a different set. It is done randomly. The benefit is that the model can learn all the variations and complexity from the input. It is not just training on the same neurons. Sometimes it is missing some of the neurons, so it learns different patterns. By deactivating random neurons during training, it can prevent the co-adaptation of features. Otherwise, the model will memorize the patterns. By dropping something randomly, we are not giving a chance to the model to memorize everything.

There are also regularization methods that can help reduce overfitting. Another way is to expand your data set by augmentation. As I mentioned, if you have a small data set, you can expand it by applying some transformations like rotation, shearing, flipping, zooming, and so on. You can also expand the data set to provide more varied training examples. By doing so, you are training the model with different variations and different scenarios. Simplify the model by reducing the number of layers. Depending upon the complexity of your model, you might be able to reduce the number of layers. It is better to use as few as possible. Apply early stopping to halt training when performance on the validation set starts to degrade. These are standard techniques that help reduce overfitting.

Hardware resources for deep learning. We have different options starting from CPU, GPU, and TPU. For smaller operations, we can use CPUs, but for medium to larger tasks, you should use a GPU. Deep learning requires significant computational resources. CPUs can handle basic operations but are slower for deep learning tasks. GPUs, with thousands of cores, are ideal for the parallel processing that deep learning requires. For our class assignments, we can use CPU, but in your projects, you can introduce GPU because if you are training larger models, it is more computationally demanding, and you will need more computational power.

TPU stands for Tensor Processing Unit. It is designed specifically for neural network operations. Neural networks deal with matrix operations, and depending upon the size of the data, it can be very computationally intensive, so TPUs can help. TPUs provide faster computation. They are more powerful than GPUs for certain workloads. A student asked what a TPU is and whether it fits in a computer. TPUs are available as cloud services, for example, on Amazon you can rent servers that have TPUs. CPUs and GPUs are available locally on your machine, and TPUs are also available through cloud platforms. TPU is the most powerful option. Depending upon how complex your application is, you can choose the right hardware. The choice of hardware can significantly impact the training time. If you are using more powerful hardware, you can finish the training faster. Some training can take overnight or even 2 days, depending upon the complexity of the model.

Sometimes we cannot use very high-end processors due to resource constraints. So there are techniques we can use to reduce the computational complexity of the model. You can even use a GPU instead of a TPU if you apply these optimization techniques. One technique is pruning. In the network, we have some redundant neurons. We do not need redundant neurons, so we can remove them. By doing so, you are reducing the complexity, perhaps by half or more, depending upon how much redundancy there is. There is a linked resource for more details on pruning and quantization that you can refer to for a deeper understanding.

Another technique is quantization, which reduces the precision of the numbers. We are dealing with numbers in neural networks. Images have pixels with values, and all the computations are mathematical operations. The precision of the numbers matters. We can reduce the precision without significantly affecting accuracy.

Another approach is to use efficient architectures. There are pre-tested, well-known architectures that are state of the art. These architectures are already optimized with the right number of layers, parameters, and configurations. They take care of a lot of computational optimization for us.

For example, mobile and edge devices need efficient models. For applications running on devices like phones, we cannot accept heavy computational loads. Everyone wants responses to be fast. So we can use existing efficient architectures that help us achieve the task faster.

One of the key challenges in computer vision is that we deal with images and videos, which require a lot of computational capacity and resources. Research is being carried out to minimize the use of resources while providing faster and more accurate results.

The future of CNN will involve integration with other deep learning techniques. As I mentioned, deep learning is a broader field, and we have techniques like natural language processing and RNN (Recurrent Neural Network). We can combine CNN with these models to create new models that provide more capabilities. For example, if you combine CNN with recurrent neural networks, you can perform video classification because RNN is popular for sequential data like video. For natural language processing, since it is good at language processing, it can understand captions and text in images or media. By combining different techniques, you can create more sophisticated applications. If you use RNN with CNN, you can perform video classification and image captioning. These integrations enable multimodal learning. The main idea is that CNN is good at processing visual data, and NLP can handle the language part. When we combine them, we get better applications. As the field of AI is evolving, CNN's scope is also growing because we can integrate different technologies.

Troubleshooting common issues. One of the major issues is overfitting, where the model performs too well on the training data but cannot generalize to unseen data. Another issue is underfitting, where the model cannot learn anything meaningful from the data. If the model is biased towards a particular class, that is also a problem.

Strategies for troubleshooting include adjusting the learning rate. The learning rate is a hyperparameter that you can set initially and then adjust during training. Modifying the network architecture is another strategy. If you are using your own architecture, you can replace it with a state of the art architecture, or you can make adjustments to the number of layers or parameters. Batch normalization and dropout can also help. Dropout deactivates some of the neurons in the layers. After applying these techniques, we check performance metrics to see if things are improving.

We need to make sure that we use a diverse data set for training and regularly monitor the performance of the model. If we see that the model performs inconsistently on the validation set or the test set, that is an indication that something needs to change.

Underfitting is the opposite of overfitting. It means the model cannot learn anything meaningful from the data. For overfitting, you reduce the training or simplify the model. But for underfitting, you need to increase the model complexity by adding more layers, because the current architecture is not good enough to learn the patterns in the data. You can also provide more diverse data so that the network can learn. Train for a longer duration, or use more powerful and diverse feature extraction techniques. Re-examine the data pre-processing steps, such as annotation, normalization, and resizing. Whatever pre-processing was done, maybe it needs improvement. These are some of the key points to keep in mind if your model is showing underfitting. Try changing the architecture by adding more layers, providing more diverse data, or adjusting the pre-processing techniques.
