# Reinforcement Learning Lecture Notes: Gymnasium Environments, Custom Environments, and Stable Baselines 3

## 1. The Role of Simulators in Reinforcement Learning

For trivial environments (grid worlds, toy problems), we can code the dynamics directly. For non-trivial domains — robotics, autonomous driving, complex games — we need **simulators**. A simulator provides a virtual version of the real environment where an agent can safely explore, fail, and learn without real-world consequences.

Not every domain has a good simulator. For example, there is no reliable simulator for the stock market. But for robotics platforms like the iRobot Create 3, a Gazebo-based simulator can model a house environment where the simulated robot subscribes to ROS 2 topics (e.g., `cmd_vel`) and behaves just like the physical robot. This allows RL training entirely in simulation before deploying to hardware.

---

## 2. Gymnasium (formerly OpenAI Gym)

### 2.1 What Is Gymnasium?

Gymnasium is the standard Python library for defining and interacting with RL environments. It is the successor to OpenAI Gym and provides a unified interface that allows any RL algorithm to work with any compatible environment.

### 2.2 Migration from Gym to Gymnasium

The standard migration trick:

```python
import gymnasium as gym
from gymnasium import spaces
```

After this import, all subsequent code that references `gym` will actually use `gymnasium`. However, **during the import block itself**, you must still write `from gymnasium import spaces` — writing `from gym import spaces` would incorrectly pull in the old library.

```python
# CORRECT
import gymnasium as gym
from gymnasium import spaces   # must use 'gymnasium' here, not 'gym'

# After imports, 'gym' now refers to gymnasium
env = gym.make("CliffWalking-v0")  # this works
```

### 2.3 Key Changes from Gym → Gymnasium

| Change | Details |
|---|---|
| **`reset()` takes a `seed` parameter** | Enables reproducibility of pseudo-random experiments. The same seed produces the same sequence of "random" events. |
| **`step()` returns 5 values instead of 4** | Returns `(observation, reward, terminated, truncated, info)`. The new fifth value is `truncated`. |

### 2.4 Terminated vs. Truncated

- **Terminated** — The episode reached its natural end condition (e.g., the agent reached the goal state).
- **Truncated** — The episode ended early for practical reasons before the natural end condition was met.

**Example — Race car on a track:** The natural episode end is crossing the finish line (`terminated = True`). But if the car veers off the track with no way to recover, the episode is cut short (`truncated = True`). The agent still learned something from the partial episode.

**Example — Cliff-walking (Sutton & Barto, p. 132):** Falling off the cliff does **not** end the episode. The agent is sent back to the start position, and the episode continues. The episode only terminates when the agent reaches the goal state. Truncation is not used here.

### 2.5 Reproducibility and Seeds

All randomness in computing is pseudo-random — generated by deterministic algorithms from a starting seed value. Given the same seed, the same "random" numbers are always produced.

This matters because **even just changing the seed — without touching any hyperparameters — can significantly change experimental outcomes**. Two identical training runs with different seeds may produce very different performance. This means:

- Always record your seed so results can be reproduced.
- When comparing algorithms or hyperparameters, be aware that variation may come from the seed rather than from the change you made.
- True randomness (if it exists at all) is a deep philosophical question — quantum randomness is the closest we get, but everything in computation is pseudo-random.

---

## 3. The Gymnasium Environment Interface

Every Gymnasium environment implements a standard set of methods, functioning like an **interface** (similar to a Java interface — a defined list of methods that any implementation must provide):

| Method | Purpose |
|---|---|
| `__init__(self, ...)` | **Constructor** — initializes the environment, sets up render mode, defines observation and action spaces. |
| `reset(self, seed=None)` | **Reset** — returns the environment to an initial state. May be a fixed start state or a randomly chosen one. Returns the initial observation. |
| `step(self, action)` | **Step** — executes the given action on the environment. Returns `(observation, reward, terminated, truncated, info)`. This is the most important method — it is where the agent interacts with the environment. |
| `render(self)` | **Render** — produces a human-viewable representation of the current state (often via Pygame). |
| `close(self)` | **Close** — cleans up resources (shuts down sub-processes, closes display windows, etc.). |

These methods define the complete contract between any agent and any environment. Any algorithm that knows how to call `reset()` and `step()` can work with any environment that implements them.

---

## 4. The Agent–Environment Interaction Loop

The fundamental RL loop in the Gymnasium framework:

```
1. Agent creates environment       →  env = gym.make("EnvName-v0")
2. Agent queries spaces            →  n_states = env.observation_space.n
                                      n_actions = env.action_space.n
3. Agent builds Q-table            →  Q = np.zeros((n_states, n_actions))
4. Agent resets environment        →  obs, info = env.reset()
5. Loop (within episode):
     Agent selects action          →  action = agent.choose(obs)
     Agent steps environment       →  obs, reward, terminated, truncated, info = env.step(action)
     Agent updates Q-table         →  agent.learn(obs, action, reward, ...)
     If terminated or truncated    →  obs, info = env.reset(); begin new episode
```

### 4.1 What the Agent Needs to Know About the Environment

Before learning can begin, a Q-learning agent must determine two things from the environment:

1. **How many states** are there? → Determines the number of rows in the Q-table.
2. **How many actions** are there? → Determines the number of columns in the Q-table.

The Q-table has shape `(number_of_states, number_of_actions)`. For example:

| Environment | States | Actions | Q-Table Shape |
|---|---|---|---|
| Cliff-walking | 48 (states 0–47) | 4 (up, down, left, right) | 48 × 4 |
| Taxi | Varies | Varies | Depends on env |
| Blocks World (3-digit) | ~120 configurations | ~90 move actions | 120 × 90 |
| Blocks World (6-digit) | ~14,400 (120 × 120) | ~90 move actions | 14,400 × 90 |

The agent does not initially know what the actions *do*. It just knows there are actions numbered 0, 1, 2, ... and states numbered 0, 1, 2, .... Through repeated interaction, the Q-table learns the value of each action in each state.

### 4.2 Environment Creation

In early hand-rolled code, the agent directly instantiates the environment class. With Gymnasium, the standard approach is:

```python
# Hand-rolled (without Gymnasium)
env = CliffWalkingEnvironment()

# Gymnasium
env = gym.make("CliffWalking-v0", render_mode="human")
```

The variable `env` refers to nothing until the environment object is actually created. The *coding* of the environment is the programmer's job. The agent doesn't write the environment — it only interacts with the finished product.

---

## 5. Observation Spaces and Action Spaces

Spaces are Gymnasium's way of formally describing what observations look like and what actions are available. They are defined in `gymnasium.spaces`.

### 5.1 Fundamental Space Types

| Space | Description | Example |
|---|---|---|
| **`Discrete(n)`** | A set of `n` integers: `{0, 1, ..., n-1}` | 4 movement actions → `Discrete(4)` |
| **`Box(low, high, shape, dtype)`** | Bounded numerical values (continuous or integer) in a multi-dimensional box | Agent `(x, y)` position with values 0–4 → `Box(0, 4, shape=(2,), dtype=int)` |
| **`MultiBinary(n)`** | Binary array of a given shape | Feature presence/absence vectors |
| **`MultiDiscrete([n1, n2, ...])`** | Multiple independent discrete variables | — |
| **`Dict({key: space, ...})`** | Dictionary of named sub-spaces | Separate entries for agent position and target position |

Every space supports `.sample()` to draw a random valid element, which is useful for exploration and testing.

### 5.2 Example: 5×5 Grid World with Dict Observation Space

A grid world environment may use a `Dict` observation space:

```python
observation_space = spaces.Dict({
    "agent":  spaces.Box(0, size - 1, shape=(2,), dtype=int),
    "target": spaces.Box(0, size - 1, shape=(2,), dtype=int),
})
action_space = spaces.Discrete(4)
```

Here, the observation is a dictionary with two entries. Each entry is a `Box` containing two integers (x, y coordinates). The `Box` specifies that values range from 0 to `size - 1` (i.e., 0 to 4 for a 5×5 grid), there are 2 values per entry (the `shape=(2,)`), and they are integers.

So if the agent is at position (1, 0) and the target is at (4, 4), the observation would be:
```python
{"agent": array([1, 0]), "target": array([4, 4])}
```

### 5.3 Adapting Spaces for Different Environments

When converting a 5×5 grid world to a 12×4 cliff-walking grid, the observation space must change:

- The x-coordinate now ranges from 0 to 11 (12 columns).
- The y-coordinate now ranges from 0 to 3 (4 rows).
- The target entry can be removed if the goal is fixed and not part of the observation.

This kind of adaptation — understanding what the spaces mean and adjusting them for a new environment — is a core skill when building custom Gymnasium environments.

### 5.4 Querying Spaces from the Agent

```python
env = gym.make("MyEnv-v0")
n_states  = env.observation_space.n   # for Discrete observation spaces
n_actions = env.action_space.n        # for Discrete action spaces
```

The `observation_space` and `action_space` are attributes of the environment. The `Discrete` space has an `.n` attribute giving the count of elements. The agent reads these after creating the environment to build its Q-table with the correct dimensions.

---

## 6. Agents, Algorithms, and the Mix-and-Match Principle

A central principle of Gymnasium-based RL:

> **Agents are implementations of algorithms, and agents can be mixed and matched with environments.**

A Q-learning agent can do Q-learning on cliff-walking, on the taxi problem, or on Blocks World. A DQN agent can be pointed at any of these environments. The agent doesn't need to know the specific semantics of the environment — it only needs to know the number of states and actions.

This decoupling is what makes Gymnasium powerful: environments expose a standard interface, and any algorithm that speaks that interface can learn on any environment.

---

## 7. Stable Baselines 3 (SB3)

### 7.1 What Is Stable Baselines 3?

Stable Baselines 3 is a library of **standard, well-tested reinforcement learning algorithms** implemented in a clean, verifiable way and packaged for easy use with Gymnasium environments. The "baseline" in the name refers to the idea that these are the foundational algorithms you'll likely use in practice.

### 7.2 Key Algorithms

| Algorithm | Type | Notes |
|---|---|---|
| **DQN** (Deep Q-Network) | Value-based (neural network) | Uses a neural network to approximate the Q-function. Famously used to play Atari games. |
| **PPO** (Proximal Policy Optimization) | Policy gradient | A popular, robust policy optimization method. |

### 7.3 Basic Usage Pattern

```python
from stable_baselines3 import DQN, PPO

# Create environment
env = gym.make("MyEnv-v0")

# Create and train model
model = DQN("MlpPolicy", env)
model.learn(total_timesteps=10000)

# Save trained model to file
model.save("dqn_model")

# Later: reload and use
del model
model = DQN.load("dqn_model")

# Prediction loop (exploitation phase)
obs, info = env.reset()
while True:
    action, _ = model.predict(obs)
    obs, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        obs, info = env.reset()
```

There are two distinct phases: **training** (`model.learn(...)`) where the agent explores and updates its internal parameters, and **exploitation** (`model.predict(...)`) where the trained agent selects actions based on what it learned. The ability to save and reload models means training only needs to happen once.

### 7.4 Policy Types

| Policy String | When to Use |
|---|---|
| `"MultiInputPolicy"` | When the observation space is a `Dict` of multiple sub-spaces (e.g., separate agent and target observations). |
| `"MlpPolicy"` | When the observation space is flat — a single `Discrete` or `Box` space. MLP stands for **Multi-Layer Perceptron**, a type of feedforward neural network (deep learning). |

The choice depends entirely on how the environment's observation space is structured.

### 7.5 Why DQN and PPO Struggle with Simple Integer States

Neural network–based algorithms (DQN, PPO) expect observations in a specific form. They want:

- **Normalized vectors**
- **High-dimensional inputs** like RGB pixel arrays (images of the game screen)

Feeding them a single integer state ID (e.g., state 0 through state 47) is not what they're designed for. They will technically run but will perform very poorly — the agent may get stuck immediately and never improve.

**Q-learning**, by contrast, is perfectly happy with integer state IDs because it maintains an explicit table indexed by state number. No neural network is involved.

Making DQN and PPO perform well requires **redesigning the observation representation** — for example, providing a rendered image of the environment state instead of a single number. This observation engineering is a topic addressed later when studying how neural network–based algorithms process their inputs.

---

## 8. Blocks World as an RL Environment

### 8.1 The Domain

Blocks World is a classic AI planning domain. Three blocks (A, B, C) exist on a table with four positions. Blocks can be on the table or stacked on other blocks. The goal is to rearrange blocks from a starting configuration to a target configuration.

### 8.2 State Representation

Each configuration is encoded as a **3-character string** indicating where each block currently rests (on another block or on a table position):

> **Example:** A is on C, B is on position 2, C is on position 4 → state string `"C24"`

The three characters represent (where A is, where B is, where C is). Each character is either a block name (A, B, C) or a table position (1, 2, 3, 4).

**Constraints on valid configurations:**
- A block cannot be on itself (A ≠ position of A, etc.).
- Two blocks cannot occupy the same location.
- At least one block must be directly on the table (the configuration must be "grounded" — no cycles where every block is on another block).

Under these constraints, there are approximately **120 valid configurations**.

### 8.3 Actions

Actions take the form `move(Block, From, To)` — move a block from one location to another. Not all actions are valid in all states (e.g., you can't move a block that has another block on top of it). There are approximately **90 possible actions** when all combinations are enumerated.

### 8.4 Encoding States as Integers for Q-Learning

Q-learning requires numeric state and action indices. A dictionary maps between string representations (like `"C24"`) and integer IDs (like state 57). The reverse mapping allows converting back from numbers to meaningful configurations.

```python
observation_space = spaces.Discrete(120)         # ~120 valid configurations
action_space      = spaces.Discrete(90)           # ~90 possible actions
```

### 8.5 Two Approaches to State Design

| Approach | State Content | State Count | Behavior |
|---|---|---|---|
| **3-digit state** | Current block configuration only | ~120 | Agent does not know the target. It performs actions and accidentally discovers the target configuration. |
| **6-digit state** | Current configuration + target configuration | ~120 × 120 ≈ 14,400 | Agent knows both where it is and where it needs to go. It can learn directed policies toward specific goals. |

The 6-digit version has a much larger state space (14,400 × 90 Q-table), so training takes significantly longer. This illustrates a fundamental trade-off: richer observations enable smarter behavior but increase the learning problem's size.

### 8.6 Model-Based vs. Model-Free Distinction

The Blocks World environment is **based on** a complete model of block dynamics (implemented in Prolog). This model determines what happens when an action is taken. In principle, we could give the agent access to this model, making it a **model-based agent** that could simply plan a path from current state to goal and execute it step by step — no learning needed.

But we deliberately **withhold the model** from the agent. The agent must learn purely from trial and error, making it a **model-free agent**. This is the whole point of applying RL here.

This connects to the taxonomy of RL agents:

| Agent Type | Description |
|---|---|
| **Value function only, no explicit policy** | Policy is derived implicitly from the value function (e.g., Q-learning: pick the action with the highest Q-value). |
| **Policy only, no value function** | The agent directly learns a mapping from states to actions. |
| **Both value function and policy** | Some algorithms maintain both (e.g., actor-critic methods). |
| **Model-based** | The agent has an internal model of the environment's dynamics and can plan ahead. |

---

## 9. Prolog Integration for Environment Dynamics

### 9.1 Why Prolog?

Prolog excels at combinatorial enumeration and logical constraint satisfaction. Generating all valid block configurations — with constraints like no self-stacking, no duplicate positions, and groundedness — is concise and elegant in Prolog but would be significantly more complex in Python.

### 9.2 Code as Data

Prolog (and Lisp) follow a paradigm called **code as data** — there is no distinction between program code and data structures. This enables **runtime program modification** using `assert` (add a new fact/rule to the knowledge base) and `retract` (remove a fact/rule). While these operations are non-logical (they introduce side effects), they are essential for simulating state changes.

### 9.3 Key Prolog Predicates for the Gymnasium Environment

| Predicate | Purpose |
|---|---|
| **`reset`** | Retracts all current block positions and asserts the standard initial state. Uses `retract_all` and `assert`. |
| **`enumerate_states`** | Generates all valid 3-digit configuration strings by finding all combinations of (where A is, where B is, where C is) subject to constraints. |
| **`enumerate_actions`** | Lists all `move(A, B, C)` actions where the first argument is a block, the other two are blocks or places, and all three are distinct. |
| **`step(Action)`** | Takes an action of the form `move(Block, From, To)`, checks if it's possible, computes the resulting positions of all blocks, retracts the current state, and asserts the new state. |

### 9.4 Enumerating Valid States in Prolog

The state enumeration predicate works as follows:

1. Let `A` = location of block A, `B` = location of block B, `C` = location of block C.
2. Each of A, B, C is either a block name or a table position.
3. **No self-stacking:** A ≠ "A", B ≠ "B", C ≠ "C" (a block can't be on itself).
4. **No duplicate positions:** A ≠ B, B ≠ C, A ≠ C (two blocks can't be in the same place).
5. **Grounded:** The configuration has no cycles — at least one block must be directly on a table position. A configuration like "A on B, B on C, C on A" is invalid because nothing is on the table.

The `grounded(A, B, C)` predicate checks that the configuration is legal (no cycles) and that at least one of the three locations is a table position rather than another block.

### 9.5 The Step Predicate

```
step(move(Block, From, To)):
    1. Check that the move is possible (Block is clear, From is where Block currently is, To is clear)
    2. Compute where A, B, C will be after the move
    3. retract(current positions)
    4. assert(new positions)
```

This is "find where they would go, then put them there."

### 9.6 Python–Prolog Communication

The SWI-Prolog library (`pyswip`) allows Python to call Prolog predicates as if they were Python functions. This means the Gymnasium environment's `step()` method can call the Prolog `step` predicate internally, and the `close()` method shuts down the Prolog interpreter to prevent orphaned background processes.

---

## 10. Building a Custom Gymnasium Environment

### 10.1 General Approach

The standard method for creating a custom environment is:

1. **Copy an existing simple environment** (e.g., a 5×5 grid world).
2. **Modify it** to match your domain — change the observation space, action space, reward structure, step logic, reset logic, and rendering.

### 10.2 Package Structure

A custom Gymnasium environment is packaged as a Python package:

```
package_name/
├── pyproject.toml
└── package_name/
    └── envs/
        └── my_env.py
```

Once registered, the environment can be instantiated with:

```python
env = gym.make("MyEnv-v0", render_mode="human")
```

### 10.3 What Must Be Defined

In the environment class:

- **`observation_space`** — a `gymnasium.spaces` object describing valid observations.
- **`action_space`** — a `gymnasium.spaces` object describing valid actions.
- **`__init__`** — set up spaces, render mode, internal state.
- **`reset`** — return to initial state, return initial observation.
- **`step`** — execute action, return (obs, reward, terminated, truncated, info).
- **`render`** — display current state (e.g., via Pygame).
- **`close`** — clean up resources.

---

## 11. Connecting Agents to Environments — Coordination Points

When an agent is paired with a new environment, the critical coordination points are:

1. **Observation space type and size** — determines Q-table rows (for tabular methods) or input shape (for neural network methods).
2. **Action space type and size** — determines Q-table columns or output shape.
3. **Observation encoding** — the agent must interpret the observation format the environment provides. For Q-learning with `Discrete` spaces, observations are just integer indices. For neural network methods, observations may need to be vectors, images, or other structured data.

A well-designed Gymnasium environment exposes `env.observation_space` and `env.action_space` so the agent can automatically determine these properties without hardcoding.

---

## 12. The Q-Table and How It Scales

The Q-table is a 2D array where:

- **Rows** = states
- **Columns** = actions
- **Each cell** = the learned Q-value for taking that action in that state

| Scenario | States | Actions | Q-Table Size | Training Difficulty |
|---|---|---|---|---|
| Cliff-walking | 48 | 4 | 192 entries | Fast |
| Blocks World (3-digit) | ~120 | ~90 | ~10,800 entries | Moderate |
| Blocks World (6-digit) | ~14,400 | ~90 | ~1,296,000 entries | Slow |

As the state-action space grows, training requires more episodes to adequately explore and learn good Q-values. This is the **curse of dimensionality** in tabular RL — and one of the motivations for function approximation methods (like DQN) that can generalize across similar states.

---

## 13. Visualizing the Q-Table

The Q-table can be visualized by mapping it onto the shape of the environment. For a grid world, each cell in the grid shows the Q-values for that state, and the agent's learned policy becomes visible as the highest-valued action in each cell.

For Blocks World, such visualization is less intuitive because states don't have a natural spatial arrangement — transitions jump between configurations rather than moving smoothly through adjacent cells.

Changing hyperparameters (learning rate, discount factor, exploration rate) and observing the animated Q-table is a powerful way to build intuition about how Q-learning behaves.

---

## 14. Summary of Key Concepts

| Concept | Key Point |
|---|---|
| **Gymnasium** | Standard Python library for RL environments; successor to OpenAI Gym. |
| **Environment interface** | `reset()`, `step()`, `render()`, `close()` — any environment implements these, any agent can call them. |
| **Observation space** | Formal description of what the agent observes (Discrete, Box, Dict, etc.). |
| **Action space** | Formal description of available actions (usually Discrete for tabular RL). |
| **Terminated vs. truncated** | Terminated = natural end; truncated = early stop (new in Gymnasium). |
| **Seeds and reproducibility** | Same seed → same pseudo-random sequence → reproducible experiments. Changing only the seed can significantly alter results. |
| **Mix-and-match** | Same algorithm works on different environments; same environment works with different algorithms. |
| **Stable Baselines 3** | Library of standard RL algorithms (DQN, PPO, etc.) compatible with Gymnasium. |
| **MlpPolicy vs. MultiInputPolicy** | MlpPolicy for flat observations; MultiInputPolicy for Dict observations. |
| **Q-table scaling** | More states × more actions = larger table = longer training. |
| **Model-free vs. model-based** | Model-free agents learn from interaction only; model-based agents can plan using an internal model. |
| **Code as data (Prolog)** | `assert`/`retract` modify the program at runtime — non-logical but necessary for state simulation. |
