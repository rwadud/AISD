Recap: LLM Evaluation and Subjective Metrics

We talked about the composite score. I also mentioned that when you evaluate an LLM application, you evaluate every component of it, because each component's degradation can cause the end result to also be different. Then I mentioned that in a lot of cases, LLM use cases are very subjective in nature. For example, if you want to evaluate something like a chatbot-generated response for a customer, you want to evaluate at least the response and how informative it is, what is the tone of the response — it is very subjective. So you cannot easily create a standard objective measure of those things. So what is typically done is using a more powerful LLM and using the LLM-as-a-judge approach.


LLM-as-a-Judge: Three Approaches

LLM as a judge works in three different ways. One is without reference, another is with reference, and the third is comparison. This method is not free of any issues, because there are a lot of biases. So you have to provide the template with the proper rubrics and proper instructions, and it is always better to ask the LLM to score on an individual scale and provide for each individual value what should be the criteria explicitly.


Biases in LLM Judges

What are the biases in LLM judges? Typically, it is known that LLMs tend to prefer text which is generated by a similar class of LLMs. For example, if you are using GPT-4 as a judge, it may prefer the text generated by GPT-4 compared to Gemini, because the kind of language, words, and everything depends upon the training data used, hyperparameters, and what type of alignment the model has. Also, it is known that if you put references of great people inside your text — say some scientist or someone of authority — then the LLM will give preference, just like humans do. It has a bias towards authority. So you need to remove all that. There are some criteria to avoid these biases. If you take care of all that, it is a good method. A very good method. And if you repeat it, and if you do it with different judges, it is a very good method for evaluating the output.


Self-Reflection vs. Application Evaluation

[Student question about LLM evaluating itself]

That is self-reflection. You get some answers and use a prompting strategy for self-reflection. That is definitely there. But we are talking about something different here. We are talking about evaluating an application. So we need to evaluate each component, and the metrics are slightly different. But yes, you can ask the LLM to do self-reflection and improve the results. That is more of a prompting strategy.


Today's Lecture: Evaluation Frameworks and Observability

So this is the kind of background of the work we did last week. Today we will talk about a slightly different aspect, a little bit deeper into this whole evaluation. We will talk about some of the evaluation frameworks. What I mean by framework is that there are two aspects. One is that you need to collect data about what the LLM is doing. When you build an LLM application, like a RAG (Retrieval Augmented Generation) system, you need to know the input, and you may know the final answer, but in between, what happened? For example, you might have the LLM do self-reflection and give an answer. So what was it thinking during that step? In the case of a RAG system, it first takes your query, converts it into an embedding, and queries against the vector database to get the context. So what are the contexts retrieved? If you want to evaluate each component, you need to see what those values were, or what the LLM was thinking if it was doing self-reflection. You need to log these things somewhere if you want to do offline evaluation. How do you capture these? This whole thing is called observability.

If you look today in cloud computing or even in LLMs, observability is a big thing. It is a big area in MLOps now. Today I will talk about what observability is and how to instrument observability in your LLM application. Then I will talk about a couple of frameworks like G-Eval, RAGAS, and DeepEval. These are open source frameworks which come with some of the previous metrics like BLEU score, METEOR, and others. They have many pre-built metrics. You just call those libraries. You do not have to create or write your own metric. You just provide the input and output. These frameworks are useful in that sense.


What is Observability?

Observability means trying to understand a software system not by going into the code and going through what logic is implemented, but rather by looking at the input and output, and some of the intermediate components. By looking at some kind of logs, trying to understand the behavior of the system. That is basically observability — understanding system behavior without needing to know how it is implemented. This is useful to diagnose and resolve issues in a software system, to understand how it is behaving.

You implement observability at every level. You can implement observability at each functional level, and you can log input and output. Observability will help to figure out where the problem is, but you might have to do further logging to figure out the root cause.

It is particularly useful today because software architectures are based on microservices. When you send a request to an application, it goes through a path. It probably calls one service, gets something, then calls an API, gets something else, then takes that output and sends it to an LLM and gets something else. It is a distributed system of microservices. Observability actually traces the entire route of the request and collects data from each point — metadata. Then it gives you that metadata.


Telemetry Data

Observability is a method of understanding a system. Telemetry is a type of data — just like image, just like text, telemetry is a kind of data emitted for observability when you instrument your system. Telemetry data extends the concept of logs into traces, spans, metrics, and logs.


Traces and Spans

In generative AI applications, the most important concept is traces. A trace is the path of a request through a software application, a distributed application, recording what happens at each component.

For example, take a RAG application. You send a request — let us say, take cybersecurity. You ask: "What do you know about this type of attack? What is the MITRE technique corresponding to this type of attack?" It is SQL injection. I want to understand more about SQL injection. I build a RAG database where there is information about the MITRE ATT&CK framework, which is a kind of universal knowledge base about all the attack techniques.

So I ask a question. I am a cybersecurity administrator. I ask: "What do we know about SQL injection? What does MITRE say about SQL injection?" That query first goes to a vector database where the MITRE techniques are stored. So that is step one — I have this question, the question is then converted into an embedding. That is step two.

So you have a vector store and you have an LLM. You have a query. The query is first converted into an embedding — that is the first step. Then the embedding is used to query the vector database — that is the second step. Then you get the context output — that is the third step. Then you combine the query along with the context — that is the fourth step. Then it is sent to the LLM and the LLM generates a response — that is the fifth step. This is a typical RAG flow.

Let us take a simpler example — say a travel knowledge base. I have some travel information stored in a vector database, already vectorized. That is the knowledge base.

Now, when you send a query — let us say "What are the best places to visit in Paris?" — a trace is generated with a trace ID. That trace ID captures the entire path of what happens during the processing of this query. The trace consists of spans. Spans record what happened during each step — what happened during the embedding conversion, what happened during the vector database query, what happened when you combined the context with the query, what happened during the LLM call, et cetera. The trace will consist of several spans — maybe at least six.


Sessions, Traces, and Spans Hierarchy

You may have multiple requests in one day — say 10 different queries. These traces can be grouped into a session. The hierarchy is: Session contains multiple Traces, and each Trace contains multiple Spans. Different cloud providers might use different terminology, but "trace" is the most common term. Each trace corresponds to one request. Each span corresponds to what happened during one operation within the trace. For example, one span will contain detail about one particular operation during a RAG process.


OpenTelemetry Standard

There are different standards for implementing observability, and the most common standard is OpenTelemetry, which is an open source industry standard for instrumenting observability.


Instrumentation Approaches

There are different ways of instrumentation:

1. SDK-based (Manual): If you want very fine granularity, you manually go to each function and instrument it. This gives you maximum control, but it is very laborious and creates problems in terms of coupling. If you want to change from OpenTelemetry to something else, you would have to change all the code.

2. Auto-instrumentation: What is typically done instead is auto-instrumentation using the OpenTelemetry standard itself. It does something called monkey patching. Dynamically, at the time of executing the function, it creates a wrapper. For example, say you are using the OpenAI chat completion library for making calls to the LLM. If you instrument using auto-tracing in OpenTelemetry, it takes the original OpenAI API function and dynamically creates a wrapper. Inside the wrapper, it still calls the original API, plus it emits the trace. This way you need minimal code changes. At the top of the function, you just add a decorator — one line saying to trace this. You do not need more code inside the function. However, it has some disadvantages in that sometimes auto-tracing can create conflicts with other logging libraries. But typically, auto-instrumentation is what is mostly used.

3. Proxy-based: Here, the application traffic is routed through a third-party server, and that server extracts things from the input and output and logs them.


Trace Data Structure

What does observability data contain? It contains traces which record the path through the application. Each trace consists of spans, which record what happened inside each specific operation.

For example, in an OpenTelemetry trace: there is a trace ID, and each span has a span ID and a parent ID. The first span — for example, creating an embedding — will have its own span ID but no parent ID because it is the starting span. The next span, corresponding to retrieving context from the vector database, will have its own span ID, and its parent span ID will be the span ID of the previous operation. That is how spans are chained. Every operation will have its own span ID and a parent ID which is the span ID of the previous operation.

There is also status information indicating whether the operation was successful. Then there are attributes — for example, you can include attributes like IP address, port number, host, and other metadata. You can also capture events.


Agent Tracing Example

Here is an example from an agent instrumented with tracing. There is a dashboard that shows traces and different spans. This example is for an agent created for computing a Fibonacci number. The agent is supposed to write a Python function to calculate the Fibonacci sequence. When you ask it to calculate, say, the 50th Fibonacci number, it will execute that function as a tool call and then calculate that number and give you the result.

In the trace, you can see the step-by-step execution. There is a predict step, then a predict stream. The LLM understood the query — that it needs to calculate a Fibonacci number. Then there is a function call to compute the Fibonacci number, defining the Fibonacci function. Then it calls and executes this function, and then completes the answer. This is a sequence of operations that happened, just like in the RAG application. The trace shows each sub-span with operational details.


Tracing Code Example

Here is a basic code example with three functions. There is a say_hello function that takes a string. It calls a format_string function to format the string, and then a print_hello function to print it.

To instrument this with tracing, you import the tracing libraries from OpenTelemetry — the tracer provider, simple span processor, and span exporter. You initiate the tracer.

In each function, you use "with tracer.start_as_current_span" and give the span a name (e.g., "say_hello"). Inside the span, you set attributes to capture data — for example, capturing the original string and the formatted string. For the print function, you can add events to capture when print was called.

When you run this with say_hello("world"), you get three spans:

The first span is "say_hello" — it has a trace ID, a span ID, and no parent ID because it is the starting span. It has an attribute where the name is "word" and the value is "world."

The second span is "format_string" — it has the same trace ID but a different span ID. Its parent ID is the span ID of the first span ("say_hello").

The third span is "print_hello" — it has its own span ID, and its parent ID is also the span ID of the first span ("say_hello").

Both the "format_string" and "print_hello" spans have the same parent ID. This is because of how we instrumented it — both functions are called within the say_hello function. It is not that print_hello is a child of format_string. Rather, both format_string and print_hello have the same parent because they are both called by say_hello. That is how we instrumented it.

So you have got an idea of how the instrumentation works and how the traces look. This is a matter of deciding how much instrumentation you want. If you want to capture what the LLM is thinking, the parameters, everything can be collected through these traces. Then you take that output and send it to a dashboard for visualization. That is how it works.


Observability Frameworks Landscape

What are the other frameworks available? OpenTelemetry is the standard, and there are many products built on top of OpenTelemetry. There are a lot of frameworks that use the OpenTelemetry standard and provide libraries for tracing:

- Arize Phoenix and RAGAS: They completely support the OpenTelemetry standard, so you do not need third-party converters.

- LangFuse: Built on the OpenTelemetry standard. It provides libraries for tracing.

- LangSmith: Provided by LangChain. LangChain has its own tracing framework. If you are building a LangGraph-based agent, it is easy to instrument your observability using LangSmith.

- MLflow: Not exactly built on top of the OpenTelemetry standard, but you can get third-party libraries to convert into OpenTelemetry standard and export logs into a UI.

- Weights & Biases: They have their own observability framework.

- Databricks: If you are building agents on Databricks, they have their own agent framework. It is built on top of the OpenTelemetry standard, so you can easily export the logs.

- Opik and others: Also open source platforms built on the OpenTelemetry standard.

The majority of observability frameworks use the OpenTelemetry standard. Some of them do not. Depending on what agent framework you are using to build your agent, and what observability dashboard you want for visualization, you can decide which observability framework to use to get your traces.


Why Observability Matters

This will be useful for you because you will be building agents in your project. These days everybody is building agents left and right, and most of these agents do not work — they break. They all work in simple scenarios, but in real production scenarios, they all break. So to begin with, you need to debug them. Observability will be very useful to look at the traces and find out what is happening.

Number two, all of those agents will have security issues. None of the agents are secured by default. If you just deploy the agents tomorrow, somebody will hack them, and your enterprise will suffer a lot of damage. So you need to secure them. For security purposes also, these traces will be useful. There are many fields in traces that can help figure out, for example, if somebody did a prompt injection or jailbreaking. You can look at the trace and see what happened.


Cloud Provider Observability (AWS)

If you are building agents using AWS, GCP, or Azure, generally all of them support the OpenTelemetry standard. Amazon has two relevant services: CloudWatch and CloudTrail.

CloudTrail captures all the user actions — like if you create an agent, rename an agent, or set agent permissions. Whatever users do on AWS resources is captured in CloudTrail.

CloudWatch captures whatever happens inside an agent. When you send a query and get a response from a RAG application or agent, what happens inside — those are captured in CloudWatch. The trace IDs and spans will be there.

In Amazon, there are two ways of creating an agent. One is that you use their UI and create an agent. Or you build an agent using a framework like CrewAI or LangGraph or whatever on your laptop, then you deploy in Amazon using Amazon Agent Core. In that case, you need to code the instrumentation. But the logs and traces will come in CloudWatch and CloudTrail.

There are different levels of instrumentation. There is auto-instrumentation, or you can manually instrument — that is up to you. But even if you do not do any instrumentation, CloudTrail will still capture that you created an agent. However, it will not capture what happened during that agent's execution in detail. There is a certain level of metrics that Amazon automatically captures by default, but for detailed traces with all spans, you need to instrument.


G-Eval Framework

So far, what I have told you is about how to capture the logs so that you can do evaluation. Now I am going to talk about some frameworks that actually do evaluation.

G-Eval is a framework from Microsoft Research. They extended and standardized the LLM-as-a-judge method.

Remember, earlier when we did the LLM-as-a-judge — for example, you have a summary and you want to evaluate the summary on a scale of 1 to 3. One means the summary is coherent but not complete. Two means coherent and complete. And so on — you give some rubric, and it gives you a score of 1, 2, or 3. But you probably want a more numerical score, not just an integer.

What G-Eval does additionally:

First, it automatically creates chain-of-thought evaluation steps for the LLM. For example, if the task is to rate a summary of a news article, the evaluation steps would be: Step one, read the news article and identify the main topic and three key points. Step two, read the summary and compare it to the news article — check whether the summary covers the main topic and three key points. Step three, check if they are presented in a clear order. Then assign a score on a scale of 1 to 5, where 1 is low and 5 is high. These chain-of-thought evaluation steps are automatically provided by the G-Eval framework.

Second, it uses token probability-weighted scoring. When the LLM outputs a score — say "1" — it is not always just "1." If you set the temperature to something other than zero, the LLM produces all tokens with certain probabilities. That is how generative models work — next token prediction. So "1" is the maximum probability token, but there are also tokens for 2, 3, 4, 5 with lower probabilities.

What G-Eval does is that when you query the LLM, you can get all the tokens with their probabilities. When you send the API request, you can get the token probability for each token. Then the G-Eval framework takes each score, multiplies it by the token probability, and takes a weighted average as the final score.

Why do this? This gives a more robust measure, because you do not know the confidence by which the LLM is giving a particular score. If you run it 10 times with temperature 0.2, maybe 8 times it gives 1, one time maybe 2, another time maybe 3. To get a sense of what the confidence value of the LLM is for the scores it is giving, this metric captures that.

These are the additional things that G-Eval proposes on top of LLM-as-a-judge.


RAGAS Framework

There is another paper — the RAGAS framework. This is an evaluation framework specifically for RAG. This paper came from a group in the UK. They came up with three metrics for evaluating RAG:

1. Faithfulness: Faithfulness measures whether the claims made in the answer can be supported by the context. It basically checks for hallucination. There is a context retrieved from the vector database and there is a response generated by the LLM. Is there any hallucination happening? It compares the generated response with the retrieved context.

What it does is that it extracts individual statements from the generated answer, then verifies each statement against the context. The faithfulness score is the fraction of statements in the response that are supported by the context. For example, if the summary has 5 sentences, and 4 of those 5 sentences can be verified against the context, the faithfulness score is 4/5.

[Student question about whether order of context matters]

Yes, the order matters. In LLMs, technically order matters — if you change the order, it gives different importance. The beginning is more important. That is true. But the point here is different. Even if the context is very long and relevant information may get lost in the middle of the context, we still want to measure: is the LLM hallucinating? Maybe the reason is that your context is too big, and you want to change the order or make the context smaller. But regardless of the cause, we want to measure as a result — is it hallucinating? So it checks each statement from the final answer and verifies whether the statement is supported by the context.

2. Answer Relevance: This measures whether the generated response directly addresses the question. This is slightly different from faithfulness. Here, we are not talking about hallucination, but whether the answer is relevant to what was asked.

What it does is take the answer and generate possible questions that this answer could respond to. It does an inverse process — take the final answer and generate possible questions for it. Then it calculates the similarity between each of those generated questions and the original question. The average similarity is the answer relevance score.

This makes sense because if your answer is highly relevant, then most of the generated questions will be very similar to the original question.

[Student question: What if the LLM generates the answer correctly but not from the context?]

If the answer is relevant and useful, but it is not derived from the context — the LLM generated it from its own training knowledge — that would be captured by the faithfulness metric. The answer is relevant, but it is not faithful to the provided context. It is generated from the LLM's own knowledge.

RAG is used in situations where the LLM does not know the context — for example, your company's internal documents. In those cases, if the LLM generates answers not grounded in the context, that is hallucination. But if it generates a correct answer using the provided context, then we know it is using the context.

3. Context Precision: This measures how relevant the retrieved context is for answering the question. Here, you take each sentence in the context and see if that sentence is relevant for generating the answer. Maybe the context has 10 sentences, but only 1 is relevant for answering. This is similar to a precision metric for retrieval.

How do you fix poor context precision? You either return fewer results or you re-vectorize so that you get better, more relevant context. This metric is evaluating the quality of your retrieval.

So to summarize: faithfulness checks for hallucination, answer relevance checks if the answer addresses the question, and context precision checks the quality of the retrieval.


DeepEval Library

These are two academic papers — the G-Eval framework from Microsoft and the RAGAS framework. Then a company called DeepEval took these and created an open source Python library which you can download and use in your applications. It includes RAGAS metrics, G-Eval metrics, plus over 30 additional LLM-based metrics. You can call them like plug-and-play. They also have the ability to generate synthetic data and do red teaming. You can check for jailbreaks and many other things.

I expect all of you in your LLM application to do some level of evaluation. When you finally show me a prototype at the end of the presentation, if you have not properly done evaluation, I will not give full credit. I expect all of you to do some level of evaluation depending on your application, and you can use the DeepEval framework.


Evaluation Rubrics and Structured Criteria

For example, with a single prompt score, you give a rubric to the LLM judge. Suppose you want to summarize a chat session, and the criteria is that the summary should cover key findings in order. For evaluating the summary, you ask the judge to evaluate on a score of 0 to 3: Score 0 means the summary does not contain any of the key points. Score 1 means the summary has some key points but they are in the wrong order. Score 2 means the summary has all the key points but they are in the wrong order. Score 3 means the summary has all the key points and they are in the correct order. You provide this rubric, and the judge evaluates and gives the score.

With DeepEval, you can provide a bit more structure. Instead of plain text rubrics, you can give a more logical flow with conditional evaluation. For example, first check: does the summary extract the key findings? If yes, are the conclusions accurate? Then check the ordering. You can give intermediate scores. You can provide more structured evaluation criteria, which makes it easier and more logical.

There are also RAG-specific metrics, multi-modal context metrics for LLMs, and safety metrics like toxicity detection — all available in DeepEval.

If you do not have access to any commercial AI APIs or LLM APIs, you can use Ollama and install the Gemma model on Ollama and do it. It will work, maybe slower, but it will work.

I strongly encourage you to go through the two notebooks and run them, because number one, you need to implement evaluation in your project. Number two, understanding this is very important because all of you will be working on some GenAI application in the future, and evaluation will be very important.
