will cover portions taken till today during play from multiple. So I have some issues for. Both. If I'm not audible there, please move to this. So, ML, right? So usually we go, you know, we see the best model available, right? We try the wages and the greatest models, state of the unbod. But every problem we will be doing for two reasons, that we want to apply that just infling architecture into a formon, right? And we want to get some experience in that that's natural to everybody think about it. But often that is not a pure way of doing it. So the recommendation is that do not use, do not just start with the state of the art, the greatest and the best one. How do you go back? Start with the reason why you should not start with the state of the art that they often come as publications which are you know, trained or evaluated on standard benchmarkont data sets, which are often clean and, you know, well documented. Whereas in real data, how you how good this perform perform. So you need to yourself evaluate that. So you got to start with some simpler models first and create some kind of baseline and which through which you can understand pure data, understand what are the variables or features which are giving good signals for your data set. That's one reason. So second reason is that these data models are more complex models, right? And they often require more appreciational sources train and also for infere. So again from that point of view, they will not be optimal, not just from the accuracy of performance point of view. and they could cause more latency between. So because of all these reasons, right, you should not just start with the set of the art model, but you need to find out which is the best model for your data set, which is the least complex and most efficient model for your data set where you wrote experiments. So you start with the simplest model. The simplest models are easy to implement then cl eye out and create a pipeline and try out to see how good it is working, right? So if you're a simple model, let's say you're taking a classification. If the simple model is only giving 60% accuracy, then it's very unlikely that even if you, you build a complex model, right, it will give the kind of accurency, let's say you're targeting 95% kind of thing, right your data is don't look at all, right? So might as only go back and try to get more data at that point.? And even if you know, it gives a, let's say, around 80, 90, 80 percent accuracy, then there's hope to improve, right? You need to understand what are the features which are useful, right? Which is, you know, you indicating food predictions right right? So simple models like any kind of linear models, linear reduition in the case of a continuous vari noistication models in the case of classification problem will help you to understand, right, those things, which are the important features. And also created good baseline, above which you can start adding more complex models I see how much using. So the baseline creating a quick model, creating a pipeline, using a cl simple model is very important. It will not take much time, but it will give you some kind of. So this is one thing thisch do create that FaceTime model. What baseline model is the festival diff on your problem, on your data. could even start with some similar, if it's a computer vision problem, you Your baseline model still could be a declinating model, but be sim declin on network or something. And these days if it's a language model, it could be a, transform it's a world model. But have a simpler model, create that baseline baseline model pipeline, which is very invaluable. You see that you would be able to use that, right for one of proper things. Then evaluate the performance, evaluate the performance using, you know, not just to, because you don't know whether you have sufficient data or your model you need to improve terms of compment.erence. You are in a feature. and you don't know in your, right? So you need to figure that out. So how do you figure that out? So there is this concept of a learning curve. Okay. So learning cur nothing but you keep suppose your data, right? Suppose, let's say, 1 million records. You create multiple from that. Mle not just by that different. But you create training data sets of different sizes.. Okay, so you create one training net as a thousand propforms. Another net does at 5,000, 10,000 report, 50,000, 100,000 report. Like that you create training data sets of different sizes. There for each of the size, you print a model. Each of the size is printed into train and violation sets. Then create a training model, you train on the model, compute the what are the metals you want to do, users evalationmentals. Let's say accuracy of classification. Compute that accuracy score. on that size data, it, both on the training data, as well as on the mation data. Because you can always calculate the training on the training, right? You do this for all the different sizes and clock is good. So what happens that typically the curve will look like this, what is your left side? So the reason is that why that very red cur coming down and the green curve going up is interesting. Okay. So the reason that the as the training data is increases, the reason that the accusive score coming down because imagine if you only has a few data space, right? Then, suppose your model, right? Always a model can fit through a few data points, right? So always you can fit the traininging anonym of this one, because it always finds some optical point, right, which can fit through all the datapoints. Okay? So you're training accuracy, the accuracy of training data, right? When you trade on a model, if you evaluate from the training data itself, the training model, the accuracy will be very high if yours, data says is four. Whereas when you increase the data size, right, it cannot fit through all the points. So you sort of finds around the pilots. So you sort of start getting their ti tiny accur starting. So that's the reason the vertical is typically it comes down. Now, why there isn't the dream girl, the dream car with that crossbidation acur? Okay, why the car going up? Because if you spot on the dat size, again, what happens in that in terms of all the possible to go to cross violation that is not seen by the world, right? It's a diver. So it could be anywhere in the space of the division, right, for an hour. So smaller data, it may not be closer to the you know, what which is train implicated, right? So it does not offer much generalization. But what happens if you increase the validation set, right? It is able to generalize well, because you see, it has all better point.. Right? So it does some points near it's cleaninging ADR, it doesn' points away from training the AD and that. So, so your valation set, right? So you will crossparation score. So if you look at the car on the right hand side, what is happening is that the training air is not coming down, because you still need to increase the data sizeO. It's overfitting. It's all fitting on the training, even with size thousands more hundred days. So probably if you take size 10,000 there, 100,000 there, you see that you down. Now, there difference between the red girl and the green girl. Okay, that's also an interesting. So that tell you how much kind of the model is all fitting on some. If there's too too much gap between the training score and cross in school, let's let's you get let's around 500 You see there's a big job, right? So that indicates that you need to increase your training data.. Whereas a smaller gas gap indicates that there are not much, you, leverage further increasing the testes. For example, in the case of left curve is, whereas model, for net model, even if you increase increase, you training at more than400 it's not going to learn more. Okay, sort of. Okay. So that is good, but is the model itself useful, right? Because the model itself ised named by a cell is not very useful, because even though the gap between tailing data set anderation curve varies more, the activ is actually is very low, soly about low by day five. Right? So you if you train the model, right to with400 samples, soleium incre sample. eventually you are going to get only a model, which is 85,000 actress. So it's not very useful. Whereas if you take the SPF, SP of the crossbar score is reaching about the rose to 1. Yes, it's over 50. Yes, because the red tub is still on the top, right? But there's hope that if you increase probably more training later. then that all will rate reduce, but your cross ar matches will be. So given these two models, you can decide which model to choose, right? And whether we need to train more on we need more training data by plotting these that are called learning girls. And you can do it very easily in cyclular. There is a library to plot the car, which just have to. you don't need to write any code. So this is very important concept from models and people don't often do this is a lot of earth. train, for example, one, two, three, four, five models. five training are to do, not five models, five different training you have to do with different sizes of your, you know, different sizes of your training. So this is is a very important.s very important, you know, Concept to understand, there's a concept of straining the validation. Is there anybody clear about this? Any questions? Okay. The other thing, you have to keep in mind for when you evalu with different algorithms. So for those who came late today, are talking about how to, you know, choose the right ML algorithm for you want to play. Because we started with data sources and then we went into data engineering pipeline, then last class we talked the it'sary, today we are talking about how we choose the right. So I started saying that don't just use. So, don't just use the theatory yard model, start with the simpler model. It's a baseline model. Then you put it on the problem.'s always like always like coverages or can be like Z Zag That typically happens if it gets stuck in some, you know, the But if you average over many times. So sometimes it's possible to get a better score for the test data than the training data. It's possible, right? Do you cross to them? You can get up. Copy the bas CP, click on, why do I copy here? He wants to clear a copy here? Where is my is my, right? I don't Usually they'top the basement.. CD. Go to this? Because that saying that the model is performing better than the data already steemed right. Intuitive the Okay. Okay. Okay, so the other thing is that. So you typically have a two type of analyors, right? False, fal. Now, in a typical real scenario, right, the costs are not same. The cost of having the fal not same as also having more. For example, take a credit card fraud detection, right? So if you have too many false force, what would happen, you are flying too many transactions are as suspicious. So you're analyst who is investigating below a lot of to investigate. Which is not a good thing, but think about if you miss the f. If you have too many fallults, you, negatives, you, say it is it's actually a bad transaction, but you said it just what will happen? Too many you get eventually a lot of customers will comply and then it'sim charged back, you to lose lot of money. Similarly, if you take any diagnos problem, right, detective cancel whatever, right? Sposity means what? Somebody don't have a cancel, but diagnose that is cancelled, right? So what's what is going to happen, right? You are going to have another, person rename confirmation. But if you miss, you know, it's going to lose his cost life.. So typically the costs are not similar here. So you need to see, you particularly you should look at metals which are awesome sensitive and. I. There are another thing you should keep in mind, right? You should not take atcing of metals in the world. It's not similar. It's symet, right? You should it's not very sensitive to you know, most famous. Then, other thing you should look at when you in a kind of organization scenario, when you decide a model, is accuracy versus a competitional cost. For example, a logistication model is 90%, right? Whereas a deep learning water giving 95%roacy, or 92 to 93%rosacy, right? It's properly five times or t times more computer expensive. So is it worth is it worth, you know, just for 2% of classification act proceed, spending, you know, 5% percent of cost. Whereas if you take a problem like, you know, in advertisement, there is a problem of the click through rate Cia, which is the probability that you click your ad show an ad. There is different because even a point one person in at Z can make a lot of different even revenues because you're going to show mediums of ads in your day, right? There probably makes sense to, you know, for a point when I could say switch from a normal model. So it's very much difference on even business context. Does it's worth, you know, going for a socialistic room. The third is latency, right? You model, like depending models are more accurate, but it take more time to do a response, then the customer cannot use the phone. So these three factors, the cost of false posity versus false negative, accuracy versusational, legency will. You need to keep in mind. these things are not taught in the theory course, because the theory course, they only willote actors. right? So they are not not concerned about it. But as an MLO, you know, as an MLO engineer, ML deployment engineer, whatever, right, you need to keep this things that's going to affect your organization's, you know, bottom line revenue and, the next step of things you keep in mind are, you know, all different models are different assumptions. What are these assumptions, right? There are those assumptions are valid for your data? That's another thing you should keep in mind. Now, here is actually a shoot.. Okay. Look. at this. It's a kind of a guy, right? This one. Yeah. You need this one right now? This is.. Okay.. So this giveseshift to tell you. what I'm not going to go through thisesh, but if you essentially will tell you a different type of algorithms, right? The linear models, the linear regression and logicalation printsression, Lassition, in your free based model, dist forestition, XG boost, GB, that you have these clustering methods associated models. sort of classical lemon models, okay?. So what is each of their what do they do? And what kind of scenos they are used and what are their advantage, what are their demon? Again, this is not to be used as a recipe, but kind of keep in mind.. So typically, these day today, you know, these are the kind of workforce orders., you're a mixture of classical an, and linear and sorry, numerical and categorical are because they take care of a lot of things like UR the noise in the data, but the correlation between different variables or fitting under fitting that kind of scenario take there. But not necessarily that everywhere that is the best model or the fishing order, right? So this is one thing, you know, just to keep. Okay, so this is a person you are just now. Imagine you're working with it. Okay, before that, you guys have done already, right? In ML. So you. Are you learning distance, are you learned already? We have our brand. I don't know we don't care? We did great news. We're not afraid in terms of brush. I mean,. That's the implementation. Yeah, both ways. We have metaphors. T the bottom of April part of the out, I don't think we' power. What?. This is not for plus, maybe. But it's good to learn this. Plus is getting a good thing to learn. And what are done? So this is actually useful because they will a pleas a bit of auralization, and you want to fix that. So it's still legal model, but it will reduce the outfitting in control of some organization. So it may be useful to learn. This one doesn't going to give you any error., your data, data, these days, can't forest to most suitables. The handle pizza. And you know, they don't need them because usually when you have the art number of features, right, there's always you need to produce the number of features feature selection as I mentioned last. right, right? So the way the Raporus graded between modern works is that each model, right, is producing a subset of the or varium, So that way, it's already sort of incorporating, you know, the feature direction. Then at the end there's will be some kind of working, and a lot of reasons, if you go into the theory of this model, you can understand at these models. That's also the Okay. So that's for choosing the right development algorithm. So there's no recipe, there's only a great line, but you need to figure out for your data set, what is the best model using the process I can. Start a midline model baseline model, use the training and valation curve, and then experiment. There's also this whole idea of hyperparameter tuning, which I' talking about. Because even if you take a model like, you know, neural network models or wasient models or SPF, there are some parameters you cannot learn from data. Those parameters are to learn by doing building models on different values of the parameter and applying on the cross variation set and finding which one use the best life. So I'll be talking about that today. So before that we go into something called distributor training. Okay. So once you decide what algorithm you are to use, right, from the previous weather? Now it comes to train. Now it's possible that in a lot of cases, typically, you know, these models like water we showed you, there are classical machinery, therefore are not very, you know, memory intency, so they will typically fit into a CPU orpology. But if you start, you're not training deep tural networks, right? There are like 10 or hundreds of layers, right? And they still forget very big memory, right? So they start becoming. They do not fit into a memory of your H. So it become necessary to have a distributed training where you distribute the training processes into multiple distribution. Now that can be done. in using different different approaches. So that's what we are going to talk about. So this tells you, you, Martin I use the distribute training necessarily, like large language models, right? LLM artificically, you know, the few business parameters and billion parameters will means, you know, a few gigabytes of memory, right? For example, if you take Simclus ama model, which is the only billion parameter to save. And if you take not each parameter to store us, let's say, 30 the station, 30 decision or 16 bit precision, you can work out. If you come out to be around, you know, maybe 12 billion, 12, to gigabytes memory. But what happens, that is not enough. When you train a model, it keeps a coffee of the entire parameter. So it keep a double, you in a double So as far as GP like a 24 GBGP will not be sufficient to train as tri per. So you need to go today, the standard is H 100 CUs, which have GPUs, are like 84B, that kind of scheme, right? So, yeah, so medicically, you know, any manag are AV, right? Medically man are hide and sc. So they will require a many, you know, big big GP machines to do. Then image genic sequences, right? You know, are huge data, right?ain, you buy huge data, you, model digit. Now, so there are different ways to. Take care of this problem. It's not necessarily that you are go. You know, get the bigger memory machine or distribute. So there's simple check technique called grade gradient checkpointing.. So usually, this is typically a flexible to neural electron models. So what happens is that usually, you know, all neurallectrol models are trained using in the back operation, right? So when you do the fourway pass, you compute each line of computing gradients, right? You store all the gradients in memory you call one why you are doing the backward fast, you have to adjust these gradients.? You have to take the error time, multiply by some spameter, add to the original gradient. That's how you update the period., right? So you store all the gradients. That's why you're using one lot of memory. So instead of doing that, what you should do is that, what is proposal is that you only grow a small number of, you know, nodes, the grad. Okay, don't everything. Then what you do, right? You still need the values. So by thatation, you actually recompute when you are doing that doing again defe forward. And you don't have to start from all the way from the, the damage the slightly point, right? It can start from where you can start from there and So you can do the last randomly, you know, pick up the n, your, layer, neural network, and each layer has, you know, a number of neurons, right? So they get set. You randomly pick up nodes and only those nodes for which you only st while forward past the radiant bipes. While backward pass, any node, right? If you need value, you compute that gradient starting from, you, not from all that from the beginning where, but somewhere there in the we reserve, there's a node, right, for which that value useful. So this what is shown is that using this, you can able to train models, which are almost 10 times bigger in memory. But there's a trade-off, right? Because you're computing your bates, right? Again, there's a trader, but that trade-off is not much. The tradeoff is only about, you know, 20% increasing, okay? So if you are okay to, you know, increase the computation time by 20%, in the same hardware, you can, you know, compute, you know, at 10 times you. So that is the advantage of very important. Okay. So this is very is a simple thing you can go through, you know, So maybe if this is something you guys can, if you're going to use our GP plus our GPUs are very small, I think. I don't ATB that.. maybe advert you have maybe 12 or 20 of them, so you maybe that. Then, so what this is a simpler about what can do, then comes the actual paralyzation, right? If you want to still not do. You want to really distribute the training across multiple multiple, you, machines. There are three, there are multiple ways of doing conceptually, there are three ways in what is called a data parallm. Second is called modern parallism, and third is called pipeline. So it's easy to understand. So data parallysm where you you split the data and multip, you know, then you keep the same model. You give the same model to all the GPs. So suppose your NGP. You take the data, splread it into N partitions. Each partition is in. Then you keep the same model all these you use. Then you do the forward pass, by one word pass, accumulate the gradients. Then you do some kind of, you know, share it in the sense, because you might at that another day, you want one motor, right? So you take the gradients from all the machines and do a reduction. And give you some kind of aing on some kind of reduction operation. Then that value would be used at this. So that is the simple what is COVI data values. Now, this other disadvantage because you are, first of all, train the same copy of the model on each machine, right? So you the data is split, right? It might not save you much in terms of memory. If you' you know, model is fed, right? It's not still tri each machine is. Okay. So not only that, it also, you need to wait till you know, each machine computing finishes training mode, right? training to do to do theuctionation, right? So there is some kind of you know, the slower machines will slow process, right? So that's a dessert one do this. Okay. So, okay, there are two ways of doing it. actually, you can't do the cronus mod or a Cymranus mode. So if you use the cronous mode, right, that will create a standardron model, because crodus mode will wait till all the machines. So also, if the number of machines are more, then more would be that problem because there be more chances of submachine, you know, being less, you know, faster.. So this will resource not a computerational resourcesces. Now, this can be redeemed from that load balancing, right? you want to make sure that because typically, these resources will be shared at centra, right? So you want to make sure that, you know, all the machines are equal free memory available, equal computed is fine. So, by the way, if you guys know that typically, you know, only 50% of the computation capacity of each mission is used. So let's say your mission is typically, say. a GP speed. Okay, does it mean that when you actually run the model, right or any operation, you use all of that sleeping power. Because itributing a lot of other operships, the kernel, a lot of data starting and scpping out, right? So what passes passes a sort of difficul your efficiency here, right? It's only 50% of the naturalual computation. So that is where what happens by sometimes some missions will slow down. It may be that time it may be, you know, stopping some data between memory and hardest, that kind of things. Okay, I' either receiving data from some other mode using network and stuff. Okay. So because of these reasons, the synchronous mode will not be very good. It creates this standard problem because they are to wait till all the n complete that training, right? So it will waste. So to some extent, this can be mitated using proper road balance. So again, as an ML engineer, right, road balancing all these things need to be, you know, aware of an part of your job. M a lot. Yeah. So you can distribute distribute model.. the second machine working on, the same part of the data, still working on. This will start working on, you know, the second part of the data. So initially there will be some slack, but after some time, you see here, everything machine is working on some part of the data. So this is more optimized than either doing a pure data panelysm or pure bodal parall. So this is for a five parallysm. Okay. So even this is not the best. even pipeline parm there is a slack because this yellow guy, right, has to wait for this green guy to finish that. There is a sequential infment, though, it is much smaller the top. Right now, because unlike in data parallelm, this machine is only working on a small set of the data, some set of the data, not the data set. So it is much smaller, but there's still that is that. small kosack comes. Okay. So what are there is another the third called Fishered data paralyysm, which you take care of that. Okay. So this is pand paralyism. Okay. Now, this fully sharp data panel takes care of that by doing up. So you fully, fully, data panels of what is done is unlike here. So here the spring or sequature, you see?.. So here, I'm splitting the model, right, the models is sprinting simple, that is the first impP which you get layers from one to ten. The next you, you can get layers technically, like that. 22 to 30 lenges. space. S of split sequently. And that's why when I go forward fast, there's the dependency. This position has to compute, then it goes to the next mission. So in FSDP, instead of doing this way, from every layer, you sample a subset of a rameter, then distribute it. Okay, so you from one, take some set of parameter, you to the GPS zero, take an set of sub set of parameter due to GPU, third subset of parameter and the GP like that. Okay. So every layer has some parameters from so every GP has some parameters from some players. It's not a sequential kind of split. The spirit of the model is more random. Okay. So in the model model distributed parall, the model parallysm, you are slicing the model layer by layer, then you're spitting. Okay, here, it's not like that. You take sample some parameters from each layer. Okay, let's say first lay, sample some parameter give to sample some other parameters. samplesles some parameters. So every GPU has some set of parameters from every level. That's a difference. at what with that is that when you do that, what happens is that So typically, if you are doing all data panels, as I mentioned earlier, there are four GP. EGP are different data sets. So compute the gradient, then activate. That's what happens, right? Here, what happens is that. GP right? This is already a subset of parameters. Okay, then what happens it calculates the gradient on a subsular data. It's a microb. The data also is. Then it has to update, right? It has to updates. So for that update, it borrow the gradients from other G views. Okay, but only for that particular layer. Okay, For example, right? So as I mentioned, you know, as I mentioned, right, you for the layer one, right, you have some parameters from here, right? Now, the remaining parameters probably are distributed across different machines, right? So to completely update that layer, you need all the parameters of that layer. So all the necessary parameters for that layer borrow acquired from other other machines.. So there's a planform of data should up. Then only once you do that, it only take all the parameters required, reduce all the parametersters required for that plan. The second chip you will do the competition for all the parameters required for another line. Okay. So once it does this, then it just distribute all the values. I want to theory that the main advantage is that this model, right, don't have to to So to hear what happened, you are the entire model, right? Enter model, you are splitting the data. Okay. So what happens if you're not fit the old model of the memory. Okay, but here you're only taking part of the data part of the variables, they're part of the parameters, right, for each layer, right? So not the ender mode, part of the parameter for each layer, right? So each GPU has parameters of all the layer, but only some sort of parameters. There's for the memory requirement, a much smaller. But after doing this operation for reduce scatter gather, we get the same information. Okay, every G you love sum of all the gradients from all the teropoints. It just sort of reordering the way you come through this song.. That's all. It's a very clever doing it. So you can just go through through this flog from. This is invented by a Facebook. It's called F panel. So there is also also a nice library. called Fair Sk open source, does this. Facebook, fair stands for Facebook A research. This was led by I left. So this. So both of this, little, you know, I'm really difficult this understand movie, but there is also a five torchs with aation. So. some problem. But this gives such gives. So today, the state of the art food training this is also the the work of there is something called 3D p results. So they, you know, in more direction. I don't really understandood that. So I think it is a first deep p is one of the caries of distributed training these days. founder. So again, you guys can experiment in this, the lib is available, right, for theose library. So, yeah, that's good. Okay, so any questions before you go to what happened?. Anyway. Mm hmm. Yes... I's not even that. was. And then the key will in there... inside the best model find you into whatever. All camera is. So the reason I think a lot of companies like Google and Microsoft push this because to do this, you need to spend lot of computation, right? It's for cheap. they make money.. So data scientists will lose money, they will make.. So this comes in. Okay, so this old autom is, again, a kind of fancy term. It comes in two varieties. One is called that has soft auto. That is where it is hyper parameter to be. It is just that pure model architecture is finalized, algorithm is, you know, but there are some parameters which cannot be learned from data directly. through, I mean, at the way you've learned from data, but it is not learned by minimizing cross function. Okay. So let me do data. So those parameters you learn. So those parameters which you cannot learn byizing a loss function or hyper parameters. So that hyper parameter tuneing, right, is the final step of your order development. So that you do through an automatic process. That is the software no problem.. Then the hard version of automile is you don't even know the architects or the motorbike. You probably know it's a neural body, but you don't know how many layers. You know, each layer of the only new roads. And if it's a pololution neural letter, what should be the size of the coration operator, what kind of loss function issue should use? What kind of activation function would you use? Not all of this more. That is where the real automal comes. It's called architecture search. Now, the soft automile, the parameter, you know, to me, you can do multiple ways. The simplest way is to do different research, right? So if you have three parameters, right, as we have two or three hypop parameters. You each hcle parameter, you create a grid. It's a plusper parameter to 1. You split into 0.1 0.2. The other hyp parameters's, and you create some training, right? Then you systematically explode every point in this grid, right? And that's got a good search. But that is extremely expensive because as the number of hyper parameters increases, this research will not exp exponentially more like possibilities are there. So this is not recommended. So what is instead, what is accommended in and do phantom? parameters, you have iniversional space. Inst going through that space sequentially. source you randomly pick up some points. And do evaluations and do the tuning. So how you do the randomly pick up a point, train a model, better the performance on the violation data set. If that is better than the best model existence so far or the best value than the parameter investment so far, we replace with that model and that is possible that you finish, so you matably samples some points, right right? It's possible that after you go through all these randomly samples samples, your final model performances to order. Then again you do another randomly samples. So the donating that this is linear, right? If you can keep doing it till you want, but it's still linear. every time you're only evaluating some ensamples. Where are the field systematically going through, right? You want to go exponential. So trying to search randomly for that kind of where is the road... So this is what is great search is now, never practised in use to practice. Okay. Un unless it is one parameter type of things. Okay. Or two parameters. It's a two. moment it become more than two parameter, always you go for hypper parameters, not a parame hyp parameters, go for a random. So now, there is even a better mother came up, certainly as was called Beian optimization. So in Beian optimismization, what you do is that you built actually a model on top of this hyperg parameter optimizing itself. Okay. So you built a model which was will sort of predict which is the best value of the hypographic. Okay, so it is like in a simple fruit sense, you're modelling the accuracy of the model as a function of the hyp parameter device. Okay. So it define not with the accuracy, but also the error, how much is your confidence introval.. So it gives you both that your predictive value of your accuracy, plus the confereency. Okay. So you create this kind of a model using the account of the or thousing process. Okay. The part what you do, you start with the you know, few values randomly some values, you actually vegetables the model values. So you have a hyper parameter space, you randomly samble some end pointsoints.. Then you compute, you for each of those points, you compute in your model atus. Then you this as a training data for. model. Okay. Then, so what's this process model? give it theper productive, which you will predict your model accuracy and the confidence of the accuracy. Then you do what is for that exploration exploitation. You now start, use different values of your hyper parameter, put it into this model equation and see what is the value of your predicted value of your atrosm. Now you are not easy to general model. You see this arrogant model, right? This metadit model. Then look at regions where your accuracy is. So this model will tell you, this gossip process for model will tell you. called this particular particular value of combation for parameter, this is your expected accuracy of the model. Okay, to subregion where this accuracy is high. but you don't not direct monic direct. Then you take those values, then actually put it in over the model and file. Then you then. So you get one more data points. When you do that, you get one more data point, right? You do data point, right? Then you update your. Okay? So you keep doing this. Okay. Keep doing, you explore different regions of the hyper parameter stage using this gussian process model, which will predict you what is expected value atrosio of your original model. If you use the micro parameter value. then take regions for this confidence is high and the app. The confidence is high, the confidence is low, you should check because that is not probably an exponation. I look to say. Then you actually take those points, for value points, then actually do an experiment in the real model. Okay, take the accuracy value and improve your processive model. Keep doing it. This is surprisingly very fast, which will give you a good hyproarameter value within like 12 seconds of. Okay, because this is a sim model. You're doing a kind of a, you know what di diural model kind of thing. So this is what is currently used in, standard, you, SVM machine or if you're doing, you are sort of by the way do a parameter to doing in R Forest or you know, the graded postal disputes. So they're all the useation of. function. So that is question. Is it clear? it consentially clear? The final thing is to not actually do the neural architective search also called NAS. Okay. So the essential idea is that you first created such space. So such space is is nothing but your importance of the neurural network. Okay, the number of layers or the kind of controolutional pressure, right? Three by the congation.. All possible kind of convolution, all possible kind of connection types, all possible kind of, you know, activation functions. This is your subst.space, because you are going to build different architectures by taking these different combinations. That's idea, right? Now you need to. Okay, so you have a suspect space, which are the different importance on the model. Then you need that kind of strategy. How do you do the search? So there are different kinds of strategies, but simple strategy is new studies for exploration and exploitation. That is, 80% of the time. You know you do something for like exploration, that is, you know certain an architectural verse well for this. For example, in objectediction,ation. You know, one or not. not in the thing. So you, 80% of the time, you take formolutionional kind of the architecture, but you experiment with, you know, cing inside or back organizations, small cets. Or on the size of the corporation, 80% of the time that. 20% of the time you do wild estation. You do completely different kind of networks. Okay, maybe you're not per network. Maybe it's, you know for an object classification, you try something like LSD. Com. So that then you keep exploring who this explorational exploation. Keep experimenting and find the best. So different such that they are also I about that. So that is the second day. So there would be such space where you keep a lot of border.. Then you need a performance is specialicious this year.. So you probably typically do some kind of cross violation. That's what I'm doing. Did you do? No. That's what he's running. So what are the differe strategies? What is for the enforcable learning based?. Oh, you guys are doneforcement learning? Yes.. What are you doing? So you guys know what is an agent, what is this action, you, right? So here you have anroller model. That's what you're doing. a model act as an agent. So that agent will propose the model description. form of language. what I' studying. Then that model that agent described, you actually implement, then measure the performance. Then the value of your performance, your, right? Let's say is accuracy. He's given us a reward back to the What is supposed to doforceform.. So we keep doing that. So you know, right, all the enforcement optimize. So when you keep doing that eventually gets you want where the act will be. That's idea. So there's an example, illustrator something called NASnet, which is a neural architecture search for a producation problem where it is shown that they beat human design models only data. Only thing is that they had to run about 800s for four months. Okay, it's possible, but, you know, it's expensive. The second way of doing is called evolutionary worth. How many fl No. Yeah, kind of. So basically, I did that use the idea of. Yeah, I did that. So you start with the random models. Okay. each of the model. each of the model you implement rest of the performance and you kill all the models are you the performances. Then you mutate the good model, the mutate means you. N of, right? Or the number of new rules in each. They' repeat this process. Okay? This way you can do the selection and natural selection and motational process. And repeat, and you get a good. So there's again, example for for a by net. Okay. But again, it's not computentially achieved, right? It's respons. Because every time you have to build these models, I. So there is a third that they're called differentiable gradious meds. Here, what they do is that they create one supermodel. They take all the models and create one supermodeling. So that is done using a kind of equational. That's. environment. So let's say this. That's under the environment now? Which one? Yeah. I' starting for a short. This one. So let's say take the scenario of, you know, that what I mentioned, right? You are such space, right? You're doing different kind of operations possible, right? Youation. I different type of you know, activation function, different kind of functions, all that's right. So each of this is a cdidate. to go into the casinate. Then you are wait for the candidate. In the form of air, not, this you, Sio function. Okay. Now this function is not different as a function al. Then you can do a kind of grater this on this. Okay, you know, and find the value of alpha. which sort of best fit the whole of fish. So that is how you do Yeah. So that is why. So this is actually, let' explained, there is another law. you should go through this. Explain very well how this works. What is it in this?. All of the unforcement are gradient based Did it actually an example and do. So just a bitical. Okay. But again for an example. That's a code. Yeah, so maybe I'll do next time or something. Okay. So this is now the popular way of doing it. And here the computational time is running four months, you can do it in few hours.. So maybe we are trying to find the example of theis. So this is another instructions. He says you have to make it. So yeah, so that's it for today. So I hope you learned that this high this. So I decided you to go and you know, mentioned read the first. 